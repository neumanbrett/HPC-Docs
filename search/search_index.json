{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#user-documentation-for-ncar-high-performance-computing","title":"User Documentation for NCAR High Performance Computing","text":"<p>This is the home of user documentation for the NCAR high-performance computing (HPC) and storage resources managed by CISL.</p> <p>The knowledge base includes searchable information specific to HPC resources, storage systems, authentication procedures and others, as well as additional how-to articles and troubleshooting articles.</p>"},{"location":"#selected-links","title":"Selected Links","text":"<ul> <li>Getting Started</li> <li>Using Derecho</li> <li>Using Casper</li> <li>Using JupyterHub</li> <li>Getting Help</li> </ul> <p>Don't find what you need? Log in here to submit a help request: NCAR Research Computing.</p> <p>You need a CIT password to submit a request. Call 303-497-2400 if you don't have one.</p> <p>Tip</p> <p>The NCAR HPC Users Group (NHUG) is a reouce group for all users of NCAR HPC resources.</p> <p>All users are welcome to join the NHUG Slack workspace.</p> <p>CISL welcomes your contributions</p> <p>This project is hosted on GitHub and your contributions are welcome!</p>"},{"location":"contributing/","title":"Contribution Guide","text":"<p>Welcome to the NCAR HPC Resources GitHub repository!</p> <p>This guide provides an overview of how to contribute to this documentations and the standards to follow when adding content to this repository. Our goal is to create a comprehensive and user-friendly documentation resource for NCAR's HPC resources.</p>"},{"location":"contributing/#repository-overview","title":"Repository Overview","text":"<p>This repository contains technical documentation for NCAR HPC resources. The documentation is written in Markdown, which is then converted to HTML/CSS/JS using the mkdocs static site generator. We have customized the mkdocs-material theme to align with NCAR branding and colors.</p> <p>Note</p> <p>Here is the reference to the mkdocs-material documentation features.</p>"},{"location":"contributing/#making-contributions","title":"Making Contributions","text":"<p>For modifications, such as a comprehensive revision of a section in the documentation, we recommend fork this repository, and clone the repository to your local machine and working from there.</p>"},{"location":"contributing/#steps-to-contribute","title":"Steps to Contribute","text":"<ol> <li> <p>Fork the repository: Go to the repository page and click the \"Fork\" button to create a copy of the repository under your GitHub account.</p> </li> <li> <p>Clone the forked repository to your local machine: This can be done by running the following command in your terminal:</p> <p><pre><code>git clone https://github.com/&lt;YOUR_USERNAME&gt;/hpc-docs-demo.git\n</code></pre>     Replace <code>&lt;YOUR_USERNAME&gt;</code> with your GitHub username.</p> </li> <li> <p>Create a new branch: It's a good practice to create a new branch before you start making changes. This can be done by running: <pre><code>git checkout -b &lt;BRANCH_NAME&gt;\n</code></pre>     Replace <code>&lt;BRANCH_NAME&gt;</code> with a name that gives a hint about the changes you're about to make.</p> </li> <li> <p>Make your changes: With your new branch checked out, you can start making changes to the documentation. Remember to save your work regularly.</p> <p>Tip</p> <p>You can live preview your changes locally by running <code>mkdocs serve</code> in your terminal. For more on this, see Building the Documentation Locally section. </p> </li> <li> <p>Commit your changes: Once you have made and tested your changes, stage the files you have modified using <code>git add &lt;file&gt;</code> or <code>git add .</code> to stage all changes. Then, commit your changes with a descriptive message using <code>git commit -m \"&lt;YOUR_COMMIT_MESSAGE&gt;\"</code>.</p> </li> <li> <p>Push your changes: You can push your changes to your forked repository by running <code>git push origin &lt;BRANCH_NAME&gt;</code>.</p> </li> <li> <p>Submit a Pull Request (PR): After pushing your changes, go to HPC-Docs github repository, and click on \"New pull request\". Fill in the necessary details and submit the PR. Once your have submitted the PR, a bunch of automatic workflows will be triggered. readthedocs will build a preview of your document and add it to the PR. This allows you to preview your changes before they are merged into the main branch.</p> </li> </ol> <p>Please note, for larger changes, it's often a good idea to discuss your plans in an issue before investing a lot of time in implementation.</p>"},{"location":"contributing/#building-the-documentation-locally","title":"Building the Documentation Locally","text":"<p>If you want to build the documentation locally to see the changes you've made, you can do so by following these steps:</p>"},{"location":"contributing/#create-an-environment","title":"Create an Environment","text":"<p>To build the documentation locally, you'll need to install certain dependencies. Although this step is optional, we strongly recommend it. </p> <p>The example provided here utilizes a conda environment, but feel free to use any Python 3 environment of your preference.</p> <pre><code>conda env create -f conda.yaml\nconda activate mkdocs\n</code></pre>"},{"location":"contributing/#preview-documentation-locally","title":"Preview Documentation Locally","text":"<p>You can preview your documentation locally to make sure that your changes do not introduce any errors. With MkDocs, you can preview your changes by running <code>mkdocs serve</code> in your terminal. This starts a local server where you can preview your work.</p> <pre><code>mkdocs serve --strict\n</code></pre> <p>Note</p> <p><code>--strict</code> flag will enable strict mode and treat warnings as errors. This is useful to ensure that your changes do not introduce any issues such as new pages that does not exist. </p>"},{"location":"contributing/#simple-contributions","title":"Simple Contributions","text":"<p>Warning</p> <p>At this stage, please avoid making any changes using this tool, since it will make changes directly to main branch. </p> <p>If you're looking to make a minor adjustment to our documentation, such as fixing a typo or adding minor enhancements to a few documents, GitHub's built-in web editor and web IDE make it easy.</p> <p>As you browse our documentation, you'll notice a pencil icon next to the header on each page. This icon is a shortcut to edit the current page. Here's how you can use this feature:</p> <ol> <li>Click on the pencil icon to open the editor.</li> <li>Make your desired changes.</li> <li>After you've made your changes, be sure to update the commit message. A clear and concise commit message helps us understand your contribution better.</li> <li>While not mandatory, we recommend renaming the branch for better organization and tracking of changes.</li> <li>Submit your changes.</li> </ol>"},{"location":"contributing/#feedback-and-support","title":"Feedback and Support","text":"<p>If you have any questions or need assistance while contributing to this repository, please reach out to the repository maintainers or open an issue on the GitHub repository page.</p> <p>Thank you for your contributions and helping us create a valuable resource for NCAR's HPC community!</p>"},{"location":"allocations/","title":"Allocations","text":"<p>The Computational and Information Systems Laboratory (CISL) provides large-scale computing resources for university researchers and NCAR scientists in the atmospheric and related sciences. To access these supercomputers, storage systems, and other resources, users must apply for allocations via the processes defined for each community.</p> <p>Applications are reviewed and time is allocated according to the needs of the projects and the availability of resources. Send questions about the following allocation opportunities to alloc@ucar.edu.</p> <p>Info</p> <p>The next university deadline for submitting Large Allocation Requests is March 13, 2023.</p>"},{"location":"allocations/#university-allocation-opportunities","title":"University allocation opportunities","text":"<p>NCAR provides computing resources to the university community for investigations that are beyond the scope of university computing centers. The CISL HPC Advisory Panel (CHAP) accepts requests for large allocations of NCAR resources every six months, in March and September.</p> <p>See University allocations for details.</p> <p>Info</p> <p>Eligibility. In general, any U.S.-based researcher with an NSF award in the atmospheric sciences or computational science in support of the atmospheric sciences is eligible to apply for a University Community allocation. There are some limited opportunities for those without NSF awards.</p>"},{"location":"allocations/#wyoming-ncar-alliance","title":"Wyoming-NCAR Alliance","text":"<p>The NCAR-Wyoming Supercomputing Center represents a collaboration between NCAR and the University of Wyoming. As part of the Wyoming-NCAR Alliance (WNA), a portion of the Cheyenne system \u2013 about 160 million core-hours per year \u2013 is reserved for Wyoming-led projects and allocated by a University of Wyoming-managed process.</p> <p>Details of the Wyoming process are available at the University of Wyoming web site.</p>"},{"location":"allocations/#ncar-lab-allocation-opportunities","title":"NCAR lab allocation opportunities","text":"<p>NCAR investigators have access to CISL resources through allocations to the NCAR labs and have opportunities to submit requests for larger-scale, project-oriented allocations. Proposals for larger-scale projects are reviewed twice per year to become NCAR Strategic Capability projects.</p> <p>See NCAR allocations for more details.</p>"},{"location":"allocations/determining-computational-resource-needs/","title":"Determining computational resource needs","text":"<p>Documenting your code\u2019s performance and scalability to demonstrate that your stated resource needs are reasonable is an important aspect of preparing an allocation request.</p> <p>These guidelines are intended to help you gather and present the data you need to support your request. Ultimately, you will complete a table like the one shown here to include in your application. With accompanying narrative that describes each experiment or experimental configuration, it will communicate clearly to reviewers what computing resources you need and how you will use those resources.</p>"},{"location":"allocations/determining-computational-resource-needs/#estimating-derecho-allocation-needs","title":"Estimating Derecho allocation needs","text":"<p>Derecho users can expect to see a 1.3x improvement over the Cheyenne system's performance on a core-for-core basis. Therefore, to estimate how many CPU core-hours will be needed for a project on Derecho, multiply the total for a Cheyenne project by 0.77.</p> <p>When requesting an allocation for Derecho GPU nodes, please make your request in terms of GPU-hours (number of GPUs used x wallclock hours). We encourage researchers to estimate GPU-hour needs by making test/benchmark runs on Casper GPUs, but will accept estimates based on runs on comparable non-NCAR, GPU-based systems.</p> <p>You can use a chart like this as a starting point. Review the documentation for the relevant allocation opportunity to learn what else is required.</p> Experiment (Experimental configuration) Core-hours per simulation Number of simulations Total core-hours Totals"},{"location":"allocations/determining-computational-resource-needs/#cesm-and-wrf","title":"CESM and WRF","text":""},{"location":"allocations/determining-computational-resource-needs/#community-earth-system-model","title":"Community Earth System Model","text":"<p>In the case of CESM, see the timing tables provided by the CESM team at NCAR for information that will help you develop a statement of resource requirements for your project.</p> <p>The computational cost of CESM experiments typically is expressed in core-hours (also known as processor element-hours or \"pe-hrs\") per simulated year, so the \u201cCost pe-hrs/yr\u201d column (the cost in core-hours for each simulated year) provides the necessary value needed to calculate the cost of a simulation; you provide the number of years.</p> <p>Your allocation request should show that the number of years proposed and the model resolution chosen are sufficient and necessary to answer your scientific question(s).</p>"},{"location":"allocations/determining-computational-resource-needs/#weather-research-and-forecasting-model","title":"Weather Research and Forecasting Model","text":"<p>For WRF projects, review the guidance on our Optimizing WRF performance page. Follow those recommendations as you do some benchmark runs to estimate the number of core-hours you will need for each planned simulation. Cite that page in your core-hour request and describe for the review panel how you estimated your resource requirements.</p>"},{"location":"allocations/determining-computational-resource-needs/#other-codes-and-models","title":"Other codes and models","text":"<p>Proposed experiments may be different enough from the documented CESM and WRF simulations that you need to run your code several times to determine how many core-hours you\u2019ll need. If you are using other codes or models that do not have well-known or published performance information, you will need to document the performance and scalability of your codes to complete your resource request. (A reference to a web site or paper with performance and scalability details for the code or model is acceptable for purposes of an allocation request.)</p> <p>Presumably you\u2019ve run your code on a multi-core system and have at least a general idea of what resources you will need in order to run at the same scale or larger on the Cheyenne or Casper systems.</p> <p>To begin fine-tuning your general idea into a specific request for resources, consider these questions:</p> <ul> <li> <p>How large is any data set that you need to load?</p> </li> <li> <p>How much memory needs to be available for you to complete a run? (The   peak_memusage tool can tell   you how much memory your program uses.)</p> </li> </ul> <p>Your answers will help you calculate the minimum number of nodes you can use.</p> <p>Memory needed / memory per node = minimum nodes</p> <p>That minimum number of nodes can serve as your starting point unless you already know that a larger number will work. You may need anything from a few dozen to hundreds or thousands of cores.</p> <p>Document your code\u2019s performance with test runs on the Cheyenne system if possible, or on a reasonably similar platform and software stack. You will want to illustrate in a graph that your code performs well at increasingly higher scale, showing at least four points indicating the results of your runs (as in Figure 1).</p> <p></p> <p>If your application\u2019s performance is not already well documented, you may need to establish a baseline by running it on a single node before scaling. On Derecho, your parallel code would be executed on a total of 128 cores on a single node. From that baseline, you can generate scaling data by making a series of runs on progressively greater numbers of nodes to determine the optimum number to use.</p> <p>Start by documenting the smallest run that you know will work. For example, say you\u2019ve run your parallel code on a similar system using 4,000 cores, or you\u2019ve had an opportunity to do some similar-size test runs on Cheyenne. Do additional larger runs to demonstrate that the code scales as you expect it to scale. To ensure accuracy, it can help to do several runs at each point on different days to detect variations that might result from changes in the machine\u2019s workload.</p> <p>Based on the hypothetical results shown in Figure 1, a run could be done efficiently using 1,000 cores. The job would finish more quickly, using less wall-clock time, than it would using 400 cores.</p> <p>To convert that into total core-hours needed for one type of simulation, multiply core-hours per simulation (the time needed to complete one simulation times the number of cores used) by the number of times you are proposing to run that type of simulation.</p> <p>Core-hours per simulation x total simulations = total core-hours</p> <p>That gives you the information you need to fill out one row of your table. Repeat the process for each type of simulation, then add the figures in the last column to get the overall core-hour total.</p>"},{"location":"allocations/determining-computational-resource-needs/#strong-vs-weak-scaling","title":"Strong vs. weak scaling","text":"<p>As you do your test runs, keep in mind that strong scaling \u2013 the degree to which performance improves as more processors are applied to a fixed problem size\u00a0\u2013 is more useful for these purposes than weak scaling, which reflects the ability to solve larger problem sizes with more processors. Strong scaling will reflect the improvement in speed (resulting in reduced overall run time) as more processors are used.</p> <p>At some point, using more processors is likely to have little additional benefit, because the potential for increasing speed is limited by the ratio of serial to parallel code in the application. Figure 2 reflects such a scenario, which shows little or no performance benefit from using more than 5,000 cores. If you identify such a point in your test runs, it would make little sense to base your allocation request on higher numbers of cores.</p> <p></p>"},{"location":"allocations/determining-computational-resource-needs/#reporting-on-performance","title":"Reporting on performance","text":"<p>When submitting your allocation request, provide documentation on how you generated your scaling data. Include a graph similar to those shown here to illustrate the results.</p> <p>Describe how flexible your code is regarding the number of processors it can use and why you chose a certain number on which to base your request. It also is helpful to include information on the portability of the code to other platforms and on your team\u2019s knowledge and experience with systems that are similar to Cheyenne.</p> <p>Use these links to download sample proposals:</p> <ul> <li> <p>Example proposal 1</p> </li> <li> <p>Example proposal 2</p> </li> </ul> <p>They are specific to large university allocations but are good examples of documenting performance that you can follow for other types of requests.</p>"},{"location":"allocations/chap/","title":"CISL HPC Allocations Panel","text":""},{"location":"allocations/chap/#overview","title":"Overview","text":"<p>CISL's advisory panel on high-performance computing and services is the CISL HPC Allocations Panel (CHAP). The CHAP's primary responsibility is to assess the merit of large computing requests for CISL supercomputers and related resources.</p> <p>The panel accepts computing proposals from U.S. university researchers in the atmospheric and closely related sciences who are supported by the National Science Foundation (NSF). The CHAP recommends action with respect to a prospective user's request on the basis of the computational experimental design, computational effectiveness, and availability of computing resources.</p> <p>Note</p> <p>The next university deadline for submitting Large Allocation Requests is March 12, 2024.</p> <p>CHAP members, most of whom are from the university community, are appointed to three-year terms by the CISL Director. Meetings are scheduled twice a year, usually in May and October. NSF's program coordinator for the NCAR and Facilities Section attends the semi-annual meetings and provides guidance from the NSF.</p>"},{"location":"allocations/chap/#core-hours-allocated-and-used-by-university-projects","title":"Core-hours Allocated and Used by University Projects","text":""},{"location":"allocations/chap/chap-allocation-review-criteria/","title":"CHAP: Allocation Review Criteria","text":"<p>The CHAP assesses the merits of large computing requests from U.S. university researchers for use of CISL supercomputing resources. Computing requests are accepted in the atmospheric and closely related sciences for projects that are supported by the National Science Foundation.</p> <p>The panel recommends action with respect to a prospective user's request on the basis of the computational experimental design, computational effectiveness, and availability of computing resources as described below.</p>"},{"location":"allocations/chap/chap-allocation-review-criteria/#overall-context","title":"Overall context","text":"<p>As part of reviewing the merits of requests for CISL resource allocations, the CHAP and CISL allocations staff will ensure that:</p> <ul> <li> <p>All requests for resources that exceed a threshold level determined by   CISL and the CHAP shall be peer-reviewed.</p> </li> <li> <p>Written reviews of the resource requests shall be completed in a   timely way and made available to the requestors.</p> </li> <li> <p>Recommendations to CISL management for the allocation of resources   based on the requests, reviews, and available resources shall be   documented.</p> </li> <li> <p>The allocations process shall be consistent with   the conflict-of-interest policy and   shall maintain confidentiality of requestors and their reviews.</p> </li> </ul>"},{"location":"allocations/chap/chap-allocation-review-criteria/#purpose-and-scope-of-reviews","title":"Purpose and scope of reviews","text":"<p>In its review of allocation proposals, the CHAP verifies the suitability of the work for CISL resources, considers the ability of the research team to complete the work, and most significantly, reviews the merit of the proposed computational plan.</p>"},{"location":"allocations/chap/chap-allocation-review-criteria/#scientific-eligibility","title":"Scientific eligibility","text":"<p>A request for resources will succinctly state the scientific impact of the research to be conducted and the existing merit-reviewed support for the research as demonstrated by current financial support from NSF. The scientific merit and approach will not be subject to further review by the CHAP. Since NCAR computing resources are provided specifically for the atmospheric and closely related sciences, the request must fall within these areas.</p>"},{"location":"allocations/chap/chap-allocation-review-criteria/#merit-review-criteria","title":"Merit review criteria","text":"<p>The justification of the resource request will be reviewed against three criteria, which apply to both computational and storage resources, with the level of detail of the review rising with the size of the requested resources:</p>"},{"location":"allocations/chap/chap-allocation-review-criteria/#effectiveness-of-methodology","title":"Effectiveness of methodology","text":"<p>For computational resource requests, the choice of applications, methods, algorithms, and techniques to be employed to accomplish the stated scientific objectives should be reasonably described and motivated. For data storage resource requests, the data to be stored should be reasonably described and motivated with respect to the stated research objectives.</p>"},{"location":"allocations/chap/chap-allocation-review-criteria/#appropriateness-of-research-plan","title":"Appropriateness of research plan","text":"<p>The steps in the research plan should explain how the scientific objectives will be achieved. For computational experiments, the proposed computations should encompass simulation parameters (grid size, time scale, ensemble parameters, and so on) that are needed to obtain accurate and meaningful results, as well as the human resources that can be devoted to the task. For storage resources, the justification should describe the rationale for determining which data will be stored. The amount of resources (of all types) requested should be derived from the methodology and research plan. If there are serious concerns about the research plan, reviewers should share their concerns with the proposer, and may decide to proceed only after the concerns have been addressed.</p>"},{"location":"allocations/chap/chap-allocation-review-criteria/#efficiency-of-resource-use","title":"Efficiency of resource use","text":"<p>The resources requested should be used as efficiently as is reasonably possible and in accordance with the recommended use guidelines of those resources. In exceptional cases, where the reviewers conclude that the proposed methods are so inefficient that they amount to a waste of public resources, they should not approve the request until the proposer has addressed their concerns. For computational resources, performance and parallel scaling data should be provided along with a discussion of optimization or other work done to improve the applications. For storage resources, the CHAP will consider the choices made in managing the project's data, the value of that data both within the proposing research team and among the wider community, approaches for data access and dissemination, and long-term retention plans.</p>"},{"location":"allocations/chap/chap-allocation-review-criteria/#prior-accomplishments","title":"Prior accomplishments","text":"<p>For ongoing computational activities, the CHAP will consider the progress made using prior allocations, including the publication of peer-reviewed manuscripts, other communications within the community, and the effective estimation and use of CISL resources from prior requests.</p>"},{"location":"allocations/chap/chap-conflict-of-interest-policy/","title":"CHAP: Conflict of Interest Policy","text":"<p>It is NCAR policy that the Computational and Information Systems Laboratory (CISL) HPC Allocations Panel (CHAP) procedures for evaluating National Science Foundation (NSF) computational resource requests are fair and equitable to all requestors and protect the integrity of the research, science, the NSF and NCAR. Recommendations are to be based on objective judgments of merit without regard to subjective personal biases. The guidelines and ethical standards presented here provide a framework by which conflict of interest (COI) situations can be identified and resolved, thus minimizing the level of personal bias in the provision of high performance computing resources to the NSF community.</p> <p>A conflict of interest is a clash between an individual\u2019s concern for the public interest or the best interest of NCAR and his or her private interests or allegiances. Conflicts of interest, actual or perceived, may compromise NCAR's integrity and standing in the research community, its sponsors, and the professional reputations of individuals. They also compromise the effectiveness of the decision-making process by warping and biasing such effectiveness. As such, conflicts of interest must be scrupulously avoided.</p> <p>Individuals involved with CHAP activities shall act impartially and not give preferential treatment to any individual or organization, and they may not use their position on CHAP or knowledge gained through CHAP activities to obtain a personal advantage either for themselves or for any other person or entity in whom or in which they have a financial or other vested interest. Potential and actual conflicts of interest, or the appearance of such, must be managed so that NCAR\u2019s assessment process is not compromised, research conducted through NCAR is free from bias, the investment of the public is protected, and confidence in the integrity of NCAR\u2019s activities is maintained.</p> <p>Conflicts of interest are common and even inevitable, so that a disqualification to review should be understood to be a positive solution and in no way is a reproach. Whether particular circumstances create an appearance that the ethical standards outlined in this document have been violated shall be determined from the perspective of a reasonable person with knowledge of the relevant facts.</p>"},{"location":"allocations/chap/chap-conflict-of-interest-policy/#responsibilities-of-members","title":"Responsibilities of members","text":"<p>Appointment as a CHAP member requires awareness of COI situations that may arise during the evaluation of resource requests.</p> <p>Conflicts of interest may arise, for example, in the following situations: professional and personal relationship with a requestor or requestor\u2019s department; use of inside information or access to such information; financial, investment, or other ownership interests; use of confidential information; subcontracts with employees, their immediate families, and their business associates; work with UCAR/NCAR contractors; involvement in legal actions against the Federal government and other sponsors; improper use of the UCAR or NCAR name or affiliation; and improper use of NCAR facilities and resources.</p> <p>The procedures followed with regard to COIs and CHAP activities are those of\u00a0Disclosure,\u00a0Avoidance, and\u00a0Removal.</p>"},{"location":"allocations/chap/chap-conflict-of-interest-policy/#disclosure","title":"Disclosure","text":"<p>Prior to the assignment of reviewers, CISL will summarize all known COI\u2019s for the CHAP chair (Chair). In some instances, a COI is known only to the individual panel member. Each panel member is responsible to declare immediately each COI and to bring the matter promptly to the attention of CISL and the Chair. The Chair, acting as an objective, disinterested third party, determines how the matter should be handled and additional steps, if any, to take. Simply stating and documenting the existence of a conflict of interest does not suffice to eliminate it. A written record of how each conflict was resolved for each CHAP panelist shall be compiled by the Chair as part of the official record of the meeting.</p>"},{"location":"allocations/chap/chap-conflict-of-interest-policy/#avoidance","title":"Avoidance","text":"<p>Members should avoid all conflicts of interest or the appearance of such. In the course of their duties with CHAP, members should avoid situations in which they can influence or appear to influence a decision or course of action, as well as any actions that may give monetary gain or personal benefit to themselves or to those with whom they are associated professionally and personally, as covered under the relationships discussed infra.</p>"},{"location":"allocations/chap/chap-conflict-of-interest-policy/#removal","title":"Removal","text":"<p>In those instances in which a CHAP member is a Requesting Scientist on a resource request for the current panel meeting, or a Principal Investigator (PI), or Co-Principal Investigator (CO-PI) on the NSF award supporting the resource request, the resource request will be evaluated and a resource allocation recommendation will be made to the CISL Director of Operations and Services prior to the panel meeting. The resource request will be discussed via email or via a teleconference involving those panelists without a COI on any of the requests being discussed. In instances where the Chair has judged that a COI exists for a CHAP member who is not a PI, CO-PI, or Requesting Scientist for a proposal being considered, or if there is a need for further discussion of a request that was reviewed in advance to balance the resources allocated, the conflicted panel member shall physically leave the room during discussion of the proposal. In other instances in which a COI is disclosed, and the Chair has judged it to be minor, the panelist may continue to participate in the discussions.</p>"},{"location":"allocations/chap/chap-conflict-of-interest-policy/#examples-of-conflict-of-interest-situations","title":"Examples of conflict of interest situations","text":"<p>Conflicts of interest exist for any of the relationships below:</p> <ol> <li> <p>Affiliations with a requestor\u2019s institution:</p> <ul> <li> <p>Current employment (formal or informal) within the same department   or institution. In case of UCAR/NCAR employees, conflicts exist   within the same division, institute or laboratory.</p> </li> <li> <p>Any affiliation with the requestor institution including, but not   limited to, current membership on a visiting committee or similar   body at the requestor\u2019s department, holder of any office,   governing board membership, or relevant committee chairpersonship   in the requestor\u2019s department.</p> </li> <li> <p>Currently seeking employment with the institution.</p> </li> </ul> </li> <li> <p>Relationships with an Investigator or other person who has a     personal, academic and/or financial interest in the request and/or     proposal:</p> <ul> <li> <p>Known family or marriage relationship.</p> </li> <li> <p>Business or commercial partnership.</p> </li> <li> <p>Present association as primary thesis advisor or thesis student or   past association in such a capacity over the last ten years.</p> </li> <li> <p>Professional collaboration involving research and publication over   the past four years or as Principal Investigator and Co-Principal   Investigator on a current resource request.</p> </li> </ul> </li> <li> <p>Other relationships with the requestor or the request. The interests     of the following persons are to be treated as if they were the panel     member\u2019s own:</p> <ul> <li> <p>Any relationship, such as close personal friendship, that might   affect the member\u2019s judgments or be seen as doing so by a   reasonable person familiar with the relationship.</p> </li> <li> <p>Any other conflicts known to the panel member that would prevent   him/her from reviewing a project in a non-biased, fair and   objective way.</p> </li> </ul> </li> </ol> <p>CHAP participants are encouraged to seek guidance on these conflict of interest guidelines at any time. For UCAR/NCAR employees other policies may apply, including: Conflict of Interest (1-1-4), Ethical Conduct (1-1-23), and Investigator Financial Disclosure (1-1-27).</p> <p>Download the Conflict of Interest policy (pdf).</p>"},{"location":"allocations/ncar-allocations/","title":"NCAR allocations","text":"<p>A new generation of NCAR supercomputing resources began when the Yellowstone system was installed at the\u00a0NCAR-Wyoming Supercomputing Center (NWSC)\u00a0in 2012. With the subsequent introduction of the Cheyenne HPC system\u00a0in 2017 and now Derecho in 2023, the capabilities and capacity of this resource environment expanded significantly.</p> <p>NCAR\u2019s portion of the Community Computing pool amounts to ~29% of these resources. For the Cheyenne system alone, this represents an estimated 350 million core-hours per year (compared to 170 million on Yellowstone and fewer than 9 million core-hours per year on Bluefire). Similar portions of the data analysis and visualization resources and GLADE storage system also are available to NCAR users.</p>"},{"location":"allocations/ncar-allocations/#allocation-categories","title":"Allocation categories","text":"<p>Three categories of allocations are available to NCAR users:</p> <ul> <li>NCAR Strategic Capability (NSC) Projects</li> <li>NCAR Director\u2019s Reserve</li> <li>NCAR Lab Grants</li> </ul> <p>Joint NCAR/university allocations have been discontinued, although projects with NCAR visitors or university-based collaborators are eligible to use or make requests for all three categories of NCAR allocations. See NCAR and university use below.</p>"},{"location":"allocations/ncar-allocations/#ncar-strategic-capability-nsc-projects","title":"NCAR Strategic Capability (NSC) projects","text":"<p>After review of scientific merit, strategic importance, technical readiness, and broader impact, these large-scale, high-priority projects receive 17% of the core-hours available to NCAR.</p> <p>See the NSC Projects page for details, including the next deadline for submitting requests.</p>"},{"location":"allocations/ncar-allocations/#ncar-directors-reserve","title":"NCAR Director's Reserve","text":"<p>The NCAR Director\u2019s Reserve comprises 2% of the available NCAR resources. Access is disbursed at the discretion of the NCAR director and is designed to accommodate work that does not fit within any other allocation mechanism (whether CSL, university, or NCAR).</p> <p>Director\u2019s Reserve requests must meet the following two criteria:</p> <ol> <li>The project should have clear relevance to the NCAR mission or strategic priorities.</li> <li>The project will be completed in less than one year and should not be an ongoing or recurring activity.</li> </ol> <p>Reserve requests also should also meet at least some of the following six criteria:</p> <ol> <li>The work or project lead does not meet the eligibility criteria for other allocation mechanisms (for example, the work is led by collaborators at or supported by non-NSF agencies or labs).</li> <li>The project has come up unexpectedly and has an urgency that cannot be accommodated by other allocation mechanisms (for example, simulations related to an ongoing wildfire, oil spill, or storm).</li> <li>The project is supported by a funding agency award separate from NCAR Base funding that was awarded at a time incompatible with the NCAR lab or NSC allocation timelines.</li> <li>The work has tangible benefits to NCAR as a whole that do not serve as valued criteria for other allocation opportunities (for example, educational, public outreach/service, political).</li> <li>The work cannot be accommodated within the relevant NCAR Lab Grant but has the backing of the NCAR lab leadership. \u201cMatching\u201d allocations are encouraged. That is, the Director\u2019s Reserve allocation will be matched by support from an NCAR Lab Grant.</li> <li>The work involves collaborators from multiple NCAR labs or multiple organizations or institutions outside of NCAR. (Note that a request that meets only this criterion is unlikely to merit a Director\u2019s Reserve allocation; typically, other criteria also must be met.)</li> </ol> <p>Projects that may be suitable for an NSC allocation but that cannot wait until the next NSC round can request a startup allocation from the Director\u2019s Reserve; such requests still must explain why they satisfy the criteria for a Director\u2019s Reserve allocation.</p> <p>To request a Director\u2019s Reserve allocation, the NCAR Lab Allocation administrator should submit a brief write-up (approximately one page) from the prospective project lead that describes the project to be conducted and its computing requirements. The NCAR Lab Allocation administrator should include a statement describing why the work should be considered for a Director\u2019s Reserve allocation. The request should be submitted to alloc@ucar.edu.</p> <p>Director\u2019s Reserve requests are reviewed as they are submitted, and decisions generally are made within a few days.</p>"},{"location":"allocations/ncar-allocations/#reporting","title":"Reporting","text":"<p>Director\u2019s Reserve allocations come with commensurate reporting requirements that vary depending on the size of the request and award. At the end of each project, the project lead will document the work conducted, resulting outcomes, and contributions toward the strategic priority.</p> <ul> <li>For all projects, the project lead should prepare a short write-up, usually less than one page.</li> <li>Larger projects also may be asked to produce, in addition to the short write-up, a brief (15-minute) presentation to the Executive Committee. This reporting requirement will be identified at the time of the award.</li> </ul>"},{"location":"allocations/ncar-allocations/#ncar-lab-grants","title":"NCAR Lab Grants","text":"<p>NCAR labs receive 10% of the HPC systems\u2019 available core-hours. The allocations are assumed to remain at the same levels while a system is in production. However, a member of the NCAR Executive Committee (EC) can request that the lab allocations be reviewed and potentially revised or adjusted due to changes in lab needs or priorities. The EC member should initiate this process at an EC meeting, which will determine the timeline and process for considering the request.</p> <p>Within the lab-level \u201cblocks,\u201d the labs allocate resources according to their strategic priorities. They are expected to accommodate small- to medium-sized activities within these locally managed allocations, including joint work with collaborators, regularly scheduled workshops and training activities, preparatory work for larger-scale projects, and work by visiting, postdoctoral, or graduate student researchers. (NCAR short-term visitors also may apply for university allocations if they meet all necessary eligibility requirements.)</p> <p>The only annual reporting requirements for lab allocations are acknowledgement of CISL resources in relevant publications and in the lab\u2019s annual report, and citations for those publications to be sent to CISL.</p> <p>In addition to 10% of the Cheyenne HPC resource, the NCAR labs receive allocations of a similar fraction of analysis and visualization resource use and GLADE project space. Storage space requests and allocations are constrained by the growth supported by CISL budget for system expansion. Within these constraints, NCAR labs and staff will have to make trade-offs and data management decisions, especially when considering storage of data that are generated on resources outside of CISL.</p>"},{"location":"allocations/ncar-allocations/#ncar-and-university-use","title":"NCAR and university use","text":"<p>The NWSC resources are shared among several allocation \u201cfacilities,\u201d including the NCAR Community and University Community in addition to the Climate Simulation Laboratory and the Wyoming Community. As in the past, the NCAR and university communities each get an equal portion of the resources, and NCAR and CISL are responsible for maintaining that balance.</p> <p>To support this NCAR/University balance, we offer the following guidelines for appropriate use of the NCAR and university resource pools.</p> <ul> <li>NCAR visitors (visiting scientists, post-docs, and so on) who are not permanent UCAR staff are eligible to apply for university allocations, subject to the eligibility policies for university allocations.</li> <li>UCAR/UCP (that is, non-NCAR) permanent staff may request university allocations, subject to the university eligibility policies.</li> <li>NCAR Labs and NSC projects may choose to allow visitors and collaborators to use those allocations as part of collaborative projects.</li> <li>For joint university/NCAR work in which permanent NCAR staff will be responsible for a significant amount of computational usage or a significant fraction of the project\u2019s total computational work, the preferred approach is for NCAR staff to request the necessary resources from the NCAR pool, while the university researchers request the necessary resources for their activities from the university pool.</li> <li>A university principal investigator with a university allocation in support of an NSF award may elect to permit an NCAR collaborator on that award to access an incidental amount of the awarded allocation in support of the collaboration.</li> </ul> <p>NCAR policies and guidelines for co-sponsorship are not affected by these revised allocation policies. Co-sponsorship remains a transaction between a lab and the proposer, and the process is monitored by UCAR Budget and Planning and PACUR. The UCAR B&amp;P office has approved rates for use in the proposal process.</p>"},{"location":"allocations/ncar-allocations/ncar-strategic-capability-nsc-projects/","title":"NCAR Strategic Capability (NSC) projects","text":"<p>The next NSC submission deadline is September 19, 2023.</p> <p>NCAR researchers and computational scientists are encouraged to submit requests for the first NCAR Strategic Capability (NSC) projects to be run on the new Derecho system. Requests will be accepted through March 6. These allocations should target large-scale projects lasting one year and align with NCAR\u2019s scientific priorities and strategic plans.</p> <p>Requests for Cheyenne core-hours will be accepted but they must indicate how the additional allocation is necessary to complete work that is already underway on Cheyenne or be limited to work that can be completed by the end of 2023.</p> <p>Due to the rapidly growing scale of the data generated by many projects and the constraints on storage available within the CISL environment, the NSC projects review panel is scrutinizing requests for disk and tape storage closely. As with poorly justified requests for computing time, poorly justified requests for storage resources will result in reduced storage allocation awards. See additional guidance in these instructions for preparing proposal documents, which apply to NSC project requests as well as to large university requests.</p> <p>Long-term storage plans for NSC project data in Campaign Storage should be coordinated with the requester's lab(s). The data management plan section in your NSC request document should describe the arrangements made with your lab.</p> <p>NCAR computational scientists are encouraged to submit requests for the next round of NCAR Strategic Capability (NSC) projects. These opportunities occur approximately every six months.</p> <p>Potential submitters are encouraged to consider applying to the opportunity that best aligns with their project's anticipated timetable and readiness. In most cases, a project should consider the opportunity that starts shortly\u00a0after\u00a0their planned start, so that preliminary and benchmarking results can be submitted as part of the NSC proposal.\u00a0A project for the same or similar work can receive an allocation only once a year.</p> <p>Because of the competitive nature of these allocations, labs may have chosen to coordinate submissions from each lab. Potential applicants are encouraged to contact their lab's or division's allocation representative before submitting a request.</p>"},{"location":"allocations/ncar-allocations/ncar-strategic-capability-nsc-projects/#nsc-allocations","title":"NSC allocations","text":"<p>NCAR Strategic Capability (NSC) project allocations target large-scale projects lasting one year to a few years (but not indefinitely long) that align with NCAR\u2019s scientific priorities and strategic plans.</p> <p>Twice a year, the NCAR Executive Committee (EC) reviews and approves a set of NSC projects for the coming 12-month period. NSC projects requiring more than one year\u2019s allocation must submit continuation requests each year to report progress toward the project\u2019s objectives.</p> <p>To be considered for an NSC allocation, a proposed project:</p> <ul> <li> <p>should directly relate to one or more specific priorities in   the\u00a02020-2024 NCAR Strategic   Plan.</p> </li> <li> <p>must be technically ready to begin production runs from the start of   the allocation period. Requests should provide sufficient details to   convince reviewers of the project's readiness; projects may be   penalized for delayed progress (see below).</p> </li> <li> <p>must require significant computational resources, above and beyond the   minimum level defined by the NCAR Executive Committee.</p> </li> <li> <p>should have a well-defined scope and completion timeline.</p> </li> <li> <p>may be linked to an agency funding award or awards separate from the   NCAR Base funding.</p> </li> </ul> <p>The progress of NSC projects will be monitored quarterly to ensure that projects are making sufficient progress and that all projects can complete their work in the remaining allocation period. After each quarter, projects may be subject to losing a portion of their unspent, prorated allocation. Exceptions can be made for projects that identify in their submissions that they have compressed or alternative timetables due to external factors, such as the need to align computations with a planned field campaign or calendar season.</p>"},{"location":"allocations/ncar-allocations/ncar-strategic-capability-nsc-projects/#nsc-eligibility","title":"NSC eligibility","text":"<p>All NSC project requests must have a full or part-time regular NCAR staff member with an R or T appointment as project lead. In order for term employees to be eligible as project leads, the period of performance of the project should not extend beyond the employee's term date.\u00a0Labs may choose to implement policies to coordinate the submissions from the lab in each request period. Joint work with university collaborators is eligible. Projects that span labs are encouraged, though a single project lead should be identified.</p> <p>NSC allocations have a\u00a0minimum\u00a0request size of 10 million core-hours; the minimum size may be revised by the NCAR EC, and the EC may choose to permit exceptions for cause. There is no maximum size limit, though in practice the review process will attempt to accommodate approximately a dozen large-scale projects each year. Consistent with the NSC objectives, NSC requests\u00a0should not\u00a0aggregate many smaller projects out of the same lab to meet the minimum request limit.</p>"},{"location":"allocations/ncar-allocations/ncar-strategic-capability-nsc-projects/#request-format","title":"Request format","text":"<p>NSC requests must prepare Proposal Documents, which should follow the guidance and structure for large allocation requests for universities. Notably, NSC requests must include a five-page summary along with relevant supporting documentation.</p>"},{"location":"allocations/ncar-allocations/ncar-strategic-capability-nsc-projects/#review-process","title":"Review process","text":"<p>NSC requests are subject to two-phase review, with final approval by the NCAR EC.</p> <p>The first phase is a scientific and strategic evaluation of the project. The panel conducting this review is composed of representatives of each NCAR lab and program who are appointed by the lab and program directors. This panel evaluates proposals according to several criteria:</p> <ul> <li> <p>The\u00a0three criteria used by the CHAP in its   evaluation of large university requests \u2013 the effectiveness of the   methodology, the appropriateness of the research plan, and the   efficiency of resource use. The technical readiness assessment   provides input into this aspect of the evaluation.</p> </li> <li> <p>The scientific appropriateness of the project and its relationship to   NCAR strategic priorities.</p> </li> <li> <p>Whether the large-scale computational resources needed are   commensurate with the project\u2019s strategic priority.</p> </li> </ul> <p>The panel recommends allocation levels and identifies priority ranking for submitted requests. A written review summary is made available to requestors following the final decision on awards. All panel representatives from labs and programs not involved in the proposed project review each request.</p> <p>In the second phase, the EC approves or agrees to modify the recommendations of the review panel. CISL then establishes the final allocation awards. Should any awarded project encounter issues that require it to stop work or be unable to complete its proposed work, additional allocation awards may be made to unawarded requests in order of priority rank and as resource availability permits.</p>"},{"location":"allocations/ncar-allocations/ncar-strategic-capability-nsc-projects/#review-schedule","title":"Review schedule","text":"<p>NSC requests are reviewed twice per year. Projects arising too late for NSC consideration will either need to wait, identify bridging allocations from NCAR labs, or apply for startup allocations via the NCAR Director\u2019s Reserve. Such NSC pre-awards must satisfy the criteria for a Director\u2019s Reserve award.</p>"},{"location":"allocations/ncar-allocations/ncar-strategic-capability-nsc-projects/#continuation-and-reporting","title":"Continuation and reporting","text":"<p>NSC allocations come with commensurate reporting requirements. For those projects requiring more than one year\u2019s allocation, a continuation request will need to be submitted as part of the next year\u2019s NSC request and review process. The continuation request should include a short write-up (approximately one page) for CISL\u2019s portion of the NCAR Annual Report.</p> <p>At the completion of each project, the project lead documents the work conducted, resulting outcomes, and contributions toward the strategic priority by preparing both a short write-up (approximately one page) for CISL\u2019s portion of the NCAR Annual Report and a brief, 15-minute presentation to the Executive Committee.</p>"},{"location":"allocations/university-allocations/","title":"University allocations","text":"<p> The next university deadline for submitting Large Allocation Requests is September\u00a012, 2023.</p> <p>University use of the NCAR HPC environment is intended to support Earth system science and related research by researchers at U.S. institutions. The emphasis is on projects that may be beyond the scope of a researcher\u2019s local university computing capabilities. Eligible researchers and activities incur no costs for the use of NCAR resources.</p> <p> Recent Changes</p> <p>We have recently updated these policies, both to prepare for Derecho and to expand opportunities for University researchers. Most notably, we have created a new Data Analysis opportunity to allow more researchers to analyze NCAR-hosted data sets, and we have redefined \u201cnew faculty\u201d to be any faculty member who has not computed at NCAR before. We have also called out options for expanding or extending the smaller-scale projects and clarified the eligibility language to emphasize the range of post-secondary institutions welcome to use NCAR\u2019s resources.</p> <p>While we often distinguish between the allocation opportunities based on the size of the HPC resource needs, most of these options allow you to request data analysis, visualization, and storage resources.</p> <p>To see what HPC resource limits apply to each type of allocation, refer to **the table below*.</p> <p>Small Derecho allocation requests will be accepted after the system enters production later this year. In the meantime, interested researchers can request a Cheyenne allocation now, and CISL will provide guidance later about how to add Derecho to Small and Exploratory projects.</p> Allocation request Initial HPC limit* Supplement HPC limit Frequency Funding eligibility Large No upper limit (subject to availability) No upper limit (subject to availability) Semi-annual panel review NSF award required Small Cheyenne: 400,000 core-hours Derecho: 1 million core-hours Derecho GPU: 2,500 GPU-hours Cheyenne: 400,000 core-hours Derecho: 1 million core-hours Derecho GPU: 2,500 GPU-hours Continuous NSF award required Exploratory &amp; Classroom Cheyenne: 400,000 core-hours Derecho: 500,000 core-hours Derecho GPU: 1,500 GPU-hours Cheyenne: 400,000 core-hours Derecho: 500,000 core-hours Derecho GPU: 1,500 GPU-hours Continuous No external funding award Data Analysis n/a (Casper only) n/a (Casper only) Continuous Any funding source <p>*For Small, Exploratory, and Classroom projects, the amounts shown are the limits for requests on one system. For requests to use more than one system, proportionally smaller limits apply\u2013e.g., up to half the Derecho limit and half the Derecho GPU limit can be requested together.\u00a0*</p> <p>Submitting Your Request</p> <p>For all types of university allocations, including any subsequent extension or supplement requests, requests should be submitted via the ARC portal\u2019s Allocations section. If you have questions about these options, please contact us via the Research Computing help desk.</p>"},{"location":"allocations/university-allocations/#large-allocations","title":"Large Allocations","text":"<p>A university researcher may submit a large request for work that is beyond the scope of the amounts available via Small allocation requests. These requests have no upper limit, aside from the portion of the system available to the university community and ensuring that we can support the breadth of work from eligible university researchers.</p> <p>CISL accepts requests for large allocations of NCAR resources every six months, in March and September. Deadlines for submitting requests are announced approximately two months in advance. The CISL HPC Allocations Panel (CHAP) reviews requests in April and October, and projects begin in May and November.</p> <p>Note</p> <p>We strongly recommend that researchers begin any new project by submitting a Small allocation request first. With a Small allocation, you can get started quickly and conduct benchmark and test runs to confirm the estimated computational costs of your planned model configurations. Such preparatory work will maximize your success during CHAP review.</p> <p>The panel recommends awards on the basis of the computational experimental design, computational effectiveness, and availability of computing resources (see Review Criteria). Check the submission instructions for preparing the required Request Summary document.</p> <p>If your Request Summary is ready, submit your Large Allocation Request here.</p> <p>Large allocations are made for the duration of the supporting NSF award. If the NSF award is extended, including no-cost extensions, you can ask for your allocation end date to be extended as well. At the end of your NSF award, you can request three extra months to wrap things up, and with concurrence from the NSF Program Officer, we can extend your allocation up to 12 months beyond the end of your NSF award.</p>"},{"location":"allocations/university-allocations/#small-allocations","title":"Small Allocations","text":"<p>Small Derecho allocation requests will be accepted after the system enters production later this year. In the meantime, interested researchers can request a Cheyenne allocation now, and CISL will provide guidance later about how to add Derecho to Small and Exploratory projects.</p> <p>Small requests can be submitted at any time and decisions are typically made within a few days.</p> <p>U.S. university researchers who are supported by an NSF award can request an initial small allocation of up to 400,000 core-hours on Cheyenne; up to 1 million core-hours on Derecho; and up to 2,500 GPU-hours on Derecho for each NSF award. These allocations can be used to complete small projects or to conduct initial runs in preparation for submitting a request for a large allocation.</p> <p>If needed to complete your work, you can request a one-time supplement to a small allocation \u2013 as much as doubling the total hours available for your project \u2013 with a brief statement that you are on track to finish or that you are aware you will need to prepare a large allocation proposal for any additional resources. You may also request a large allocation at any point after receiving a small allocation.</p> <p>Small allocations are also made for the duration of the supporting NSF award, and they can be extended according to the same rules as for large allocations.</p> <p>Submit a Small Allocation Request here.</p>"},{"location":"allocations/university-allocations/#exploratory-allocations","title":"Exploratory Allocations","text":"<p>Exploratory requests can be submitted at any time and decisions are typically made within a few days.</p> <p>Resources for unsponsored graduate students, postdocs, and new faculty</p> <p>A graduate student, post-doctoral researcher, or new faculty member at a U.S. university can request a one-time allocation of up to 400,000 Cheyenne core-hours; 500,000 Derecho core-hours; and 1,500 Derecho GPU-hours. These awards typically support dissertations, help foster early career research, or provide seed resources for pursuing funded research.</p> <p>An individual can request a new exploratory project at each stage of their career path. A new faculty member is any eligible researcher who has not previously had an NCAR allocation as a faculty member.</p> <p>For these allocations, in addition to meeting the domain and affiliation eligibility requirements (below):</p> <ul> <li> <p>The work must be the individual's own research project rather than a   project assigned by the institution that is hosting the grad student,   postdoc, or faculty member.</p> </li> <li> <p>The work should not lie within the scope of any funded grant.</p> </li> <li> <p>A letter from the individual\u2019s advisor or department head must affirm   the quality of the proposed research and that the work is not within   the scope of a funded grant.</p> </li> </ul> <p>If needed to complete your work, you can request a one-time supplement of the allocation, up to twice the original amount, with a brief statement acknowledging that you will finish within the supplemental amount.</p> <p>Exploratory requests are accepted at any time and decisions are typically made within a few days. Exploratory allocations are made for one year, but may be extended up to two additional years to complete the original project.</p> <p>Submit an Exploratory Allocation Request here.</p>"},{"location":"allocations/university-allocations/#data-analysis-allocations-new","title":"Data Analysis Allocations (NEW!)","text":"<p>Data Analysis allocation requests can be submitted at any time and decisions are typically made within a few days.</p> <p>Earth system scientists can request access to NCAR\u2019s Casper data analysis cluster and the Campaign Storage file system to allow them to conduct analyses on data sets curated by NCAR data services and accessible via our storage systems.</p> <p>We are pleased to make Data Analysis allocations available to researchers from any eligible institution regardless of the source of funding for the planned analysis. Researchers are only required to identify the specific NCAR-hosted data sets that are essential to completing their science objectives. (Because of this expanded eligibility, Data Analysis projects cannot make use of Cheyenne or Derecho.)</p> <p>Data Analysis allocations are made for one year, but may be extended up to two additional years to complete the original project.</p> <p>Requests for the CMIP Analysis Platform, unless the work requires use of NCAR\u2019s HPC resource, fall within the scope of a Data Analysis project.</p> <p>Submit a Data Analysis Allocation Request here.</p>"},{"location":"allocations/university-allocations/#classroom-allocations","title":"Classroom Allocations","text":"<p>Classroom Allocation Requests can be submitted at any time and decisions are generally made within a few days.</p> <p>CISL offers opportunities to undergraduate and graduate faculty and instructors at U.S. 2- and 4-year institutions to use NCAR HPC and analysis resources in their courses in Earth system science and related areas. \u201cSpecial projects\u201d or honors thesis courses under the guidance of a faculty member are eligible.</p> <p>Classroom allocations can also be used for shorter-term training courses and workshops aimed at the university community with the goal of developing a skilled workforce relevant to all aspects of NCAR\u2019s mission.</p> <p>Individual accounts are provided to all students and the instructor(s), so be sure to allow at least a week for setting up accounts. Very large classes may require more setup time. NCAR can provide consulting assistance to the instructors.</p> <p>Classroom allocations are typically made for the duration of the course, plus a reasonable limited period afterwards.</p> <p>Submit a Classroom Allocation Request here.</p>"},{"location":"allocations/university-allocations/#eligibility-details","title":"Eligibility Details","text":"<p>NCAR\u2019s HPC environment represents a significant resource for the U.S. university community. Access to the environment is governed by three primary eligibility criteria.</p>"},{"location":"allocations/university-allocations/#1-earth-system-science-and-related-research","title":"1. Earth system science and related research","text":"<p>To use NCAR resources, a project must be within Earth system science or be Earth system\u2013related research.</p> <p>According to a recent report from the National Academies, \u201cEarth system science\u201d aims to discover and integrate knowledge on the structure, nature, and scales of the dynamics and interactions among natural and social processes throughout the Earth system, which includes the atmosphere, hydrosphere, geosphere, cryosphere, biosphere, as well as the individuals, institutions, and technologies that respond to and influence these dynamics and their interactions and feedback through time.</p> <p>Because scientific progress often relies on contributions from many fields, NCAR resources are also available to support Earth system\u2013related work from other domains \u2013 that is, work that has a demonstrable benefit to or reliance on Earth system science.</p>"},{"location":"allocations/university-allocations/#2-affiliation","title":"2. Affiliation","text":"<p>A prime component of NCAR\u2019s mission is to support atmospheric science at U.S. post-secondary educational institutions. Eligible institutions encompass 2- and 4-year colleges and universities, including community colleges, minority serving institutions (MSIs), and predominantly undergraduate-serving institutions, as well as non-profit research organizations. Recipients of NSF research grants in eligible domains from other types of institutions are also deemed eligible. NCAR resources normally do not support research groups in federal agencies.</p>"},{"location":"allocations/university-allocations/#3-sponsorship","title":"3. Sponsorship","text":"<p>NSF grants. Researchers can apply for an NCAR allocation in support of an associated NSF grant for Earth system science or related research as long as their proposed computing supports the objectives of the grant. The NSF is kept informed to ensure appropriate use of NCAR resources.</p> <p>Unsponsored projects. NCAR provides opportunities for graduate students, postdocs, and new faculty at eligible institutions for work not within the scope of a funded research grant. For this purpose, \u201cnew faculty\u201d includes any faculty member who has not previously had a university allocation at NCAR.</p> <ul> <li> <p>The work must be the individual's own research project rather than a   project assigned by the institution that is hosting the grad student,   postdoc, or faculty member.</p> </li> <li> <p>Their work should not lie within the scope of any funded grant.</p> </li> <li> <p>They must provide a letter from their advisor or department head   commenting on the quality of the proposed research and affirming that   funds are not within the scope of a funded grant.</p> </li> </ul> <p>Unsponsored researchers may also request Data Analysis allocations. These projects cannot use NCAR HPC resources and must make use of NCAR-hosted data sets.</p> <p>Non-NSF funding</p> <p>Due to high demand for NCAR resources at this time, we are unable to provide HPC support to Earth system scientists who have funding solely from non-NSF sources. However, researchers who want to analyze NCAR data sets can request use of NCAR analysis resources regardless of their funding source.</p> <p>The University of Wyoming allocation opportunity has eligibility criteria that permit funding by sources other than NSF, for projects involving U Wyoming collaborators.</p>"},{"location":"allocations/university-allocations/university-large-allocation-request-preparation-instructions/","title":"University Large Allocation Request Preparation Instructions","text":"<p>The next university deadline for submitting Large Allocation Requests is March 13, 2023.</p> <p>Note: In addition to Large Allocation Requests, CISL offers opportunities for NSF awardees, graduate students, and postdocs to request smaller allocations. See the University allocations page for eligibility requirements and other information.</p>"},{"location":"allocations/university-allocations/university-large-allocation-request-preparation-instructions/#submitting-a-large-allocation-request","title":"Submitting a Large Allocation Request","text":"<p>Requesters must prepare and submit a Request Summary document and attach it to the\u00a0Large Allocation Request\u00a0form as a PDF file that is no larger than 5 MB.</p> <p>The instructions below explain how to prepare the document. When your document is ready, submit your request via the Large Allocation Request form.</p>"},{"location":"allocations/university-allocations/university-large-allocation-request-preparation-instructions/#request-summary-document","title":"Request Summary document","text":"<p>The Request Summary document must provide a self-contained description of your project and allocation request. The document may be no more than five (5) pages for Sections A\u2013D below; Sections E\u2013G should be included in the same uploaded document and are permitted an additional five (5) pages. The five-page limit is mandatory for all requests, and it is strongly recommended that you follow the template below to help the panel locate required information in your request.</p> <p>The CHAP Allocation Review Criteria provide further detail on considerations the review panel uses to identify meritorious requests. Recent successful requests are available: Click to download Proposal Sample 1 and Proposal Sample 2.</p> <p>Due to the rapidly growing scale of data generated by many university projects and constraints on the available storage within the CISL environment, the CHAP is scrutinizing user requests for storage resources much more closely. As with poorly justified requests for computing time, poorly justified requests for storage resources will result in reduced storage allocation awards. Review the guidance below for Section D and ensure that your submission addresses the relevant points supporting your efficient, effective, and appropriate use of CISL storage resources. Keep in mind that CISL storage resources are typically available to university projects only for the lifetime of the allocated project and associated NSF award; see Campaign Storage.</p> <p>A. Project information</p> <ul> <li> <p>Title of project</p> </li> <li> <p>Title of NSF award (if different from project title) supporting the   computational experiments. Important: The NSF award must   explicitly support the computational experiments being proposed.</p> </li> <li> <p>NSF award number. Due to high demand for resources, the CHAP no longer   reviews requests for pending awards. Submit your request only after   the NSF has awarded a grant for the research.</p> </li> <li> <p>Name of Project Lead and his or her institution</p> </li> <li> <p>Submission date</p> </li> </ul> <p>B. Project overview</p> <p>The overview of the project should typically be less than half a page and include:</p> <ul> <li> <p>A summary of the science question and computational plan.</p> </li> <li> <p>The relationship of the proposed work to atmospheric and closely   related sciences.</p> </li> <li> <p>The explicit linkage between the NSF award and the computational   experiments being proposed. This is especially important if the   published NSF award abstract does not clearly describe the   computational component of the work funded.</p> </li> </ul> <p>C. Science objectives The science objectives should be described briefly. This section should give sufficient information for understanding the computational plan in section D; it is not necessary to justify the science objectives as they must have already passed NSF review.</p> <p>!!! note \"Advice for preparing your request.     \"Brief\" is the operative word for     your science description. The panel is not judging your science, only     trying to understand how and if your computational experiments     (described in Section D) will help you find answers to your science     questions. This section should be between 1/2 and 1 page long.</p> <p>D. Computational experiments and resource requirements The bulk of the Request Summary should focus on Section D. Discuss your planned computational experiments and the resources needed to conduct the work in this section. Please cover these topics and follow the advice for how to best address each topic.</p>"},{"location":"allocations/university-allocations/university-large-allocation-request-preparation-instructions/#computational-experiments","title":"Computational experiments","text":"<ul> <li> <p>Numerical approach. Briefly describe the numerical approach(es)   or model(s). If a standard community model is being used, simply   explain why it is appropriate for the scientific objectives and   include a reference to a published description of the model or method.   If a community model is being modified, include a description of the   modification sufficient to explain any changes in the computational   cost of the model and explain why modifications are necessary for the   scientific objectives. For a non-standard or non-community model, the   numerical description should briefly describe the approximations and   other methods proposed to obtain valid solutions to the problem.</p> </li> <li> <p>Computational experiments. Describe the computational   experiments needed to address the scientific objectives. Clearly   indicate the number and type of experiments and how they relate to the   objectives. Be sure to justify the model configuration choices   that are relevant to the experiment, such as domain, grid size, time   steps, simulated time span, ensemble size, and parameter choices.   References on the selection of the ensemble size are strongly   encouraged. Without an adequate justification of the model   configuration, the panel may reduce or deny your computing request.</p> </li> <li> <p>Code performance. Include documentation on program code   performance (for example, timings, performance monitoring tools). You   may refer to a web page detailing code performance. Describe how   flexible your code is in the number of processors it can use and why   you may choose a particular number. The CHAP will evaluate the   likelihood that a request can scale up its production runs based on   this information. Information on the portability of the code to other   platforms may also be useful to the CHAP; requesters are strongly   encouraged to provide this information about the code and the team\u2019s   knowledge/use of HPC systems similar to Cheyenne.</p> </li> <li> <p>Output and analysis. Describe the key variables to be output,   plans for analysis, and any steps taken to use storage resources   efficiently and effectively. Projects with large-scale data output   will be scrutinized in greater detail. In addition, the section should   include a brief description of the post-processing calculations to be   carried out and what output needs to be retained beyond the   post-processing stage.</p> </li> </ul>"},{"location":"allocations/university-allocations/university-large-allocation-request-preparation-instructions/#resource-requirements","title":"Resource requirements","text":"<p>Provide a table summarizing the resources required for each experimental configuration and the complete set of computational experiments. This must include the number of core-hours needed and the data output volume. The table should be accompanied by a narrative that elaborates on how you arrived at the numbers in the table and describes how you will use appropriate storage options or data analysis and visualization resources as detailed below.</p> <p>With the petascale systems\u2019 ability to generate vast amounts of data, CISL allocation requests require users to consider their data workflows and to justify their storage resource use. Requesters should note that the CHAP Allocation Review Criteria also apply to requests for allocated storage resources \u2013 that is, GLADE project spaces and Campaign Storage archive use. The CHAP reviewers do not expect lengthy justifications; in most cases, a paragraph that demonstrates forethought commensurate with the scale of anticipated need will suffice. For requests over 50 TB, longer justifications become appropriate.</p> <p>In addressing the CHAP review criteria with respect to storage resources, justify the project\u2019s rationale for which data will be stored, the project\u2019s work plan for managing data, and the project\u2019s need to retain the data to be stored long term. While some statement of storage resource needs is expected in Section D, you may choose to provide details related to longer-term storage needs in Section E (Data Management Plan) to keep sections A-D within the five-page limit. While use of scratch disk space need not be formally justified in the project's allocation request, demonstrating effective use of scratch space within the overall work plan can help reviewers understand your needs for project space or Campaign Storage space.</p> <p>1. HPC. The table should give the core-hours per simulated year or appropriate time period and the total core-hours needed for each experimental configuration as well as the total core-hours for the request. Describe how you arrived at the number of core-hours for each proposed computational experiment. If not provided elsewhere, details on how HPC resource requirements are estimated MUST be included to help reviewers evaluate whether the resources sought are justified and will be used efficiently.</p> <p>Estimating Derecho resource needs.</p> <p>Derecho users can expect to see a 1.3x improvement over the Cheyenne system's performance on a core-for-core basis. Therefore, to estimate how many CPU core-hours will be needed for a project on Derecho, multiply the total for a Cheyenne project by 0.77. When requesting an allocation for Derecho GPU nodes, please make your request in terms of GPU-hours (number of GPUs used x wallclock hours). Derecho GPU-hour estimates can be based on any reasonable GPU performance estimate from another system, including Casper.</p> <p>Estimating Cheyenne core-hours.</p> <p>Cheyenne allocations are made in core-hours. The recommended method for estimating your resource needs is to perform benchmark runs. The core-hours used for a job are calculated by multiplying the number of processor cores used by the wall-clock duration in hours. Cheyenne core-hour calculations should assume that all jobs will run in the regular queue and that they are charged for use of all 36 cores on each node.</p> <p>2. Campaign Storage. In the table showing core-hours, include a column for the final, post-processed, post-analysis amount of data intended for Campaign Storage for each experimental configuration and include the total terabytes planned. Any plans to store raw, unprocessed or temporary data should be justified carefully. Projects with more than 50 TB planned for Campaign Storage are expected to provide more detailed justifications.</p> <p>Justifying archive space requests.</p> <p>A successful justification for archive space would describe, for example, that the data have a meaningful purpose beyond the post-processing phase; that variables or time steps not needed for planned analyses will be filtered out; and that HPC and analysis stages are interleaved where practical to eliminate the need for short-term use of archive space. A simple summation of all bytes generated by all proposed HPC runs may set an upper limit on archive space needs but will not automatically constitute a sufficient justification in and of itself. In most cases, it is not necessary to archive all output; only the subset of data that is critical to the science. Where appropriate, the justification should also describe how the project will reduce archival holdings in subsequent years (for example, a project may have a larger need during peak activity that will decrease in out-years). As with computational justifications, the detail for storage justifications should grow commensurately with the project\u2019s anticipated need.</p> <p>3. Data analysis and visualization (DAV). Describe any planned need for CISL\u2019s DAV clusters to analyze or visualize your results. For standard interactive access to these clusters, we will provide up to 10,000 core-hours with no special justification. Projects with more extensive plans for use of the clusters should justify their needs in a manner similar to their HPC requests based on benchmarks and cost estimates from sample runs.</p> <p>Multi-year plan. While the CHAP makes an effort to award each project its full resource need, very large requests should consider providing a breakdown showing the projected use of core-hours and, if applicable, post-processed data during each year of the allocation. Tie this to the planned computational experiments completed or partially completed each year.</p> <p>Special requirements. Please specify any resource requirements that fall outside of the default environment limits, such as the need for longer job runtime limits, that may affect your ability to complete the proposed computational experiments.</p>"},{"location":"allocations/university-allocations/university-large-allocation-request-preparation-instructions/#additional-supporting-information","title":"Additional supporting information","text":"<p>Sections E through G together must be no more than an additional five pages.</p> <p>E. Data management plan. Consistent with NSF\u2019s requirement that all proposals include a data management plan, summarize your plan for managing the data resulting from this computational work throughout and beyond the period of performance for the NSF award. This section can be used to provide additional details or justification for the storage resource needs, to describe plans for sharing the project\u2019s data, and to summarize any anticipated long-term storage needs. A well-justified data management plan is critical because of the potential for large-scale projects to produce extensive data output.</p> <p>F. Accomplishment report. The accomplishment report should encompass computations performed using CISL resources by the PI or Project Lead. Clearly distinguish accomplishments on this CISL project (that is, for prior CISL use associated with the same NSF award) and accomplishments from all past use of CISL resources. Related work performed on CISL resources by other members of a larger research group may be described, if relevant to this request. Briefly describe the calculations and scientific accomplishments that were completed. Include publications submitted or published that resulted from use of CISL resources. List graduate students who used these computational resources and indicate if these resources supported their thesis or dissertation research. If so, please include the thesis or dissertation title(s).</p> <p>G. References. Please limit to those directly related to the proposed project and referenced in your Request Summary document.</p> <p>H. Figures and captions. Optional. Figures may be embedded within the main body of the Request Summary; embedded figures count against the five-page limit. Figures and charts at the end of the Request Summary will not count against the five-page limit.</p>"},{"location":"compute-systems/jupyter-and-ipython/","title":"Jupyter and IPython","text":"<p>Consider NCAR's JupyterHub instance first!!</p> <p>This page describes an older, manual approach for launching Jupyter on NCAR resources, prior to the deployment of our hosted JupyterHub instance.</p> <p>Still. this approach is valid and may be useful in circumstances where the hosted JupyterHub is under maintenane.</p> <p>The Jupyter package is designed to facilitate interactive computing, especially for code editing, mathematical expressions, plots, code/data visualization, and parallel computing. The IPython kernel is included in the package. Jupyter supports many alternative kernels, also known as language interpreters. See details below.</p> <p>The instructions below describe how to start the browser-based JupyterLab, the IPython shell, and Jupyter QtConsole on the NCAR systems. visualization nodes.</p> <p>For additional information, see Jupyter documentation.</p>"},{"location":"compute-systems/jupyter-and-ipython/#starting-jupyterlab","title":"Starting JupyterLab","text":"<ul> <li> <p>Start an interactive job using the <code>qinteractive</code> command.   (Alternative: Start the job on Casper using <code>execcasper</code> if it   requires GPUs or more memory than is available on Cheyenne.)</p> </li> <li> <p>Load the <code>ncarenv</code> and <code>python</code> modules.</p> </li> <li> <p>Run the <code>ncar_pylib</code> command to load the NCAR Python Library virtual   environment.</p> </li> <li> <p>Run the <code>start-jupyter</code> command.</p> </li> </ul> <p>The output includes instructions like those shown in the image just below. Including the <code>-N</code> after the <code>ssh</code> command establishes the tunneling that allows you to use JupyterLab in your local browser. It also prevents you from actually opening a second ssh session. If you prefer to have a second session open, omit the <code>-N</code>. When you close that session, you will be closing your browser connection to JupyterLab.</p> <p></p> <p>On your local computer, run the <code>ssh</code> command as instructed.</p> <p>The session will appear to hang after you log in. At that point, start <code>http://localhost:nnnn</code> in your browser. (The port numbers may be different from those in the output example above.)</p> <p>JupyterLab will request a password or \"token,\" which is a long string as shown in the output above that you can copy and paste from your terminal.</p> <p></p> <p>Your browser will open the JupyterLab web interface after you log in with the token.</p> <p></p>"},{"location":"compute-systems/jupyter-and-ipython/#related-links","title":"Related links","text":"<ul> <li> <p>Notebook: Extract NECOFS water levels using NetCDF4-Python and analyze/visualize with Pandas</p> </li> <li> <p>Notebook: Access data from the NECOFS (New England Coastal Ocean Forecast System) via OPeNDAP</p> </li> </ul>"},{"location":"compute-systems/jupyter-and-ipython/#starting-ipython-shell","title":"Starting IPython shell","text":"<ul> <li> <p>Start an interactive job using the <code>qinteractive</code> command.   (Alternative: Start the job on Casper using <code>execcasper</code> if it   requires GPUs or more memory than is available on Cheyenne.)</p> </li> <li> <p>Load the <code>ncarenv</code> and <code>python</code> modules.</p> </li> <li> <p>Run the <code>ncar_pylib</code> command to load the NCAR Python Library virtual   environment.</p> </li> <li> <p>Run the <code>ipython</code> command to start the shell.</p> </li> </ul> <p></p>"},{"location":"compute-systems/jupyter-and-ipython/#starting-jupyter-qtconsole","title":"Starting Jupyter QtConsole","text":"<ul> <li> <p>Log in with X tunneling (using the ssh <code>-X</code> option).</p> </li> <li> <p>Start an interactive job using the <code>qinteractive</code> command.   (Alternative: Start the job on Casper using <code>execcasper</code> if it   requires GPUs or more memory than is available on Cheyenne.)</p> </li> <li> <p>Load the <code>ncarenv</code> and <code>python</code> modules.</p> </li> <li> <p>Run the <code>ncar_pylib</code> command to load the NCAR Python Library virtual   environment.</p> </li> <li> <p>Run the <code>jupyter qtconsole</code> command to start the console.</p> </li> </ul> <p></p>"},{"location":"compute-systems/jupyter-and-ipython/#using-alternative-language-kernels","title":"Using alternative language kernels","text":"<p>Jupyter supports multiple language interpreters (known as \"kernels\"). The Python interpreter is loaded by default as the language kernel when using Jupyter, but you can specify use of another kernel when invoking a particular command. To see a list of installed language kernels, run this command: <pre><code>jupyter kernelspec list\n</code></pre></p> <p>To use a kernel, specify it by name when invoking a command. For example, to use the R 3.4.0 interpreter on Cheyenne in the Jupyter QtConsole, enter the following: <pre><code>jupyter qtconsole --kernel=r-3.4\n</code></pre> The console will load with the R command line interpreter active.</p> <p>If you need a language kernel that has not been installed, you can install it yourself in your local directory or contact the NCAR Research Computing help desk to have it added in the system library.</p>"},{"location":"compute-systems/additional-resources/cron/","title":"Cron services","text":"<p>Occasionally users may want to automate a common recurring task.  Typical use cases are to initiate batch jobs, transfer input data from an external site, or run some automated testing. The UNIX <code>cron</code> service allows users to schedule scripts to be run based on a recurrence rule.  As of December 2023 we have deployed a high-availability <code>cron</code> service independent of the individual HPC systems to support these workflows.  This separated, HA solution allows us to perform maintenance on the HPC resources while not interrupting <code>cron</code> workflows that can tolerate the downtime.</p>"},{"location":"compute-systems/additional-resources/cron/#logging-in","title":"Logging in","text":"<p>Once you have an HPC systems account, you can log in to <code>cron.hpc.ucar.edu</code> via <code>ssh</code> command: <pre><code>ssh username@cron.hpc.ucar.edu\n</code></pre> After the usual two-factor authentication process, this will place you on the high-availability <code>cron</code> server.</p>"},{"location":"compute-systems/additional-resources/cron/#cron-server-ip-address","title":"Cron server IP address","text":"<p>For certain automation workflows external sites may need to \"allow\" access from NCAR's systems based on IP address.</p> <ul> <li><code>cron.hpc.ucar.edu</code> : <code>128.117.211.234</code></li> </ul> <p>If you are performing automated connections to remote sites and encounter access issues, it may be necessary to work with the remote site's administrators to add this IP address to their trusted connections configuration (details are site- and process-specific, work with your remote site support team).</p>"},{"location":"compute-systems/additional-resources/cron/#appropriate-usage-user-environment-and-resource-restrictions","title":"Appropriate usage, user environment, and resource restrictions","text":"<p>The primary use case for this resources is to initiate routine, scheduled work that is primarily performed elsewhere, such as in the HPC batch environment on either Derecho or Casper.  As a result, the user software environment is intentionally sparse, and each user is placed into a control group limited to 1GB of system memory to protect system resource utilization. The typical GLADE file systems are accessible, however there is no default software environment provided.</p> <p>Typical usage of this <code>cron</code> resource is:</p> <ol> <li>Interacting with PBS,</li> <li>Performing small, automated file processing activities, and</li> <li>Connecting to the HPC systems directly through <code>ssh</code> to perform additional processing tasks.</li> </ol>"},{"location":"compute-systems/additional-resources/cron/#accessing-pbs-commands","title":"Accessing PBS commands","text":"<p>The typical PBS commands <code>`qsub</code>, <code>qstat</code>, etc... are available by default, and users can access both Derecho and Casper PBS queues from the <code>cron</code> system provided that PBS server names are appended to the usual queue specifications (similar to the usual PBS cross-submission described here):</p> Derecho AccessCasper Access <p>Command-line specification of a Derecho queue: <pre><code>cron$ qsub -q main@desched1 [...]\n</code></pre> PBS Script specification of a Derecho queue: <pre><code>#PBS -q main@desched1\n</code></pre></p> <p>Command-line specification of a Casper queue: <pre><code>cron$ qsub -q casper@casper-pbs [...]\n</code></pre> PBS Script specification of a Casper queue: <pre><code>#PBS -q casper@casper-pbs\n</code></pre></p>"},{"location":"compute-systems/additional-resources/cron/#connecting-to-other-ncar-hpc-resources","title":"Connecting to other NCAR HPC resources","text":"<p>The <code>cron</code> servers are trusted by other HPC resources, allowing users to <code>ssh</code> to other systems without additional two-factor authentication.  A common workflow then is for a small, lightweight script to be initiated on the <code>cron</code> servers which in turn runs additional commands on Derecho or Casper.</p> Derecho AccessCasper Access <pre><code># ssh to a Derecho login node and do something useful...\ncron$ ssh derecho \"hostname &amp;&amp; uptime\"\n</code></pre> <pre><code># ssh to a Casper login node and do something useful...\ncron$ ssh casper \"hostname &amp;&amp; uptime\"\n</code></pre>"},{"location":"compute-systems/additional-resources/cron/#installing-and-editing-crontab-entries","title":"Installing and editing <code>crontab</code> entries","text":"<p>To schedule a process with <code>cron</code>, a user must establish a <code>crontab</code> entry. This can be done interactively by running the command <code>crontab -e</code> to edit your crontab directly, or by creating a file and \"installing\" it with <code>crontab &lt;filename&gt;</code>. Additionally, you can list your current crontab entries via <code>crontab -l</code>.  (See <code>man crontab</code> for more details.)</p> <p>In either case, the <code>crontab</code> entry has a very particular, fixed format.</p>"},{"location":"compute-systems/additional-resources/cron/#crontab-syntax","title":"<code>crontab</code> syntax","text":"<p>sample crontab entry format<pre><code># \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 minute (0\u201359)\n# \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 hour (0\u201323)\n# \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 day of the month (1\u201331)\n# \u2502 \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 month (1\u201312)\n# \u2502 \u2502 \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 day of the week (0\u20136) (Sunday to Saturday);\n# \u2502 \u2502 \u2502 \u2502 \u2502\n# \u2502 \u2502 \u2502 \u2502 \u2502\n# \u2502 \u2502 \u2502 \u2502 \u2502\n  * * * * * &lt;command to execute&gt;\n</code></pre> That is, 5 fields defining the recurrence rule, and a command to execute.  The syntax also supports ranges and stepping values. Some examples:</p> <p>sample crontab entries<pre><code># run every 15 minutes:\n*/15 * * * * &lt;my rapid command&gt;\n\n# run every night at 23:04 (11:04 PM):\n4 23 * * * &lt;my daily command&gt;\n\n# 09:23 on every day-of-week from Monday through Friday.\n23 9 * * 1-5 &lt;my weekday commands&gt;\n\n# the first day of every-other month\n0 0 1 */2 * &lt;my infrequent command&gt;\n</code></pre> The crontab guru is a helpful resource for translating crontab time syntax into human-friendly time specifications.</p>"},{"location":"compute-systems/additional-resources/cron/#crontab-commands","title":"<code>crontab</code> commands","text":"<p>Keep  your <code>crontab</code> commands as simple as possible, and do not make any assumptions regarding the execution environment (initial working directories, environment variables, etc...). We also recommend redirecting script output to aid in monitoring and debugging.   The command can be a short sequence of commands chained together with the shell operator <code>&amp;&amp;</code> if desired, for example: <pre><code># run every night at 23:04 (11:04 PM):\n4 23 * * * cd /glade/work/&lt;username&gt;/my_cron_stuff/ &amp;&amp; ./run_nightly.sh &amp;&gt;&gt; ./run_nightly.log\n</code></pre> This will run the command <code>run_nightly.sh</code> from within the directory <code>/glade/work/&lt;username&gt;/my_cron_stuff/</code>, appending both standard output and standard error into the file <code>run_nightly.log</code>.</p>"},{"location":"compute-systems/additional-resources/cron/#best-practices-for-cron-jobs","title":"Best practices for cron jobs","text":""},{"location":"compute-systems/additional-resources/cron/#locking-for-exclusive-execution","title":"Locking for exclusive execution","text":"<p>Many shell scripts are not designed to be run concurrently, and even for those that are, concurrent execution is often not the users' intent.  In such scenarios the user should make provisions so that only one occurrence of the script is running at a time.</p> <p>File locking is a convenient mechanism to implement this behavior, whereby a process will only run after it has gained exclusive access to a resource - in this case a \"lock file\". This approach is particularly useful under <code>cron</code> - if a particular instance of a script is running slow, <code>cron</code> could re-launch the same script potentially many times.  File locking prevents such script \"pile-up.\"</p> <p>The utility <code>lockfile</code> can be incorporated into shell scripts as follows:</p> using lockfile to prevent concurrent execution<pre><code>LOCK=\"${HOME}/.my_cron_job.lock\"\n\nremove_lock()\n{\n    rm -f \"${LOCK}\"\n}\n\nanother_instance()\n{\n    echo \"Cannot acquire lock on ${LOCK}\"\n    echo \"There is another instance running, exiting\"\n    exit 1\n}\n\nlockfile -r 5 -l 3600 \"${LOCK}\" || another_instance\ntrap remove_lock EXIT\n</code></pre> <p>We declare two utility functions: <code>remove_lock</code> and <code>another_instance</code>.  The command <code>lockfile</code> will attempt to gain exclusive access to our <code>${LOCK}</code> file, retrying 5 times (<code>-r 5</code>).  If we cannot acquire the desired lock, <code>another_instance</code> prints some information and exits the script. Note that when the script exits - cleanly or not - we want to remove our lock file.  We use the bash <code>trap</code> mechanism to accomplish this by calling <code>remove_lock</code> at <code>EXIT</code>.</p> <p>In this case we forcibly remove any \"stale\" lock files when more than an hour old (3,600 seconds, <code>-l 3600</code>) as defensive measure.  This would be appropriate for a short running script, when we can assume any leftover lock files beyond some threshold age are invalid. See <code>man lockfile</code> for additional options.</p>"},{"location":"compute-systems/additional-resources/cron/#pedantic-error-checking","title":"Pedantic error checking","text":"<p>It is always a good idea to perform error checking inside shell scripts, but especially so when running under cron.  For example, when changing directories inside a script: <pre><code># go to desired directory, exit on failure:\ncd /glade/work/${USER}/mydir || exit 1\n[...]\n</code></pre> This will abort the job if the <code>cd</code> fails.  (The <code>|| exit 1</code> construct is executed only if the first command exits with a failure status.) Without this type of pedantic error checking the script would continue to run, and the remainder of the script would attempt to run, but in the wrong directory - especially dangerous if later steps remove files!!</p>"},{"location":"compute-systems/additional-resources/cron/#logging","title":"Logging","text":"<p>In addition to any typical logging of expected output from your automated processes, it is beneficial to capture some information from the system as well.</p> <p>For example, logging the execution environment<pre><code>timestamp=\"$(date +%F@%H:%M)\"\ncron_logdir=\"${HOME}/.my_cron_logs/\"\nscriptdir=\"$( cd \"$( dirname \"${BASH_SOURCE[0]}\" )\" &gt;/dev/null 2&gt;&amp;1 &amp;&amp; pwd )\"\nmkdir -p ${cron_logdir} || exit 1\necho \"[${timestamp}]: Running ${0} on $(hostname) from $(pwd); scriptdir=${scriptdir}\" \\\n    | tee -a ${cron_logdir}/particular_tool.log\n</code></pre> creates the following output: <pre><code>[2023-12-11@14:33]: Running ./sample_cron_job.sh on derecho6 from [...]; scriptdir=[...]\n</code></pre> which can be useful in the future; particularly many years from now if <code>cron</code> stops working for you and you have forgotten where your <code>cron</code> scripts were located.</p>"},{"location":"compute-systems/additional-resources/cron/#sample-cron-script-and-crontab-processes","title":"Sample Cron script and <code>crontab</code> processes","text":"<p>Sample Cron script and <code>crontab</code> installation processes</p> <p>Our <code>cron_driver.sh</code> script will submit two PBS batch jobs: one for prepossessing to run on Casper, and another model execution to run on Derecho. cron_driver.sh<pre><code>#!/bin/bash -l\n\n#--------------------------------------------------------------\nLOCK=\"${HOME}/.my_cron_job.lock\"\nremove_lock()\n{\n    rm -f \"${LOCK}\"\n}\n\nanother_instance()\n{\n    echo \"Cannot acquire lock on ${LOCK}\"\n    echo \"There is another instance running, exiting\"\n    exit 1\n}\n\n# acquire an exclusive lock on our ${LOCK} file to make sure\n# only one copy of this script is running at a time.\nlockfile -r 5 -l 3600 \"${LOCK}\" || another_instance\ntrap remove_lock EXIT\n\n#--------------------------------------------------------------\n# logging:  Print status to standard out, and redirect also to our\n# specified cron_logdir.\ntimestamp=\"$(date +%F@%H:%M)\"\ncron_logdir=\"${HOME}/.my_cron_logs/\"\nscriptdir=\"$( cd \"$( dirname \"${BASH_SOURCE[0]}\" )\" &gt;/dev/null 2&gt;&amp;1 &amp;&amp; pwd )\"\nmkdir -p ${cron_logdir} || exit 1\n\necho -e \"[${timestamp}]: Running ${0} on $(hostname)\\n\\tfrom $(pwd)\\n\\tscriptdir=${scriptdir}\" \\\n    | tee -a ${cron_logdir}/sample_job.log\n\n#--------------------------------------------------------------\n# go to desired directory, exit on failure:\ncd /glade/work/${USER}/my_cron_job/ || { echo \"cannot cd to the desired directory!!\"; exit 1; }\n\n# clean up any old INPUT_DATA\nrm -f ./INPUT_DATA\n\n# lauch preprocessing job, capturing its job ID\nPREREQ=$(qsub -q casper@casper-pbs ./prep_job.pbs) || { echo \"cannot connect to Casper PBS\"; exit 1; }\necho ${PREREQ}\n\n# lauch model job\nqsub -q main@desched1 -W depend=afterok:${PREREQ} ./run_model.pbs || { echo \"cannot connect to Derecho PBS\"; exit 1; }\n</code></pre> The script begins by establishing an exclusive file lock, performing some logging, and then moving to the intended run directory. The first job (<code>prep_job.pbs</code>) will create a file called <code>INPUT_DATA</code> which is used by the second job (<code>run_model.pbs</code>).  Because the second job depends on the first, we submit the second using a PBS job dependency.</p> <p>We also remove the <code>INPUT_DATA</code> at the beginning of the script - since it should be created by <code>prep_job.pbs</code>, we want to make sure that step functioned as intended.  In this way if <code>INPUT_DATA</code> exists when we execute <code>run_model.pbs</code> it can only be because the preceding step ran successfully - and our <code>INPUT_DATA</code> is not stale.</p> <p>Sample <code>crontab</code> entries</p> <p>First we prepare a simple text file that contains the entries for all our desired cron processes: my_crontab:<pre><code># run my frequent cron job every 15 minutes\n*/15 * * * * cd /glade/u/home/benkirk/my_cron/ &amp;&amp; mkdir -p logs &amp;&amp; ./frequent.sh &amp;&gt;&gt; logs/frequent.log\n\n# run my hourly cron jobs at 10 minutes past every hour\n10 * * * * cd /glade/u/home/benkirk/my_cron/ &amp;&amp; mkdir -p logs &amp;&amp; ./hourly.sh &amp;&gt;&gt; logs/hourly.log\n\n# run my daily cron jobs at 00:45 am.\n45 0 * * * cd /glade/u/home/benkirk/my_cron/ &amp;&amp; mkdir -p logs &amp;&amp; ./daily.sh &amp;&gt;&gt; logs/daily.log\n</code></pre></p> <p>Installing <code>crontab</code> entries</p> <p>We can then install, inspect, and edit our entries with the <code>crontab</code> command. <pre><code># Install my_crontab:\ncron$ crontab ./my_crontab\n</code></pre> Inspecting <code>crontab</code> entries</p> <pre><code># Inspect installed crontab:\ncron$ crontab -l\n# run my frequent cron job every 15 minutes\n*/15 * * * * cd /glade/u/home/benkirk/my_cron/ &amp;&amp; mkdir -p logs &amp;&amp; ./frequent.sh &amp;&gt;&gt; logs/frequent.log\n\n# run my hourly cron jobs at 10 minutes past every hour\n10 * * * * cd /glade/u/home/benkirk/my_cron/ &amp;&amp; mkdir -p logs &amp;&amp; ./hourly.sh &amp;&gt;&gt; logs/hourly.log\n\n# run my daily cron jobs at 00:45 am.\n45 0 * * * cd /glade/u/home/benkirk/my_cron/ &amp;&amp; mkdir -p logs &amp;&amp; ./daily.sh &amp;&gt;&gt; logs/daily.log\n</code></pre> <p>Editing <code>crontab</code> entries <pre><code># edit my crontab\n# (uses the default editor, or whatever is specified by the EDITOR environment variable)\ncron$ crontab -e\n</code></pre></p>"},{"location":"compute-systems/additional-resources/thunder-test-system/","title":"Thunder test system","text":"<p>The Thunder cluster is a test system that features Marvell's ThunderX2 Arm processors. These processors use the <code>aarch64</code> instruction set, rather than the <code>x86-64</code> instruction set used by Intel and AMD processors.</p> <p>Thunder consists of:</p> <ul> <li> <p>one node with 128 GB of memory</p> </li> <li> <p>four nodes with 256 GB of memory</p> </li> <li> <p>100GbE Mellanox Ethernet interconnect</p> </li> </ul> <p>Follow the procedures below to begin using the system.</p> <p>Additional information and screen captures depicting various steps in the process are included in this slide presentation:</p> <p>ChameleonCloud-WIP-09212022.pdf</p>"},{"location":"compute-systems/additional-resources/thunder-test-system/#getting-started","title":"Getting started","text":"<p>Users access the Thunder nodes by establishing a connection to them via Chameleon Cloud.</p> <p>To get started:</p> <ul> <li> <p>Create a Chameleon Cloud account.</p> </li> <li> <p>Email hpcrd@ucar.edu and request access to the NCARExplore   project.</p> </li> </ul>"},{"location":"compute-systems/additional-resources/thunder-test-system/#accessing-thunder","title":"Accessing Thunder","text":"<p>Once you have been added to the NCARExplore project, access the Thunder nodes by going to CHI@NCAR and logging into your Chameleon account. From there you'll be able to lease nodes and start up instances.</p>"},{"location":"compute-systems/additional-resources/thunder-test-system/#leasing-thunder-nodes","title":"Leasing Thunder nodes","text":"<p>An individual user can request or lease one or more nodes for up to seven (7) days and create an IP address for accessing them. Follow these steps:</p> <ol> <li> <p>From the Project menu, select Reservations, then Leases.</p> </li> <li> <p>Select the Create Lease button.</p> </li> <li> <p>Complete the General section by specifying a name (required), start     time (defaults to now), and the length of your lease (defaults to 1     day).</p> </li> <li> <p>Complete the Hosts section by checking the Reserve Hosts box and     selecting the minimum and maximum number of hosts (both default to     1). There is only one type of node on Thunder, so there is no need     to select Resource Properties.</p> </li> <li> <p>Complete the Networks section by checking the Reserve Floating IPs     box and specifying the number of Floating IP addresses you want to     reserve (typically 1). There is only one physical network on Thunder     controlled by Chameleon, so there is no need to select Reserve     Network.</p> </li> </ol> <p></p> <p>Once you have entered all your selections, select the Create button.</p>"},{"location":"compute-systems/additional-resources/thunder-test-system/#launching-an-instance","title":"Launching an instance","text":"<p>When your lease status is ACTIVE, you can launch an instance on your leased Thunder nodes.</p> <ol> <li> <p>From the Project menu, select Compute, then Instances.</p> </li> <li> <p>Select the Launch Instance button.</p> </li> <li> <p>Complete the Details section by specifying an Instance Name,     selecting your lease in the Reservation box, and selecting the     number of instances you want to start in the Count box (defaults     to 1).</p> </li> <li> <p>Complete the Source section by choosing an image: Click the ^     next to the CC-Ubuntu20.04-ARM64 RAW image to choose the default     image.</p> </li> <li> <p>Complete the Key Pair section by selecting either the Create Key     Pair button or the Import KeyPair button. This key will be used     to log in to your instance using the \"cc\" user account. If you have     already uploaded a key pair to <code>CHI@NCAR</code>, that key pair will already     be selected as the default.</p> </li> </ol> <p>Once you have entered all your selection, select Launch Instance.</p>"},{"location":"compute-systems/additional-resources/thunder-test-system/#accessing-your-instance","title":"Accessing your instance","text":"<p>Once your instance has been provisioned and starts (status is Active), select Floating IPs on the Network menu, then the Associate button next to your reserved IP address.</p> <p>Associate your floating IP address with a port by selecting your instance under \"Port to be associated.\" (See image).</p> <p></p> <p>Once the status of that association is \"Up\" you can use your IP address and your key to <code>ssh</code> to your active instance by following this example (substituting your own IP address): <pre><code>ssh cc@128.117.250.23\n</code></pre></p> <p>At that point, you can use the reserved Thunder nodes for your work.</p>"},{"location":"compute-systems/additional-resources/thunder-test-system/#more-information-and-getting-help","title":"More information and getting help","text":"<p>Detailed Chameleon Cloud documentation is available here.</p> <p>Since Thunder is a test system, send any support inquiries, software and hardware concerns, or requests for access to hpcfl@ucar.edu instead of the CISL Help Desk.</p> <p>We also welcome any feedback and performance reports that you can share as you test your workflows on Thunder.</p>"},{"location":"compute-systems/casper/","title":"Casper cluster","text":"<p>The Casper cluster is a system of specialized data analysis and visualization resources; large-memory, multi-GPU nodes; and high-throughput computing nodes.</p> <p>Casper is composed of over 100 nodes featuring a mixture of Intel and AMD processors, with a variety of NVIDIA General Purpose Graphical Processing Units.</p> <p>Please refer to the hardware summary table below for detailed specifications.</p>"},{"location":"compute-systems/casper/#quick-start","title":"Quick Start","text":""},{"location":"compute-systems/casper/#logging-in","title":"Logging in","text":"<p>Once you have an account, have reviewed the Casper Use Policies, and have a Casper resource allocation you can log in and run jobs on the Casper data analysis and visualization cluster.</p> <p>To log in, start your terminal or Secure Shell client and run an ssh command as shown here: <pre><code>ssh -X username@casper.hpc.ucar.edu\n</code></pre></p> <p>Some users (particularly on Macs) need to use <code>-Y</code> instead of <code>-X</code> when calling SSH to enable X11 forwarding.</p> <p>You can omit <code>username</code> in the command above if your Casper username is the same as your  username on your local computer.</p> <p>After running the <code>ssh</code> command, you will be asked to authenticate to finish logging in.</p> <p>Casper has full access to NCAR storage resources, including GLADE. Users can transfer data to and from Casper.</p> <p>To run data analysis and visualization jobs on the Casper system's nodes, follow the procedures described here. There is no need to transfer output files from Derecho for this since Derecho and Casper mount the same <code>GLADE</code> file systems.</p> <p>Don\u2019t run <code>sudo</code> on NCAR systems!</p> <p>If you need help with tasks that you think require <code>sudo</code> privileges, or if you aren\u2019t sure, please contact HPC User Support before trying to run <code>sudo</code> yourself. The command fails when unauthorized users run it and sends a security alert to system administrators.</p>"},{"location":"compute-systems/casper/#environment","title":"Environment","text":"<p>The Casper HPC system uses OpenSUSE Linux Version 15 and supports widely used shells on its login and compute nodes. Users also have several compiler and MPI library choices.</p>"},{"location":"compute-systems/casper/#shells","title":"Shells","text":"<p>The default login shell for new Casper users is <code>bash</code>. You can change the default after logging in to the Systems Accounting Manager (SAM). It may take several hours for a change you make to take effect. You can confirm which shell is set as your default by entering <code>echo $SHELL</code> on your Casper command line.</p>"},{"location":"compute-systems/casper/#environment-modules","title":"Environment modules","text":"<p>The Casper <code>module</code> utility enables users to easily load and unload compilers and compatible software packages as needed, and to create multiple customized environments for various tasks. See the Environment modules page for a general discussion of <code>module</code> usage.  Casper's default module environment is listed here.</p>"},{"location":"compute-systems/casper/#accessing-software-and-compiling-code","title":"Accessing software and compiling code","text":"<p>Casper users have access to Intel, NVIDIA, and GNU compilers. The Intel compiler and OpenMPI modules are loaded by default and provide access to pre-compiled HPC Software and Data Analysis and Visualization Resources.</p> <p>See this page for a full discussion of compiling on Casper.</p> <p>Many Casper data analysis and AI/ML workflows benefit instead from using Conda, especially NCAR's Python Library (NPL) or to gain access to several Machine Learning Frameworks.</p>"},{"location":"compute-systems/casper/#running-jobs-on-casper","title":"Running jobs on Casper","text":"<p>Users can run a variety of types of jobs on Casper, including both traditional batch jobs submitted through PBS and also interactive and/or graphics-intensive analysis, often through remote desktops on Casper.</p>"},{"location":"compute-systems/casper/#job-scripts","title":"Job scripts","text":"<p>Job scripts are discussed broadly here. Users already familiar with PBS and batch submission may find Casper-specific PBS job scripts helpful in porting their work.</p>"},{"location":"compute-systems/casper/#casper-hardware","title":"Casper hardware","text":"Data Analysis&amp; Visualization nodes 22 Supermicro 7049GP-TRT SuperWorkstation nodes         Up to 384 GB DDR4-2666 memory per node         2 18-core 2.3-GHz Intel Xeon Gold 6140 (Skylake) processors per node         2 TB local NVMe Solid State Disk         1 Mellanox ConnectX-4 100Gb Ethernet connection (GLADE, Campaign Storage, external connectivity)         1 Mellanox ConnectX-6 HDR100 InfiniBand link         1 NVIDIA Quadro GP100 GPU 16GB PCIe on each of 9 nodes         1 NVIDIA Ampere A100 GPU 40 GB PCIe on each of 3 nodes        Machine Learning/Deep Learning &amp; General Purpose GPU (GPGPU) nodes 4 Supermicro SuperServer nodes with 4 V100 GPUs         768 GB DDR4-2666 memory per node         2 18-core 2.6-GHz Intel Xeon Gold 6240 (Cascade Lake) processors per node         2 TB local NVMe Solid State Disk         1 Mellanox ConnectX-4 100Gb Ethernet connection (GLADE, Campaign Storage, external connectivity)         2 Mellanox ConnectX-6 HDR200 InfiniBand adapters. HDR100 link on each CPU socket         4 NVIDIA Tesla V100 32GB SXM2 GPUs with NVLink 6 Supermicro SuperServer nodes with 8 V100 GPUs         1152 GB DDR4-2666 memory per node         2 18-core 2.6-GHz Intel Xeon Gold 6240 (Cascade Lake) processors per node         2 TB local NVMe Solid State Disk         1 Mellanox ConnectX-4 100Gb Ethernet connection (GLADE, Campaign Storage, external connectivity)         2 Mellanox ConnectX-6 HDR200 InfiniBand adapters, HDR100 link on each CPU socket         8 NVIDIA Tesla V100 32GB SXM2 GPUs with NVLink 4 Supermicro nodes with 4 A100 GPUs         1024 GB memory per node         2 64-core 2.45-GHz AMD EPYC Milan 7763 processors per node         1.5 TB local NVMe Solid State Disk         4 Mellanox ConnectX-6 network adapters         4 NVIDIA Ampere A100 80GB SXM4 GPUs with NVLink High-Throughput Computing nodes 62 small-memory workstation nodes         384 GB DDR4-2666 memory per node          2 18-core 2.6-GHz Intel Xeon Gold 6240 (Cascade Lake) processors per node         1.6 TB local NVMe Solid State Disk         1 Mellanox ConnectX-5 100Gb Ethernet VPI adapter (GLADE, Campaign Storage, external connectivity)         1 Mellanox ConnectX-6 HDR200 InfiniBand VPI adapter. HDR100 link on each CPU socket 2 large-memory workstation nodes         1.5 TB DDR4-2666 memory per node          2 18-core 2.3-GHz Intel Xeon Gold 6240 (Cascade Lake) processors per node         1.6 TB local NVMe Solid State Disk         1 Mellanox ConnectX-5 100Gb Ethernet VPI adapter (GLADE, Campaign Storage, external connectivity)         1 Mellanox ConnectX-6 HDR200 InfiniBand VPI adapter, HDR100 link on each CPU socket        Research Data Archivenodes (reserved forRDA use) 4 Supermicro Workstation nodes         94 GB DDR4-2666 memory per node         2 16-core 2.3-GHz Intel Xeon Gold 5218 (Cascade Lake) processors per node         1.92 TB local Solid State Disk         1 Mellanox ConnectX-6 VPI 100Gb Ethernet connection (GLADE, Campaign Storage, internal connectivity)"},{"location":"compute-systems/casper/#status","title":"Status","text":""},{"location":"compute-systems/casper/#nodes","title":"Nodes","text":""},{"location":"compute-systems/casper/#gpu-usage","title":"GPU Usage","text":""},{"location":"compute-systems/casper/#queues","title":"Queues","text":""},{"location":"compute-systems/casper/casper-modules-list/","title":"Casper modules list","text":""},{"location":"compute-systems/casper/casper-modules-list/#casper-default-module-environment","title":"Casper default module environment","text":"<pre><code>-------------------------- Module Stack Environments ---------------------------\n   ncarenv/23.09 (S,L)\n\n------------------------- Compilers and Core Software --------------------------\n   apptainer/1.1.9               ncl/6.6.2\n   cdo/2.2.2                     nco/5.1.6\n   charliecloud/0.33             ncview/2.1.9\n   clang/16.0.6                  ncvis/2022.08.28\n   cmake/3.26.3                  nvhpc/23.7\n   conda/latest                  octave/8.2.0\n   cuda/11.8.0            (L)    paraview/5.11.1\n   cudnn/8.7.0.84-11.8           pcre/8.45\n   darshan-util/3.4.2            peak-memusage/3.0.1\n   doxygen/1.8.20                perl/5.38.0\n   eigen/3.4.0                   pocl/3.0\n   gcc/12.2.0                    podman/4.5.1\n   go/1.20.6                     texlive/20220321\n   grads/2.2.3                   ucx/1.14.1          (L)\n   grib-util/1.2.4               vapor/3.9.0\n   intel-classic/2023.2.1        vexcl/1.4.3\n   intel-oneapi/2023.2.1         visit/3.3.3\n   intel/2023.2.1         (L)    vtune/2023.2.0\n   julia/1.9.2                   wgrib2/3.1.1\n   linaro-forge/23.0             xxdiff/latest\n   nccmp/1.9.1.0\n\n-------------------- Compiler-dependent Software - [oneapi] --------------------\n   eccodes/2.25.0    hdf5/1.12.2         (L)    netcdf/4.9.2   (L)\n   fftw/3.3.10       mkl/2023.2.0               openmpi/4.1.5  (L)\n   geos/3.9.1        mpi-serial/2.3.0           proj/8.2.1\n   hdf/4.2.15        ncarcompilers/1.0.0 (L)    udunits/2.2.28\n\n----------------- MPI-dependent Software - [oneapi + openmpi] ------------------\n   adios2/2.9.1             fftw-mpi/3.3.10     parallel-netcdf/1.12.3\n   darshan-runtime/3.4.2    hdf5-mpi/1.12.2     parallelio/2.6.1\n   esmf/8.5.0               netcdf-mpi/4.9.2    parallelio/2.6.2       (D)\n\n  Where:\n   D:  Default Module\n   L:  Module is loaded\n   S:  Module is Sticky, requires --force to unload or purge\n\nIf the avail list is too long consider trying:\n\n\"module --default avail\" or \"ml -d av\" to just list the default modules.\n\"module overview\" or \"ml ov\" to display the number of modules for each name.\n\nUse \"module spider\" to find all possible modules and extensions.\nUse \"module keyword key1 key2 ...\" to search for all possible modules matching\nany of the \"keys\".\n</code></pre>"},{"location":"compute-systems/casper/casper-modules-list/#casper-complete-module-listing","title":"Casper complete module listing","text":"<pre><code>----------------------------------------------------------------------------\nThe following is a list of the modules and extensions currently available:\n----------------------------------------------------------------------------\n  adios2: adios2/2.9.1\n  apptainer: apptainer/1.1.9\n  cdo: cdo/2.2.2\n  charliecloud: charliecloud/0.33\n  clang: clang/16.0.6\n  cmake: cmake/3.26.3\n  conda: conda/latest\n  cuda: cuda/11.8.0\n  cudnn: cudnn/8.7.0.84-11.8\n  darshan-runtime: darshan-runtime/3.4.2\n  darshan-util: darshan-util/3.4.2\n  doxygen: doxygen/1.8.20\n  eccodes: eccodes/2.25.0\n  eigen: eigen/3.4.0\n  esmf: esmf/8.5.0\n  fftw: fftw/3.3.10\n  fftw-mpi: fftw-mpi/3.3.10\n  gcc: gcc/12.2.0\n  gdal: gdal/3.7.1\n  geos: geos/3.9.1\n  go: go/1.20.6\n  grads: grads/2.2.3\n  grib-util: grib-util/1.2.4\n  hdf: hdf/4.2.15\n  hdf5: hdf5/1.12.2\n  hdf5-mpi: hdf5-mpi/1.12.2\n  intel: intel/2023.2.1\n  intel-classic: intel-classic/2023.2.1\n  intel-oneapi: intel-oneapi/2023.2.1\n  julia: julia/1.9.2\n  linaro-forge: linaro-forge/23.0\n  mkl: mkl/2023.2.0\n  mpi-serial: mpi-serial/2.3.0\n  ncarcompilers: ncarcompilers/1.0.0\n  ncarenv: ncarenv/23.09\n  nccmp: nccmp/1.9.1.0\n  ncl: ncl/6.6.2\n  nco: nco/5.1.6\n  ncview: ncview/2.1.9\n  ncvis: ncvis/2022.08.28\n  netcdf: netcdf/4.9.2\n  netcdf-mpi: netcdf-mpi/4.9.2\n  nvhpc: nvhpc/23.7\n  octave: octave/8.2.0\n  openblas: openblas/0.3.23\n  openmpi: openmpi/4.1.5\n  parallel-netcdf: parallel-netcdf/1.12.3\n  parallelio: parallelio/2.6.1, parallelio/2.6.2\n  paraview: paraview/5.11.1\n  pcre: pcre/8.45\n  peak-memusage: peak-memusage/3.0.1\n  perl: perl/5.38.0\n  pocl: pocl/3.0\n  podman: podman/4.5.1\n  proj: proj/8.2.1\n  texlive: texlive/20220321\n  ucx: ucx/1.14.1\n  udunits: udunits/2.2.28\n  vapor: vapor/3.9.0\n  vexcl: vexcl/1.4.3\n  visit: visit/3.3.3\n  vtune: vtune/2023.2.0\n  wgrib2: wgrib2/3.1.1\n  xxdiff: xxdiff/latest\n----------------------------------------------------------------------------\nTo learn more about a package execute:\n   $ module spider Foo\nwhere \"Foo\" is the name of a module.\nTo find detailed information about a particular package you\nmust specify the version if there is more than one version:\n   $ module spider Foo/11.1\n----------------------------------------------------------------------------\n</code></pre>"},{"location":"compute-systems/casper/casper-modules/","title":"Casper Modules","text":""},{"location":"compute-systems/casper/casper-modules/#casper-default-module-environment","title":"Casper default module environment","text":"<pre><code>-------------------------- Module Stack Environments ---------------------------\n   ncarenv/23.09 (S,L)\n\n------------------------- Compilers and Core Software --------------------------\n   apptainer/1.1.9               ncl/6.6.2\n   cdo/2.2.2                     nco/5.1.6\n   charliecloud/0.33             ncview/2.1.9\n   clang/16.0.6                  ncvis/2022.08.28\n   cmake/3.26.3                  nvhpc/23.7\n   conda/latest                  octave/8.2.0\n   cuda/11.8.0            (L)    paraview/5.11.1\n   cudnn/8.7.0.84-11.8           pcre/8.45\n   darshan-util/3.4.2            peak-memusage/3.0.1\n   doxygen/1.8.20                perl/5.38.0\n   eigen/3.4.0                   pocl/3.0\n   gcc/12.2.0                    podman/4.5.1\n   go/1.20.6                     texlive/20220321\n   grads/2.2.3                   ucx/1.14.1          (L)\n   grib-util/1.2.4               vapor/3.9.0\n   intel-classic/2023.2.1        vexcl/1.4.3\n   intel-oneapi/2023.2.1         visit/3.3.3\n   intel/2023.2.1         (L)    vtune/2023.2.0\n   julia/1.9.2                   wgrib2/3.1.1\n   linaro-forge/23.0             xxdiff/latest\n   nccmp/1.9.1.0\n\n-------------------- Compiler-dependent Software - [oneapi] --------------------\n   eccodes/2.25.0    hdf5/1.12.2         (L)    netcdf/4.9.2   (L)\n   fftw/3.3.10       mkl/2023.2.0               openmpi/4.1.5  (L)\n   geos/3.9.1        mpi-serial/2.3.0           proj/8.2.1\n   hdf/4.2.15        ncarcompilers/1.0.0 (L)    udunits/2.2.28\n\n----------------- MPI-dependent Software - [oneapi + openmpi] ------------------\n   adios2/2.9.1             fftw-mpi/3.3.10     parallel-netcdf/1.12.3\n   darshan-runtime/3.4.2    hdf5-mpi/1.12.2     parallelio/2.6.1\n   esmf/8.5.0               netcdf-mpi/4.9.2    parallelio/2.6.2       (D)\n\n  Where:\n   D:  Default Module\n   L:  Module is loaded\n   S:  Module is Sticky, requires --force to unload or purge\n\nIf the avail list is too long consider trying:\n\n\"module --default avail\" or \"ml -d av\" to just list the default modules.\n\"module overview\" or \"ml ov\" to display the number of modules for each name.\n\nUse \"module spider\" to find all possible modules and extensions.\nUse \"module keyword key1 key2 ...\" to search for all possible modules matching\nany of the \"keys\".\n</code></pre>"},{"location":"compute-systems/casper/casper-modules/#casper-complete-module-listing","title":"Casper complete module listing","text":"<pre><code>----------------------------------------------------------------------------\nThe following is a list of the modules and extensions currently available:\n----------------------------------------------------------------------------\n  adios2: adios2/2.9.1\n  apptainer: apptainer/1.1.9\n  cdo: cdo/2.2.2\n  charliecloud: charliecloud/0.33\n  clang: clang/16.0.6\n  cmake: cmake/3.26.3\n  conda: conda/latest\n  cuda: cuda/11.8.0\n  cudnn: cudnn/8.7.0.84-11.8\n  darshan-runtime: darshan-runtime/3.4.2\n  darshan-util: darshan-util/3.4.2\n  doxygen: doxygen/1.8.20\n  eccodes: eccodes/2.25.0\n  eigen: eigen/3.4.0\n  esmf: esmf/8.5.0\n  fftw: fftw/3.3.10\n  fftw-mpi: fftw-mpi/3.3.10\n  gcc: gcc/12.2.0\n  gdal: gdal/3.7.1\n  geos: geos/3.9.1\n  go: go/1.20.6\n  grads: grads/2.2.3\n  grib-util: grib-util/1.2.4\n  hdf: hdf/4.2.15\n  hdf5: hdf5/1.12.2\n  hdf5-mpi: hdf5-mpi/1.12.2\n  intel: intel/2023.2.1\n  intel-classic: intel-classic/2023.2.1\n  intel-oneapi: intel-oneapi/2023.2.1\n  julia: julia/1.9.2\n  linaro-forge: linaro-forge/23.0\n  mkl: mkl/2023.2.0\n  mpi-serial: mpi-serial/2.3.0\n  ncarcompilers: ncarcompilers/1.0.0\n  ncarenv: ncarenv/23.09\n  nccmp: nccmp/1.9.1.0\n  ncl: ncl/6.6.2\n  nco: nco/5.1.6\n  ncview: ncview/2.1.9\n  ncvis: ncvis/2022.08.28\n  netcdf: netcdf/4.9.2\n  netcdf-mpi: netcdf-mpi/4.9.2\n  nvhpc: nvhpc/23.7\n  octave: octave/8.2.0\n  openblas: openblas/0.3.23\n  openmpi: openmpi/4.1.5\n  parallel-netcdf: parallel-netcdf/1.12.3\n  parallelio: parallelio/2.6.1, parallelio/2.6.2\n  paraview: paraview/5.11.1\n  pcre: pcre/8.45\n  peak-memusage: peak-memusage/3.0.1\n  perl: perl/5.38.0\n  pocl: pocl/3.0\n  podman: podman/4.5.1\n  proj: proj/8.2.1\n  texlive: texlive/20220321\n  ucx: ucx/1.14.1\n  udunits: udunits/2.2.28\n  vapor: vapor/3.9.0\n  vexcl: vexcl/1.4.3\n  visit: visit/3.3.3\n  vtune: vtune/2023.2.0\n  wgrib2: wgrib2/3.1.1\n  xxdiff: xxdiff/latest\n----------------------------------------------------------------------------\nTo learn more about a package execute:\n   $ module spider Foo\nwhere \"Foo\" is the name of a module.\nTo find detailed information about a particular package you\nmust specify the version if there is more than one version:\n   $ module spider Foo/11.1\n----------------------------------------------------------------------------\n</code></pre>"},{"location":"compute-systems/casper/casper-use-policies/","title":"Casper use policies","text":""},{"location":"compute-systems/casper/casper-use-policies/#appropriate-use-of-login-nodes","title":"Appropriate use of login nodes","text":"<p>Users may run short, non-memory-intensive processes interactively on the Derecho\u00a0system's login nodes. These include tasks such as text editing or running small serial scripts or programs.</p> <p>However, the login nodes\u00a0may not\u00a0be used to run processes that consume excessive resources. This is to ensure an appropriate balance between user convenience and login node performance.</p> <p>This applies to individual processes that consume excessive amounts of CPU time, more than a few GB of memory, or excessive I/O resources. It also applies collectively to multiple concurrent tasks that an individual user runs.</p> <p>Processes that use excessive resources on the login nodes are terminated automatically. Affected users are informed by email that their sessions were terminated. They are also advised to run such processes in batch or interactive jobs on the Casper cluster.</p>"},{"location":"compute-systems/casper/casper-use-policies/#job-scheduling-priorities","title":"Job scheduling priorities","text":"<p>The PBS Pro workload management system scheduling policy for running jobs in the Derecho environment requires balancing several factors. Jobs generally are sorted based on the following:</p> <ol> <li> <p>Job priority (user selectable)</p> </li> <li> <p>Fair share factor</p> </li> <li> <p>Eligible time in queue</p> </li> <li> <p>Job size</p> </li> </ol> <p>Job sorting is adjusted frequently in response to varying demands and workloads. PBS examines the jobs in sorted order in each scheduling cycle and starts those that it can. Jobs that cannot be started immediately are either scheduled to run at a future time or bypassed for the current cycle. Under typical system usage, multiple scheduling cycles are initiated every minute.</p> <p>The scheduler may not start a job for a number of reasons, including:</p> <ul> <li> <p>The necessary resources are not yet available.</p> </li> <li> <p>The system has been reserved for a scheduled outage.</p> </li> <li> <p>The job has been placed on hold.</p> </li> <li> <p>You have reached your concurrent core-usage limit when using the   develop queue.</p> </li> </ul> <p>A high-priority job might be delayed by one of the limits on the list, while a lower-priority job from a different user or a job requesting fewer resources might not be blocked.</p> <p>If your job is waiting in the queue, you can run the <code>qstat</code> command as shown to obtain information that can indicate why it has not started running. (Use this command sparingly.) <pre><code>qstat -s jobID\n</code></pre></p> <p>Note</p> <p>To prevent jobs from languishing in the queues indefinitely, PBS reserves resources for the top-priority jobs and doesn't allow lower-priority jobs to start if they would delay the start time of a higher-priority job.</p>"},{"location":"compute-systems/casper/casper-use-policies/#pbs-sorting-factors","title":"PBS sorting factors","text":""},{"location":"compute-systems/casper/casper-use-policies/#stakeholder-shares-and-fair-share-factor","title":"Stakeholder shares and fair-share factor","text":"<p>CISL manages scheduling priorities to ensure fair access to the system by these stakeholder groups: the university community, the NCAR community, the CESM community,\u00a0and the Wyoming community.</p> <p>Each stakeholder group is allocated a certain percentage of the available processors. A job cannot start if that action would cause the group to exceed its share, unless another group is using less than its share and has no jobs waiting. In such a case, the high-use group can \"borrow\" processors from the lower-use stakeholder group for a short time.</p> <p>When jobs are sorted, jobs from groups that are using less of their share are picked before jobs from groups using more of their shares. Shares are evaluated based on usage over the past week with usage the prior week being decayed by half.</p>"},{"location":"compute-systems/casper/casper-use-policies/#job-size","title":"Job size","text":"<p>Jobs asking for more nodes are favored over jobs asking for fewer. The reasoning is that while it is easier for small jobs to fill gaps in the schedule, larger jobs need help collecting enough CPUs or GPUs to start.</p>"},{"location":"compute-systems/casper/casper-use-policies/#gpu-usage","title":"GPU usage","text":"<p>In order to submit jobs that will use GPUs, you must be associated with a project that has an allocation of GPU hours. If you submit a job with a project code that does not have an allocation of GPU hours, your job will be rejected.</p>"},{"location":"compute-systems/casper/casper-use-policies/#backfilling","title":"Backfilling","text":"<p>When a job cannot start immediately, PBS sets aside resources for it before examining other jobs to see if any of them can run as backfill. That is, PBS looks at running jobs to determine when they will finish based on wall-time requested. From those finish times, PBS decides when enough resources (such as CPUs, memory, and job limits) will become available to run the top job. PBS then reserves the resources that the job requests at that identified time.</p> <p>When PBS looks at other jobs to see if they can start immediately, it also checks whether starting any of them would collide with one of these resource reservations. Only if there are no collisions will PBS start the lower-priority jobs.</p>"},{"location":"compute-systems/casper/remote-desktops/","title":"Using remote desktops on Casper","text":""},{"location":"compute-systems/casper/remote-desktops/#using-vnc","title":"Using VNC","text":"<p>Most programs on Casper and Cheyenne are designed to run in the terminal, but a few either require the use of a graphical interface or work best in one. While using the default method \u2013 X-forwarding \u2013 is sufficient for simple programs like text editors, it can be prohibitively slow for more complex programs like MATLAB, IDL, VAPOR, and RStudio.</p> <p>When you log on to your workstation, you typically interact with programs using a graphical desktop shell. With virtual network computing (VNC), you can use a graphical desktop shell to work remotely on the Casper data analysis and visualization cluster.</p> <p>The remote desktop runs on Casper in a VNC session that is managed by the VNC server. You access it with a VNC client, which runs on your local workstation.</p>"},{"location":"compute-systems/casper/remote-desktops/#to-get-started","title":"To get started","text":"<p>Download and install a VNC client on your local machine. CISL recommends the TigerVNC client and provides this video to help Mac users install it: Installing TigerVNC on a Mac laptop. (Installing on Windows machines is less complex.)</p> <p>TurboVNC also works, but systems using Java versions &gt;8 do not include the necessary runtime libraries to enable TurboVNC's VNC viewer. Some other VNC clients \u2013 RealVNC, for example \u2013 do not work well with the VNC software installed on Casper.</p>"},{"location":"compute-systems/casper/remote-desktops/#connecting-to-a-vnc-session","title":"Connecting to a VNC session","text":"<p>To begin using a remote desktop with VNC, you will need to start a VNC session. Your session will run within a Casper job and can persist for up to 24 hours.</p> <p>These basic steps for starting a session are described in detail below:</p> <ul> <li> <p>Run the <code>vncmgr</code> script, which enables you to configure your VNC   desktop session and start both a Casper batch job and the VNC server.</p> </li> <li> <p>Connect to the Casper batch node with your VNC client, using a port   that VNC specifies.</p> </li> <li> <p>Enter a one-time password to access your VNC session.</p> </li> </ul> <p>If you are connected to the NCAR Internal Network or using the NCAR VPN, you will be able to connect directly to your session on the Casper batch node using the VNC client.</p> <p>If you are not on the NCAR network or VPN, you will need to create an SSH tunnel to connect your local machine and remote desktop. How to create the SSH tunnel is described below.</p>"},{"location":"compute-systems/casper/remote-desktops/#overview-of-the-vncmgr-script","title":"Overview of the <code>vncmgr</code> script","text":"<p>CISL provides the <code>vncmgr</code> script for initiating and managing VNC sessions on Casper. How to run it is described in detail in the following section. It can be used in interactive mode or command-line mode.</p>"},{"location":"compute-systems/casper/remote-desktops/#interactive-mode","title":"Interactive mode","text":"<p>If you run the script without any command-line arguments, it will launch in the interactive mode. In this mode, you can start a new session, list existing sessions, query a session to retrieve connection instructions and obtain a new one-time-password, and kill a running session. The script enables you to name your session, state how long you want the server to run, and select which desktop shell to use (GNOME2, GNOME3, or KDE). It also allows for custom requests to both the job scheduler and the VNC server program.</p>"},{"location":"compute-systems/casper/remote-desktops/#command-line-mode","title":"Command-line mode","text":"<p>In command-line mode, you specify a subcommand and provide any desired options as command line arguments. Here are the available commands: <pre><code>vncmgr list\n\nvncmgr create [SESSION] --account PROJECT [--time WALLTIME \u2026]\n\nvncmgr query [SESSION]\n\nvncmgr kill [SESSION]\n</code></pre></p> <p>Choosing a name <code>SESSION</code> is optional. If you do not provide a name, the name <code>default</code> will be assigned and referenced in each subcommand.</p>"},{"location":"compute-systems/casper/remote-desktops/#customizing-the-casper-job-and-vnc-server","title":"Customizing the Casper job and VNC server","text":"<p>The <code>vncmgr</code> script allows you to customize both the Casper session in which the server will run and the server itself. This customization can be done in both interactive and command-line modes. The most common uses involve increasing the resources allocated to your job. For example, you could allocate 4 CPUs and 20 GB of memory to a 2-hour VNC session using the command-line mode as follows:</p> <pre><code>vncmgr create \\\n       --account PROJECT \\\n       --time 2:00:00 \\\n       --job-opts=\"-l select=1:ncpus=4:mem=20GB\"\n</code></pre> <p>You do not need to specify GPU resources, as all VNC jobs are automatically placed on nodes with NVIDIA Quadro GP100 GPUs.</p> <p>Run <code>vncmgr --help</code> in a Casper login session for more information about using the script and customizing your session.</p>"},{"location":"compute-systems/casper/remote-desktops/#running-vncmgr-from-your-local-machine","title":"Running <code>vncmgr</code> from your local machine","text":"<p>You can run the <code>vncmgr</code> command directly from your local machine as shown in the example below without first starting a login session on Casper. While both command-line and interactive mode will work, CISL recommends interactive mode as it will allow you to generate new one-time passwords via the query option without having to authenticate to Caspe every time.</p>"},{"location":"compute-systems/casper/remote-desktops/#example-vnc-session","title":"Example VNC Session","text":"<p>This demonstrates how to run <code>vncmgr</code>, create and configure a customized VNC session, and then connect to the session with a VNC client. In this example, the user is not connected to the NCAR VPN and needs to use an SSH tunnel. (Alternative for PuTTY users.</p> <ol> <li> <p>Run this command to get started, using your own username: <pre><code>ssh -t -l username casper.hpc.ucar.edu /glade/u/apps/opt/vncmgr/bin/vncmgr\n</code></pre>     You will be prompted to authenticate, after which you will have access     to the vncmgr menu. (If you exit vncmgr, you will have to rerun the     command to regain access to it.)</p> </li> <li> <p>Create your new VNC session with a name up to 10 characters long.     The session is named \"vapor\" in this example. (Choose names that     will help you avoid confusion if you run multiple sessions.)</p> <p></p> </li> <li> <p>A series of prompts on the next screen (below) will ask you to     specify:</p> <ul> <li>Your project code.</li> <li>The wallclock time you want in <code>HH:MM:SS</code> format. The default is 4 hours and the maximum is 24 hours.</li> <li>Which desktop shell to use. The default setting is 2d, which configures your desktop to use a shell with the MATE user interface.</li> <li>Any optional arguments. In the example, the user requests 20 GB of memory.</li> </ul> <p>All VNC jobs must run on a node with a GP100 GPU. If you specify custom resource requirements, those requirements will be modified if necessary to ensure that the job can run on the correct node.</p> <p></p> </li> <li> <p>When the job starts, follow the instructions and choose how you want     to connect to your VNC session: tunneling or using the UCAR internal     network or VPN.</p> <p></p> <p>Pressing enter at this point will return you to the interactive menu. Use the query option if you need to return to the instructions for creating the SSH tunnel later.</p> </li> <li> <p>Your desktop on Casper will be displayed after you connect to the     specified host and enter the onetime password. On the desktop, start     a terminal from the list of applications.</p> <p></p> </li> <li> <p>To start an application, load any required modules and run the     executable.</p> <p></p> </li> </ol>"},{"location":"compute-systems/casper/remote-desktops/#ssh-tunneling-with-putty","title":"SSH tunneling with PuTTY","text":"<p>The general process for creating an <code>ssh</code> tunnel described above will work well for clients with a command line <code>ssh</code> installation, however additional steps are required for Windows users with PuTTY as their <code>ssh</code> client. Expand the note below for additional details.</p> SSH tunneling with PuTTY <p>The output from your <code>vncmgr</code> command includes a line in this format. (Each x is a number.) <pre><code>ssh -l username -L xxxx:localhost4:xxxx casperxx.ucar.edu \"bash .vnctunnel-default\"\n</code></pre> Follow these steps to copy and paste the necessary information from the <code>vncmgr</code> command into the PuTTY interface for Windows.</p> <ul> <li>Load a PuTTY session with <code>casperxx.ucar.edu</code> as the hostname.</li> <li>Select Connection, then SSH.</li> <li>Enter the following in the Remote command field. <pre><code>bash .vnctunnel-default\n</code></pre></li> </ul> <p></p> <ul> <li>Under SSH, select Tunnels.<ul> <li>Paste the first four digits from the ssh command above into the Source port field.</li> <li>Paste the <code>localhost4:xxxx</code> into the Destination field.</li> </ul> </li> </ul> <p></p> <ul> <li>Click Add, Click Open, then log in.</li> <li>Follow the instructions provided in your terminal window to start your   VNC client. <pre><code>Starting SSH tunnel to the VNC server...\n\nNow load VNC on your local computer and connect to:\n\nlocalhost:xxxx\n\nVNC will ask for a one-time password. Use the following:\n\nxxxxxxxx\n\nThis terminal session will hang until the tunnel is killed.\n\nTo kill the tunnel, simply type C-c/Control-C.\n</code></pre></li> </ul>"},{"location":"compute-systems/casper/remote-desktops/#using-fastx","title":"Using FastX","text":"<p>The FastX remote desktop service gives users access to Casper for performing lightweight tasks such as text editing, running programs such as <code>xxdiff</code> and <code>ncview</code>, or running analysis scripts that consume little in the way of graphics resources.</p> <p>A user can log out of a FastX remote desktop and return to it later. This service will remain available while NCAR and UCAR building closures are in effect.</p> <p>FastX sessions that consume excessive resources are subject to being killed. For resource-intensive workloads that do not require GPU-based rendering, consider starting a Casper job instead by running an <code>execcasper</code> command from a FastX terminal window. For more resource-intensive work with high-end, GPU-accelerated graphics, consider using VNC as described above rather than FastX.</p> <p>FastX can be accessed through a web browser or a desktop client. How to use both of these options is described below.</p>"},{"location":"compute-systems/casper/remote-desktops/#fastx-via-web-browser-with-vpn-access","title":"FastX via web browser with VPN access","text":"<p>To use FastX without installing any software, connect to the NCAR VPN and use an updated version of any common browser. See the following section for an alternative to using the VPN.</p> <ol> <li> <p>Connect to the NCAR VPN.</p> </li> <li> <p>Go to https://fastx.ucar.edu:3300.</p> </li> <li> <p>Authenticate with your username and token response.</p> </li> <li> <p>Click the + button in the upper-left corner of the FastX window.</p> <p></p> </li> <li> <p>In the next window, click the KDE button and then Launch.</p> <p></p> </li> <li> <p>The KDE desktop will open in a new tab or a new browser window.     Right-click on the desktop to start a Kconsole terminal window.</p> <p></p> </li> </ol> <p>Missing Modules under FastX?</p> <p>Some users will find that their terminal environment configuration is not complete (module commands will not be available, for example). To initialize your environment, run the following command once you open the Kconsole:</p> bash/zshtcsh <pre><code># source bash/zsh login environment definition scripts:\nsource /etc/profile\n</code></pre> <pre><code># source tcsh login environment definition scripts:\nsource /etc/csh.login\n</code></pre>"},{"location":"compute-systems/casper/remote-desktops/#reconnecting-to-your-fastx-session","title":"Reconnecting to your FastX session","text":"<p>If you want to retain your session to return to it later, just log out or kill the browser window. When you log in again, select the icon to re-open the session. </p>"},{"location":"compute-systems/casper/remote-desktops/#terminating-your-fastx-session","title":"Terminating your FastX session","text":"<p>If you do not want to retain your session, terminate it as shown here before logging out: </p>"},{"location":"compute-systems/casper/remote-desktops/#fastx-via-web-browser-and-ssh-tunnel","title":"FastX via web browser and ssh tunnel","text":"<p>If you are not connected to or do not want to connect to the NCAR VPN, you can still use FastX via web browser by creating an ssh tunnel from your laptop or desktop to FastX.</p> <ol> <li>Start by running the following on your command line, inserting your     own username. <pre><code>ssh -L 3300:fastx.ucar.edu:3300 username@fastx.ucar.edu\n</code></pre></li> <li>Authenticate as usual and you will be in a terminal session on a Casper node.</li> <li>Leave that terminal session running, open your browser, and go to https://localhost:3300/. You may see a warning about the site being unsafe, but you may ignore the warning and continue.</li> <li>Authenticate with your username and token response.</li> <li>Continue as described in the previous section to launch the KDE desktop.</li> </ol>"},{"location":"compute-systems/casper/remote-desktops/#alternatives-for-creating-ssh-tunnel","title":"Alternatives for creating ssh tunnel","text":"<p>To create a tunnel using PuTTY or SecureCRT, following the examples in these video demonstrations:</p> <ul> <li>PuTTY ssh tunnel</li> <li>SecureCRT ssh tunnel</li> </ul> <p>After creating the tunnel, proceed as described in the previous section.</p>"},{"location":"compute-systems/casper/remote-desktops/#fastx-via-desktop-client","title":"FastX via desktop client","text":"<p>If you\u2019re not connected to the NCAR VPN or prefer to use a faster, more robust remote desktop service, consider downloading and installing the FastX desktop client.</p> <p>FastX desktop client for Windows users</p> <p>If you do not have admin privileges to install the client on your machine, choose the \u201cWindows Nonroot\u201d client.</p> <ol> <li>Start the FastX client.</li> <li> <p>Click the + button in the upper-left corner.     </p> </li> <li> <p>Fill in the fields of the pop-up boxes as follows:</p> <ul> <li>Host: <code>fastx.ucar.edu</code></li> <li>User: Enter your username</li> <li>Port: 22</li> <li>Name: CISL remote desktop (or any string)</li> <li>Run (advanced tab): <code>/ncar/opt/fastx/latest/bin/fastx-protocol</code></li> </ul> <p> </p> </li> <li> <p>Click OK.</p> </li> <li>Select the remote desktop collection (double-click or press Enter).</li> <li>Authenticate with your username and token response.</li> <li> <p>Click the + button in the upper-left corner of the next window.     </p> </li> <li> <p>Select the KDE icon that displays <code>startplasma-x11</code> in the required command field.</p> <ol> <li>Double-click the icon to start the KDE desktop. You can then     right-click on the desktop to get a Kconsole menu, OR</li> <li>Double-click the xterm icon instead to get a single xterm.</li> </ol> <p></p> </li> </ol>"},{"location":"compute-systems/casper/compiling-code-on-casper/","title":"Compiling code on NCAR systems","text":""},{"location":"compute-systems/casper/compiling-code-on-casper/#compilers-available-on-casper","title":"Compilers available on Casper","text":"<p>Several\u00a0C/C++ and Fortran\u00a0compilers are available on all NCAR HPC systems. The information on this page applies to all of those systems except where noted.</p> Compiler Language Commands for serial programs Commands for programs           using MPI Flags to enable OpenMP Intel (Classic/OneAPI)* Fortran <pre>ifort / ifx</pre> <pre>mpif90</pre> <pre>-qopenmp</pre> C <pre>icc / icx</pre> <pre>mpicc</pre> C++ <pre>icpc / icpx</pre> <pre>mpicxx</pre> NVIDIA HPC SDK Fortran <pre>nvfortran</pre> <pre>mpif90</pre> <pre>-mp</pre> C <pre>nvc</pre> <pre>mpicc</pre> C++ <pre>nvc++</pre> <pre>mpicxx</pre> GNU Compiler Collection (GCC) Fortran <pre>gfortran</pre> <pre>mpif90</pre> <pre>-fopenmp</pre> C <pre>gcc</pre> <pre>mpicc</pre> C++ <pre>g++</pre> <pre>mpicxx</pre> * Intel OneAPI is a cross-platform toolkit that           supports C, C++, Fortran, and Python programming languages           and replaces Intel             Parallel Studio. Casper supports both Intel OneAPI and Intel           Classic Compilers. Intel is planning to retire the Intel Classic           compilers and is moving toward Intel OneAPI. Intel Classic Compiler           commands (ifort, icc, and icpc) will be replaced by the Intel OneAPI           compilers (ifx, icx, and icpx)."},{"location":"compute-systems/casper/compiling-code-on-casper/#compiler-commands","title":"Compiler Commands","text":"<p>All supported compilers are available via the <code>module</code> utility. After loading the compiler module you want to use, refer to the table above to identify and run the appropriate compilation wrapper command.</p> <p>If your script already includes one of the following generic MPI commands, there is no need to change it:</p> <ul> <li> <p><code>mpif90</code>, <code>mpif77</code></p> </li> <li> <p><code>mpicc</code></p> </li> <li> <p><code>mpiCC</code></p> </li> </ul> <p>Build any libraries that you need to support an application with the same compiler, compiler version, and compatible flags used to compile the other parts of the application, including the main executable(s). Also, before you run the applications, be sure you have loaded the same module/version environment in which you created the applications. This will help you avoid job failures that can result from missing MPI launchers and library routines.</p>"},{"location":"compute-systems/casper/compiling-code-on-casper/#compiler-man-pages","title":"Compiler <code>man</code> pages","text":"<p>To refer to the <code>man</code> page for a compiler, log in to the system where you intend to use it, load the module, then execute <code>man</code> for the compiler. For example: <pre><code>module load nvhpc\nman nvfortran\n</code></pre></p> <p>You can also use <code>-help</code> flags for a description of the command-line options for each compiler. Follow this example: <pre><code>ifort -help\n\nnvfortran -help [=option]\n</code></pre></p> <p>Tip</p> <p>Use compiler diagnostic flags to identify potential problems while compiling the code.</p>"},{"location":"compute-systems/casper/compiling-code-on-casper/#changing-compilers","title":"Changing compilers","text":"<p>To change from one compiler to another, use <code>module swap</code>. In this example, you are switching from Intel to NVIDIA: <pre><code>module swap intel nvhpc\n</code></pre></p> <p>When you load a compiler module or change to a different compiler, the system makes other compatible modules available. This helps you establish a working environment and avoid conflicts.</p> <p>If you need to link your program with a library, use <code>module load</code> to load the library as in this example: <pre><code>module load netcdf\n</code></pre></p> <p>Then you can invoke the desired compilation command, including any library linking options such as <code>-lnetcdf</code>. Here's an example: <pre><code>mpif90 -o foo.exe foo.f90 -lnetcdf\n</code></pre></p>"},{"location":"compute-systems/casper/compiling-code-on-casper/#compiling-cpu-code","title":"Compiling CPU code","text":"<p>Optimizing code for multiple types of CPUs</p> <p>Be aware that compiling CPU code on Casper can be complicated by the heterogeneous nature of the nodes. (Casper nodes contain a mixture of Intel Skylake, Intel Cascade Lake, and AMD Milan CPUs.)</p> <p>In general users will want to compile binaries that can execute on any of the CPU types.  This can be accomplished by manually specifying the target CPU architecture:</p> Intel CompilersGCC CompilersNVHPC Compilers <p><code>-march=core-avx2</code></p> <p><code>-march=core-avx2</code></p> <p><code>-tp=zen3</code></p> <p>If your application fails to run with an <code>illegal instruction</code> message, this indicates the compiled binary contains instructions incompatible with the current CPU.  Try compiling with flags as indicated above, or reach out to consulting for help.</p>"},{"location":"compute-systems/casper/compiling-code-on-casper/#using-the-default-intel-compiler-collection","title":"Using the default Intel compiler collection","text":"<p>The Intel compiler suite is available via the <code>intel</code> module, which is loaded by default. It includes compilers for C, C++, and Fortran codes.</p> <p>To see which versions are available, use the <code>module avail</code> command. <pre><code>module avail intel\n</code></pre></p> <p>To load the default Intel compiler, use <code>module load</code> without specifying a version. <pre><code>module load intel\n</code></pre></p> <p>To load a different version, specify the version number when loading the module.</p> <p>Similarly, you can swap your current compiler module to Intel by using the <code>module swap</code> command. <pre><code>module swap gcc intel\n</code></pre> Extensive documentation for using the Intel compilers is available online here. To review the manual page for a compiler, run the <code>man</code> command for it as in this example: <pre><code>man ifort\n</code></pre></p> <p>What's the difference between the <code>intel</code>, <code>intel-oneapi</code>, <code>intel-classic</code> modules?</p> <p>Users migrating from Cheyenne and previous Casper deployments may note there are several \"flavors\" of the Intel compiler available through the module system.</p> <p>Intel is currently moving from their \"classic\" compiler suite to the new \"OneAPI\" family.  During this process both sets of compilers are available, but through different commands under different <code>module</code> selections:</p> Module Fortran C C++ <code>intel-classic</code> <code>ifort</code> <code>icc</code> <code>icpc</code> <code>intel-oneapi</code> <code>ifx</code> <code>icx</code> <code>icpx</code> <code>intel</code>(default) <code>ifort</code> <code>icx</code> <code>icpx</code> <p>The <code>intel-classic</code> module makes the familiar <code>ifort/icc/icpc</code> compilers available, however it is expected these will be deprecated during Casper's lifetime.  At this stage we expect to keep existing compiler versions available, however there will be no further updates.</p> <p>The <code>intel-oneapi</code> module uses the new <code>ifx/icx/icpx</code> compilers.</p> <p>The default <code>intel</code> module presently uses the older <code>ifort</code> Fortran compiler along with the newer <code>icx/icpx</code> C/C++ compilers. This choice is intentional as the newer <code>ifx</code> does not reliably match the performance of <code>ifort</code> in all cases.  We will continue to monitor the progress of the OneAPI compilers and will change this behavior in the future.</p>"},{"location":"compute-systems/casper/compiling-code-on-casper/#optimizing-your-code-with-intel-compilers","title":"Optimizing your code with Intel compilers","text":"<p>Intel compilers provide several different optimization and vectorization options. By default, they use the <code>-O2</code> option, which includes some optimizations.</p> <p>Using <code>-O3</code> instead will provide more aggressive optimizations that may not improve the performance of some programs, while <code>-O1</code> enables minimal optimization. A higher level of optimization might increase your compile time significantly.</p> <p>You can also disable any optimization by using <code>-O0</code>.</p>"},{"location":"compute-systems/casper/compiling-code-on-casper/#examples","title":"Examples","text":"<p>To compile and link a single Fortran program and create an executable, follow this example: <pre><code>ifort filename.f90 -o filename.exe\n</code></pre></p> <p>To enable multi-threaded parallelization (OpenMP), include the <code>-qopenmp</code> flag as shown here: <pre><code>ifort -qopenmp filename.f90 -o filename.exe\n</code></pre></p>"},{"location":"compute-systems/casper/compiling-code-on-casper/#other-compilers","title":"Other compilers","text":"<p>These additional compilers are available on Casper.</p> <ul> <li>NVIDIA\u2019s HPC SDK</li> <li>the GNU Compiler Collection (GCC)</li> </ul>"},{"location":"compute-systems/casper/compiling-code-on-casper/#compiling-gpu-code","title":"Compiling GPU code","text":"<p>On Casper, GPU applications should be built with either the NVIDIA HPC SDK compilers and libraries, or with two-stage linking. In the following examples, we demonstrate the use of NVIDIA\u2019s tools. To compile CUDA code to run on the Casper data analysis and visualization nodes, use the appropriate NVIDIA compiler command:</p> <ul> <li> <p><code>nvc</code> \u2013 NVIDIA C compiler</p> </li> <li> <p><code>nvcc</code> \u2013 NVIDIA CUDA compiler (Using <code>nvcc</code> requires a C compiler to   be present in the background; <code>nvc</code>, <code>icc</code>, or <code>gcc</code>, for example.)</p> </li> <li> <p><code>nvfortran</code> \u2013 CUDA Fortran</p> </li> </ul> <p>Additional compilation flags for GPU code will depend in large part on which GPU-programming paradigm is being used (e.g., OpenACC, OpenMP, CUDA) and which compiler collection you have loaded. The following examples show basic usage, but note that many customizations and optimizations are possible. You are encouraged to read the relevant man page for the compiler you choose.</p>"},{"location":"compute-systems/casper/compiling-code-on-casper/#openacc","title":"OpenACC","text":"<p>To compile with OpenACC directives, simply add the <code>-acc</code> flag to your invocation of nvc, nvc++, or nvfortan. A Fortran example: <pre><code>nvfortran -o acc_bin -acc acc_code.f90\n</code></pre></p> <p>You can gather more insight into GPU acceleration decisions made by the compiler by adding <code>-Minfo=accel</code> to your invocation. Using compiler options, you can also specify which GPU architecture to target. This example will request compilation for both V100 and A100 GPUs: <pre><code>nvfortran -o acc_bin -acc -gpu=cc70,cc80 acc_code.f90\n</code></pre></p> <p>Specifying multiple acceleration targets will increase the size of the binary and the time it takes to compile the code.</p>"},{"location":"compute-systems/casper/compiling-code-on-casper/#openmp","title":"OpenMP","text":"<p>Using OpenMP to offload code to the GPU is similar to using OpenACC. To compile a code with OpenMP offloading, use the <code>-mp=gpu</code> flag. The aforementioned diagnostic and target flags also apply to OpenMP offloading. <pre><code>nvfortran -o omp_gpu -mp=gpu omp.f90\n</code></pre></p>"},{"location":"compute-systems/casper/compiling-code-on-casper/#cuda","title":"CUDA","text":"<p>The process for compiling CUDA code depends on whether you are using C++ or Fortran. For C++, the process often involves multiple stages in which you first use <code>nvcc</code>, the NVIDIA CUDA compiler, and then your C++ compiler of choice. <pre><code>nvcc -c -arch=sm_80 cuda_code.cu\ng++ -o cuda_bin -lcuda -lcudart main.cpp cuda_code.o\n</code></pre> Using the <code>nvcc</code> compiler driver with a non-NVIDIA C++ compiler requires loading a <code>cuda</code> environment module in addition to the compiler of choice.</p> <p>The compiler handles CUDA code directly, so the compiler you use must support CUDA. This means you should use <code>nvfortran</code>. If your source code file ends with the <code>.cuf</code> extension, nvfortran will enable CUDA automatically. Otherwise, you can specify the <code>-Mcuda</code> flag to the compiler. <pre><code>nvfortran -Mcuda -o cf_bin cf_code.f90\n</code></pre></p> <p>The sample below demonstrates how to compile CUDA C code on casper.</p> <code>hello_world.cu</code> on Casper with CUDA <p><code>hello_world.cu</code> source file:</p> <pre><code>/* hello_world.cu\n * ---------------------------------------------------\n * A Hello World example in CUDA\n * ---------------------------------------------------\n * This is a short program which uses multiple CUDA\n * threads to calculate a \"Hello World\" message which\n * is then printed to the screen.  It's intended to\n * demonstrate the execution of a CUDA kernel.\n * ---------------------------------------------------\n */\n#define SIZE 12\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;cuda_runtime.h&gt;\n\n/* CUDA kernel used to calculate hello world message */\n__global__ void hello_world(char *a, int N);\n\nint main(int argc, char **argv)\n{\n   /* data that will live on host */\n   char *data;\n\n   /* data that will live in device memory */\n   char *d_data;\n\n   /* allocate and initialize data array */\n   data = (char*) malloc(SIZE*sizeof(char));\n   data[0]  =  72; data[1]  = 100; data[2]  = 106;\n   data[3]  = 105; data[4]  = 107; data[5]  =  27;\n   data[6]  =  81; data[7]  = 104; data[8]  = 106;\n   data[9]  =  99; data[10] =  90; data[11] =  22;\n\n   /* print data before kernel call */\n   printf(\"Contents of data before kernel call: %s\\n\", data);\n\n   /* allocate memory on device */\n   cudaMalloc(&amp;d_data, SIZE*sizeof(char));\n\n   /* copy memory to device array */\n   cudaMemcpy(d_data, data, SIZE, cudaMemcpyHostToDevice);\n\n   /* call kernel */\n   hello_world&lt;&lt;&lt;4,3&gt;&gt;&gt;(d_data, SIZE);\n\n   /* copy data back to host */\n   cudaMemcpy(data, d_data, SIZE, cudaMemcpyDeviceToHost);\n\n   /* print contents of array */\n   printf(\"Contents of data after kernel call:  %s\\n\",data);\n\n   /* clean up memory on host and device */\n   cudaFree(d_data);\n   free(data);\n   return(0);\n}\n\n/* hello_world\n * Each thread increments an element of the input\n * array by its global thread id\n */\n__global__ void hello_world(char *a, int N)\n{\n   int i = blockDim.x * blockIdx.x + threadIdx.x;\n   if(i &lt; N) a[i] = a[i] + i;\n}\n</code></pre> <p>Log into a GPU-Enabled node</p> <p>Log in to either Casper or Derecho and run <code>execcasper</code> with a GPU resource request to start an interactive job on a GPU-accelerated Casper node:</p> <pre><code>execcasper -l gpu_type=gp100 --ngpus=1\n</code></pre> <p>Compile <code>hello_world.cu</code></p> NVHPCGCC <pre><code>module reset\nmodule load nvhpc cuda\nnvcc -o hello hello_world.cu\n</code></pre> <pre><code>module reset\nmodule load gnu cuda\nnvcc -c hello_world.cu\ngcc -o hello hello_world.o\n</code></pre> <p>Run the program</p> <pre><code>./hello\nContents of data before kernel call: HdjikhjcZ\nContents of data after kernel call:  Hello World!\n</code></pre>"},{"location":"compute-systems/casper/compiling-code-on-casper/#native-compiler-commands","title":"Native Compiler Commands","text":"<p>We recommend using the module wrapper commands described above. However, if you prefer to invoke the compilers directly without the <code>ncarcompilers</code> wrappers, see this note:</p> Native Compiler Commands <p>We recommend using the module wrapper commands described above. However, if you prefer to invoke the compilers directly, unload the NCAR default compiler wrapper environment by entering this on your command line: <pre><code>module unload ncarcompilers\n</code></pre></p> <p>You can still use the environment variables that are set by the modules that remain loaded, as shown in the following examples of invoking compilers directly to compile a Fortran program.</p> Intel compilerNVIDIA HPC compilerGNU compiler collection (GCC) <pre><code>ifort -o a.out $NCAR_INC_&lt;PACKAGE&gt; program_name.f $NCAR_LDFLAGS_&lt;PACKAGE&gt; -l&lt;package_library&gt;\n</code></pre> <pre><code>nvfortran -o a.out $NCAR_INC_&lt;PACKAGE&gt; program_name.f $NCAR_LDFLAGS_&lt;PACKAGE&gt; -l&lt;package_library&gt;\n</code></pre> <pre><code>gfortran -o a.out $NCAR_INC_&lt;PACKAGE&gt; program_name.f $NCAR_LDFLAGS_&lt;PACKAGE&gt; -l&lt;package_library&gt;\n</code></pre>"},{"location":"compute-systems/casper/compiling-code-on-casper/#multiple-compiler-versions-and-user-applications","title":"Multiple Compiler Versions and User Applications","text":"<p>In addition to multiple compilers, CISL keeps available multiple versions of libraries to accommodate a wide range of users' needs. Rather than rely on the environment variable <code>LD_LIBRARY_PATH</code> to find the correct libraries dynamically, we encode library paths within the binaries when you build Executable and Linkable Format (ELF) executables. To do this, we use <code>RPATH</code> rather than <code>LD_LIBRARY_PATH</code> to set the necessary paths to shared libraries.</p> <p>This enables your executable to work regardless of updates to new default versions of the various libraries; it doesn't have to search dynamically at run time to load them. It also means you don't need to worry about setting the variable or loading another module, greatly reducing the likelihood of runtime errors.</p>"},{"location":"compute-systems/casper/compiling-code-on-casper/#common-compiler-options-and-diagnostic-flags","title":"Common Compiler Options and Diagnostic Flags","text":"<p>Portability and correctness both are important goals when developing code. Non-standard code may not be portable, and its execution may be unpredictable.</p> <p>Using diagnostic options when you compile your code can help you find potential problems. Since the compiler is going to analyze your code anyway, it pays to take advantage of the diagnostic options to learn as much as you can from the analysis. Please note that some compilers disable the default optimization when you switch on certain debugging flags.</p> <p>Because of differences in compilers, it also is good practice to compile your code with each compiler that is available on the system, note any diagnostic messages you get, and revise your code accordingly.</p> <p>The following options can be helpful as you compile code to run in the HPC environment that CISL manages.</p> Compiler Flag Effect IntelIntel C++ diagnostic options <code>-debug all</code> provides complete debugging information. <code>-g</code> places symbolic debugging information in the executable program. <code>-check all</code> performs all runtime checks (includes bounds checking). <code>-warn all</code> enables all warnings. <code>-stand f08</code> warns of usage that does not conform to the Fortran 2008 standard. <code>-traceback</code> enables stack trace if the program crashes. GCCGCC diagnostic warning ptions <code>-ggdb</code> places symbolic debugging information in the executable program for use by GDB. <code>-fcheck=all</code> performs all runtime checks (includes bounds checking). <code>-Wall</code> enables all warnings. <code>-std=f2008</code> warns of usage that does not conform to the Fortran 2008 standard. NVIDIA HPC SDKNVIDIA HPC SDK documentation. <code>-g</code> Include symbolic debugging information in the object modules with optimization disabled (<code>-O0</code>). <code>-gopt</code> Include symbolic debugging information in the object modules without affecting any optimizations. <code>-C</code> or <code>-Mbounds</code> Add array bounds checking. <code>-Mchkptr</code> Check for unintended de-referencing of NULL pointers. <code>-Minform=inform</code> Display all the error messages of any severity (inform, warn, severe and fatal) during compilation phase."},{"location":"compute-systems/casper/starting-casper-jobs/","title":"Starting Casper jobs","text":"<p>This page describes how to use PBS Pro to submit jobs to run on nodes in the Casper cluster. Unless GPUs are required, run jobs that require the use of more than one compute node on Derecho.</p> <p>Procedures for starting both interactive jobs and batch jobs on Casper are described below. Also:</p> <ul> <li> <p>Compile your code on Casper nodes if you will run it on Casper.</p> </li> <li> <p>See Calculating charges to   learn how core-hours charges are calculated for jobs that run on   Casper.</p> </li> </ul> <p>Begin by logging in on Casper or Derecho.</p> <p>Casper Wall-clock limits</p> <p>The wall-clock limit on the Casper cluster is 24 hours except as noted below.</p> <p>Specify the hours your job needs as in the examples below. Use either the <code>hours:minutes:seconds</code> format or <code>minutes:seconds</code>.</p>"},{"location":"compute-systems/casper/starting-casper-jobs/#interactive-jobs","title":"Interactive jobs","text":""},{"location":"compute-systems/casper/starting-casper-jobs/#starting-a-remote-command-shell-with-execcasper","title":"Starting a remote command shell with execcasper","text":"<p>Run the <code>execcasper</code> command to start an interactive job. Invoking it without an argument will start an interactive shell on the first available HTC node. The default wall-clock time is 6 hours.</p> <p>To use another type of node, include a select statement specifying the resources you need. The <code>execcasper</code> command accepts all PBS flags and resource specifications as detailed by <code>man qsub</code>.</p> <p>If you do not include a resource specification by using either a select statement or convenience flags, you will be assigned 1 CPU with 10 GB of memory and no GPUs.</p> <p>If no project is assigned with either the <code>-A</code> option or the <code>DAV_PROJECT</code> environment variable, any valid project listed for your username will be chosen at random.</p>"},{"location":"compute-systems/casper/starting-casper-jobs/#starting-a-virtual-desktop-with-vncmgr","title":"Starting a virtual desktop with vncmgr","text":"<p>If your work with complex programs such as MATLAB and VAPOR requires the use of virtual network computing (VNC) server and client software, use <code>vncmgr</code> instead of <code>execcasper</code>.</p> <p>Using <code>vncmgr</code> simplifies configuring and running a VNC session in a Casper batch job. How to do that is documented here.</p>"},{"location":"compute-systems/casper/starting-casper-jobs/#batch-jobs","title":"Batch jobs","text":"<p>Prepare a batch script by following one of the examples here. Most Casper batch jobs use the <code>casper</code> submission queue. The exception is for GPU development jobs, which are submitted to the <code>gpudev</code> submission queue.</p> <p>Be aware that the system does not import your login environment by default, so make sure your script loads the software modules that you will need to run the job.</p> <p>Caution:  Avoid using the PBS -V option with cross submission</p> <p>Avoid using the PBS <code>-V</code> option to propagate your environment settings to the batch job; it can cause odd behaviors and job failures when used in submissions to Casper from Derecho. If you need to forward certain environment variables to your job, use the lower-case <code>-v</code> option to specify them. (See <code>man qsub</code> for details.)</p> <p>When your job script is ready, use <code>qsub</code> to submit it from the Casper login nodes.</p>"},{"location":"compute-systems/casper/starting-casper-jobs/#gpu-development-jobs","title":"GPU development jobs","text":"<p>A submission queue called <code>gpudev</code> is available between 8 a.m. and 5:30 p.m. Mountain time Monday to Friday to support application development and debugging efforts on general purpose and ML/AI GPU applications. This queue provides rapid access to up to 4 V100 GPUs, avoiding the sometimes lengthy queue wait times in the <code>gpgpu</code> execution queue.</p> <p>Job submissions to this queue are limited to 30 minutes walltime instead of the 24-hour wallclock limit for all other submissions. All jobs submitted to the queue must request one or more V100 GPUs (up to four) in their resource directives. Node memory can be specified  explicitly as usual, but by default jobs will be assigned N/4 of the total memory on a node, where N is the number of V100 GPUs requested.</p>"},{"location":"compute-systems/casper/starting-casper-jobs/#concurrent-resource-limits","title":"Concurrent resource limits","text":"<p>Job limits are in place to ensure short dispatch times and a fair distribution of system resources. The specific limits that apply to your submission depend on the resources requested by your job. Based on your request, your submission will be classified as shown in the table.</p> Submission queue Job category(execution queue) Job resource requests Limits <pre>casper</pre>         24-hour wallclock limit <pre>largemem</pre> <pre>mem&gt;361 GB</pre> <pre>ncpus&lt;=36</pre> <pre>ngpus=0</pre> Up to 5 jobs eligible for execution at any one time (more can be         queued) <pre>htc</pre> <pre>mem&lt;=361 GB</pre> <pre>ncpus&lt;=36</pre> <pre>ngpus=0</pre> Up to 468 CPUs in use per user at any one time.         Up to 4680 GB memory per user at any one time         (across all jobs in category) <pre>vis</pre> <pre>gpu_type=gp100</pre> Up to 2 GPUs in use per user at any one time         Individual jobs are limited to a single gp100 (no multi-GPU jobs) <pre>gpgpu</pre> <pre>gpu_type=v100|a100</pre> Up to 32 GPUs in use per user at any one time; users may submit jobs         requesting more than 32 GPUs for execution on weekends. <pre>gpudev</pre>         30-minute wallclock limit <pre>ncpus&lt;=36</pre> <pre>1&lt;=ngpus&lt;=4</pre> Queue is only operational from 8 a.m. to 5:30 p.m. Mountain time,         Monday to Friday. Users may have only one active job in the queue at any         time."},{"location":"compute-systems/casper/starting-casper-jobs/#nvme-node-local-storage","title":"NVMe node-local storage","text":"<p>Casper nodes each have 2 TB of local NVMe solid-state disk (SSD) storage. Some is used to augment memory to reduce the likelihood of jobs failing because of excessive memory use.</p> <p>NVMe storage can also be used while a job is running. (Recommended only for I/O-intensive jobs.) Data stored in <code>/local_scratch/pbs.$PBS_JOBID</code> are deleted when the job ends.</p> <p>To use this disk space while your job is running, include the following in your batch script after customizing as needed. <pre><code>### Copy input data to NVMe (can check that it fits first using \"df -h\")\ncp -r /glade/scratch/$USER/input_data /local_scratch/pbs.$PBS_JOBID\n\n### Run script to process data (NCL example takes input and output paths as command line arguments)\nncl proc_data.ncl /local_scratch/pbs.$PBS_JOBID/input_data /local_scratch/pbs.$PBS_JOBID/output_data\n\n### Move output data before the job ends and your output is deleted\nmv /local_scratch/pbs.$PBS_JOBID/output_data ${SCRATCH}\n</code></pre></p>"},{"location":"compute-systems/casper/starting-casper-jobs/#script-examples","title":"Script examples","text":"<p>See this page for many Casper PBS job script examples: Casper job script examples</p> <p>When your script is ready, submit your batch job for scheduling as shown\u00a0here.</p>"},{"location":"compute-systems/casper/starting-casper-jobs/casper-job-script-examples-content/","title":"Casper job script examples content","text":"<p>Batch script to run a high-throughput computing (HTC) job on Casper</p> <p>This example shows how to create a script for running a high-throughput computing (HTC) job. Such jobs typically use only a few CPU cores and likely do not require the use of an MPI library or GPU.</p> bashtcsh <pre><code>#!/bin/bash -l\n### Job Name\n#PBS -N htc_job\n### Charging account\n#PBS -A &lt;project_code&gt;\n### Request one chunk of resources with 1 CPU and 10 GB of memory\n#PBS -l select=1:ncpus=1:mem=10GB\n### Allow job to run up to 30 minutes\n#PBS -l walltime=30:00\n### Route the job to the casper queue\n#PBS -q casper\n### Join output and error streams into single file\n#PBS -j oe\n\nexport TMPDIR=${SCRATCH}/temp\nmkdir -p ${TMPDIR}\n\n### Load Conda/Python module and activate NPL environment\nmodule load conda\nconda activate npl\n\n### Run analysis script\npython myscript.py datafile.dat\n</code></pre> <pre><code>#!/bin/tcsh\n### Job Name\n#PBS -N htc_job\n### Charging account\n#PBS -A &lt;project_code&gt;\n### Request one chunk of resources with 1 CPU and 10 GB of memory\n#PBS -l select=1:ncpus=1:mem=10GB\n### Allow job to run up to 30 minutes\n#PBS -l walltime=30:00\n### Route the job to the casper queue\n#PBS -q casper\n### Join output and error streams into single file\n#PBS -j oe\n\nsetenv TMPDIR ${SCRATCH}/temp\nmkdir -p ${TMPDIR}\n\n### Load Conda/Python module and activate NPL environment\nmodule load conda\nconda activate npl\n\n### Run analysis script\npython myscript.py datafile.dat\n</code></pre> <p>Batch script to run an MPI GPU job on Casper</p> bashtcsh <pre><code>#!/bin/bash -l\n#PBS -N mpi_job\n#PBS -A &lt;project_code&gt;\n#PBS -l select=2:ncpus=4:mpiprocs=4:ngpus=4:mem=40GB\n#PBS -l gpu_type=v100\n#PBS -l walltime=01:00:00\n#PBS -q casper\n#PBS -j oe\n\nexport TMPDIR=${SCRATCH}/temp\nmkdir -p ${TMPDIR}\n\n### Provide CUDA runtime libraries\nmodule load cuda\n\n### Run program\nmpirun ./executable_name\n</code></pre> <pre><code>#!/bin/tcsh\n### Job Name\n#PBS -N mpi_gpu_job\n### Charging account\n#PBS -A &lt;project_code&gt;\n### Request two resource chunks, each with 4 CPUs, GPUs, MPI ranks, and 40 GB of memory\n#PBS -l select=2:ncpus=4:mpiprocs=4:ngpus=4:mem=40GB\n### Specify that the GPUs will be V100s\n#PBS -l gpu_type=v100\n### Allow job to run up to 1 hour\n#PBS -l walltime=01:00:00\n### Route the job to the casper queue\n#PBS -q casper\n### Join output and error streams into single file\n#PBS -j oe\n\nsetenv TMPDIR ${SCRATCH}/temp\nmkdir -p ${TMPDIR}\n\n### Provide CUDA runtime libraries\nmodule load cuda\n\n### Run program\nmpirun ./executable_name\n</code></pre> <p>Batch script to run a pure OpenMP job on Casper</p> bashtcsh <pre><code>#!/bin/bash -l\n#PBS -N OpenMP_job\n#PBS -A &lt;project_code&gt;\n#PBS -l select=1:ncpus=8:ompthreads=8\n#PBS -l walltime=00:10:00\n#PBS -q casper\n#PBS -j oe\n\nexport TMPDIR=${SCRATCH}/temp\nmkdir -p ${TMPDIR}\n\n### Run program\n./executable_name\n</code></pre> <pre><code>#!/bin/tcsh\n#PBS -N OpenMP_job\n#PBS -A &lt;project_code&gt;\n#PBS -l select=1:ncpus=8:ompthreads=8\n#PBS -l walltime=00:10:00\n#PBS -q casper\n#PBS -j oe\n\nsetenv TMPDIR ${SCRATCH}/temp\nmkdir -p ${TMPDIR}\n\n### Run program\n./executable_name\n</code></pre> <p>Batch script to run a hybrid MPI/OpenMP job on Casper</p> bashtcsh <pre><code>#!/bin/bash -l\n#PBS -N hybrid_job\n#PBS -A &lt;project_code&gt;\n#PBS -l select=2:ncpus=8:mpiprocs=2:ompthreads=4\n#PBS -l walltime=00:10:00\n#PBS -q casper\n#PBS -j oe\n\nexport TMPDIR=${SCRATCH}/temp\nmkdir -p ${TMPDIR}\n\n### Run program\nmpirun ./executable_name\n</code></pre> <pre><code>#!/bin/tcsh\n#PBS -N hybrid_job\n#PBS -A &lt;project_code&gt;\n#PBS -l select=2:ncpus=8:mpiprocs=2:ompthreads=4\n#PBS -l walltime=00:10:00\n#PBS -q casper\n#PBS -j oe\n\nsetenv TMPDIR ${SCRATCH}/temp\nmkdir -p ${TMPDIR}\n\n### Run program\nmpirun ./executable_name\n</code></pre> <p>Batch script to run a job array on Casper</p> <p>Job arrays are useful for submitting and managing collections of similar jobs \u2013 for example, running the same program repeatedly on different input files. PBS can process a job array more efficiently than it can process the same number of individual non-array jobs.</p> <p>This example uses environment variable <code>PBS_ARRAY_INDEX</code> as an argument in running the jobs. This variable is set by the scheduler in each of your array subjobs, and spans the range of values set in the <code>#PBS -J</code> array directive.</p> bashtcsh <pre><code>#!/bin/bash -l\n#PBS -N job_array\n#PBS -A &lt;project_code&gt;\n### Each array subjob will be assigned a single CPU with 4 GB of memory\n#PBS -l select=1:ncpus=1:mem=4GB\n#PBS -l walltime=00:10:00\n#PBS -q casper\n### Request 10 subjobs with array indices spanning 2010-2020 (input year)\n#PBS -J 2010-2020\n#PBS -j oe\n\nexport TMPDIR=${SCRATCH}/temp\nmkdir -p ${TMPDIR}\n\n### Run program\n./executable_name data.year-$PBS_ARRAY_INDEX\n</code></pre> <pre><code>#!/bin/tcsh\n#PBS -N job_array\n#PBS -A &lt;project_code&gt;\n### Each array subjob will be assigned a single CPU with 4 GB of memory\n#PBS -l select=1:ncpus=1:mem=4GB\n#PBS -l walltime=01:00:00\n#PBS -q casper\n### Request 10 subjobs with array indices spanning 2010-2020 (input year)\n#PBS -J 2010-2020\n#PBS -j oe\n\nsetenv TMPDIR ${SCRATCH}/temp\nmkdir -p ${TMPDIR}\n\n### Run program\n./executable_name data.year-$PBS_ARRAY_INDEX\n</code></pre> <p>If you need to include a job ID in a subsequent <code>qsub</code> command, be sure to use quotation marks to preserve the <code>[]</code> brackets, as in this example: <pre><code>qsub -W \"depend=afterok:317485[]\" postprocess.pbs\n</code></pre></p> <p>Using NVIDIA MPS in Casper GPU jobs</p> <p>Some workflows benefit from processing more than one CUDA kernel on a GPU concurrently, as a single kernel is not sufficient to keep the GPU fully utilized. NVIDIA\u2019s Multi-Process Service (MPS) enables this capability on modern NVIDIA GPUs like the V100s on Casper.</p> <p>Consider using MPS when you are requesting more MPI tasks than physical GPUs. Particularly for jobs with large problem sizes, using multiple MPI tasks with MPS active can sometimes offer a performance boost over using a single task per GPU.</p> <p>The PBS job scheduler provides MPS support via a chunk-level resource. When you request MPS, PBS will perform the following steps on each specified chunk:</p> <ol> <li> <p>Launch the MPS control daemon on each job node.</p> </li> <li> <p>Start the MPS server on each node.</p> </li> <li> <p>Run your GPU application.</p> </li> <li> <p>Terminate the MPS server and daemon.</p> </li> </ol> <p>To enable MPS on job hosts, add <code>mps=1</code> to your select statement chunks as follows: <pre><code>#PBS -l select=1:ncpus=8:mpiprocs=8:mem=60GB:ngpus=1:mps=1\n</code></pre> On each V100 GPU, you may use MPI to launch up to 48 CUDA contexts (GPU kernels launched by MPI tasks) when using MPS. MPS can be used with OpenACC and OpenMP offload codes as well, as the compiler generates CUDA code from your directives at compile time.</p> <p>Jobs may not request MPS activation on nodes with GP100 GPUs.</p> <p>In this example, we run a CUDA Fortran program that also uses MPI. The application was compiled using the NVIDIA HPC SDK compilers, the CUDA toolkit, and Open MPI. We request all GPUs on each node and use NVIDIA MPS to use multiple MPI tasks on CPU nodes for each GPU.</p> bash <pre><code>#!/bin/bash\n#PBS -A &lt;project_code&gt;\n#PBS -N gpu_mps_job\n#PBS -q casper@casper-pbs\n#PBS -l walltime=01:00:00\n#PBS -l select=2:ncpus=36:mpiprocs=36:ngpus=4:mem=300GB:mps=1\n#PBS -l gpu_type=v100\n\n# Use scratch for temporary files to avoid space limits in /tmp\nexport TMPDIR=${SCRATCH}/temp\nmkdir -p ${TMPDIR}\n\n# Load modules to match compile-time environment\nmodule purge\nmodule load ncarenv nvhpc/22.5 cuda/11.4 openmpi/4.1.4\n\n# Run application using Open MPI\nmpirun ./executable_name\n</code></pre>"},{"location":"compute-systems/casper/starting-casper-jobs/casper-job-script-examples/","title":"Casper job Script Examples","text":"<p>These PBS batch script examples work for executables generated by any compiler and MPI library installed on Casper. (The defaults are Intel and OpenMPI, respectively.) Remember to substitute your own job name and project code, and customize the other directives and commands as necessary.</p> <p>The examples are similar to PBS examples for running jobs on Derecho. For help with any of them, contact the NCAR Research Computing help desk.</p> <p>When your script is ready, submit your batch job from a Casper login node by using the <code>qsub</code> command followed by the name of your script file. <pre><code>qsub script_name\n</code></pre></p> <p>Batch script to run a high-throughput computing (HTC) job on Casper</p> <p>This example shows how to create a script for running a high-throughput computing (HTC) job. Such jobs typically use only a few CPU cores and likely do not require the use of an MPI library or GPU.</p> bashtcsh <pre><code>#!/bin/bash -l\n### Job Name\n#PBS -N htc_job\n### Charging account\n#PBS -A &lt;project_code&gt;\n### Request one chunk of resources with 1 CPU and 10 GB of memory\n#PBS -l select=1:ncpus=1:mem=10GB\n### Allow job to run up to 30 minutes\n#PBS -l walltime=30:00\n### Route the job to the casper queue\n#PBS -q casper\n### Join output and error streams into single file\n#PBS -j oe\n\nexport TMPDIR=${SCRATCH}/temp\nmkdir -p ${TMPDIR}\n\n### Load Conda/Python module and activate NPL environment\nmodule load conda\nconda activate npl\n\n### Run analysis script\npython myscript.py datafile.dat\n</code></pre> <pre><code>#!/bin/tcsh\n### Job Name\n#PBS -N htc_job\n### Charging account\n#PBS -A &lt;project_code&gt;\n### Request one chunk of resources with 1 CPU and 10 GB of memory\n#PBS -l select=1:ncpus=1:mem=10GB\n### Allow job to run up to 30 minutes\n#PBS -l walltime=30:00\n### Route the job to the casper queue\n#PBS -q casper\n### Join output and error streams into single file\n#PBS -j oe\n\nsetenv TMPDIR ${SCRATCH}/temp\nmkdir -p ${TMPDIR}\n\n### Load Conda/Python module and activate NPL environment\nmodule load conda\nconda activate npl\n\n### Run analysis script\npython myscript.py datafile.dat\n</code></pre> <p>Batch script to run an MPI GPU job on Casper</p> bashtcsh <pre><code>#!/bin/bash -l\n#PBS -N mpi_job\n#PBS -A &lt;project_code&gt;\n#PBS -l select=2:ncpus=4:mpiprocs=4:ngpus=4:mem=40GB\n#PBS -l gpu_type=v100\n#PBS -l walltime=01:00:00\n#PBS -q casper\n#PBS -j oe\n\nexport TMPDIR=${SCRATCH}/temp\nmkdir -p ${TMPDIR}\n\n### Provide CUDA runtime libraries\nmodule load cuda\n\n### Run program\nmpirun ./executable_name\n</code></pre> <pre><code>#!/bin/tcsh\n### Job Name\n#PBS -N mpi_gpu_job\n### Charging account\n#PBS -A &lt;project_code&gt;\n### Request two resource chunks, each with 4 CPUs, GPUs, MPI ranks, and 40 GB of memory\n#PBS -l select=2:ncpus=4:mpiprocs=4:ngpus=4:mem=40GB\n### Specify that the GPUs will be V100s\n#PBS -l gpu_type=v100\n### Allow job to run up to 1 hour\n#PBS -l walltime=01:00:00\n### Route the job to the casper queue\n#PBS -q casper\n### Join output and error streams into single file\n#PBS -j oe\n\nsetenv TMPDIR ${SCRATCH}/temp\nmkdir -p ${TMPDIR}\n\n### Provide CUDA runtime libraries\nmodule load cuda\n\n### Run program\nmpirun ./executable_name\n</code></pre> <p>Batch script to run a pure OpenMP job on Casper</p> bashtcsh <pre><code>#!/bin/bash -l\n#PBS -N OpenMP_job\n#PBS -A &lt;project_code&gt;\n#PBS -l select=1:ncpus=8:ompthreads=8\n#PBS -l walltime=00:10:00\n#PBS -q casper\n#PBS -j oe\n\nexport TMPDIR=${SCRATCH}/temp\nmkdir -p ${TMPDIR}\n\n### Run program\n./executable_name\n</code></pre> <pre><code>#!/bin/tcsh\n#PBS -N OpenMP_job\n#PBS -A &lt;project_code&gt;\n#PBS -l select=1:ncpus=8:ompthreads=8\n#PBS -l walltime=00:10:00\n#PBS -q casper\n#PBS -j oe\n\nsetenv TMPDIR ${SCRATCH}/temp\nmkdir -p ${TMPDIR}\n\n### Run program\n./executable_name\n</code></pre> <p>Batch script to run a hybrid MPI/OpenMP job on Casper</p> bashtcsh <pre><code>#!/bin/bash -l\n#PBS -N hybrid_job\n#PBS -A &lt;project_code&gt;\n#PBS -l select=2:ncpus=8:mpiprocs=2:ompthreads=4\n#PBS -l walltime=00:10:00\n#PBS -q casper\n#PBS -j oe\n\nexport TMPDIR=${SCRATCH}/temp\nmkdir -p ${TMPDIR}\n\n### Run program\nmpirun ./executable_name\n</code></pre> <pre><code>#!/bin/tcsh\n#PBS -N hybrid_job\n#PBS -A &lt;project_code&gt;\n#PBS -l select=2:ncpus=8:mpiprocs=2:ompthreads=4\n#PBS -l walltime=00:10:00\n#PBS -q casper\n#PBS -j oe\n\nsetenv TMPDIR ${SCRATCH}/temp\nmkdir -p ${TMPDIR}\n\n### Run program\nmpirun ./executable_name\n</code></pre> <p>Batch script to run a job array on Casper</p> <p>Job arrays are useful for submitting and managing collections of similar jobs \u2013 for example, running the same program repeatedly on different input files. PBS can process a job array more efficiently than it can process the same number of individual non-array jobs.</p> <p>This example uses environment variable <code>PBS_ARRAY_INDEX</code> as an argument in running the jobs. This variable is set by the scheduler in each of your array subjobs, and spans the range of values set in the <code>#PBS -J</code> array directive.</p> bashtcsh <pre><code>#!/bin/bash -l\n#PBS -N job_array\n#PBS -A &lt;project_code&gt;\n### Each array subjob will be assigned a single CPU with 4 GB of memory\n#PBS -l select=1:ncpus=1:mem=4GB\n#PBS -l walltime=00:10:00\n#PBS -q casper\n### Request 10 subjobs with array indices spanning 2010-2020 (input year)\n#PBS -J 2010-2020\n#PBS -j oe\n\nexport TMPDIR=${SCRATCH}/temp\nmkdir -p ${TMPDIR}\n\n### Run program\n./executable_name data.year-$PBS_ARRAY_INDEX\n</code></pre> <pre><code>#!/bin/tcsh\n#PBS -N job_array\n#PBS -A &lt;project_code&gt;\n### Each array subjob will be assigned a single CPU with 4 GB of memory\n#PBS -l select=1:ncpus=1:mem=4GB\n#PBS -l walltime=01:00:00\n#PBS -q casper\n### Request 10 subjobs with array indices spanning 2010-2020 (input year)\n#PBS -J 2010-2020\n#PBS -j oe\n\nsetenv TMPDIR ${SCRATCH}/temp\nmkdir -p ${TMPDIR}\n\n### Run program\n./executable_name data.year-$PBS_ARRAY_INDEX\n</code></pre> <p>If you need to include a job ID in a subsequent <code>qsub</code> command, be sure to use quotation marks to preserve the <code>[]</code> brackets, as in this example: <pre><code>qsub -W \"depend=afterok:317485[]\" postprocess.pbs\n</code></pre></p> <p>Using NVIDIA MPS in Casper GPU jobs</p> <p>Some workflows benefit from processing more than one CUDA kernel on a GPU concurrently, as a single kernel is not sufficient to keep the GPU fully utilized. NVIDIA\u2019s Multi-Process Service (MPS) enables this capability on modern NVIDIA GPUs like the V100s on Casper.</p> <p>Consider using MPS when you are requesting more MPI tasks than physical GPUs. Particularly for jobs with large problem sizes, using multiple MPI tasks with MPS active can sometimes offer a performance boost over using a single task per GPU.</p> <p>The PBS job scheduler provides MPS support via a chunk-level resource. When you request MPS, PBS will perform the following steps on each specified chunk:</p> <ol> <li> <p>Launch the MPS control daemon on each job node.</p> </li> <li> <p>Start the MPS server on each node.</p> </li> <li> <p>Run your GPU application.</p> </li> <li> <p>Terminate the MPS server and daemon.</p> </li> </ol> <p>To enable MPS on job hosts, add <code>mps=1</code> to your select statement chunks as follows: <pre><code>#PBS -l select=1:ncpus=8:mpiprocs=8:mem=60GB:ngpus=1:mps=1\n</code></pre> On each V100 GPU, you may use MPI to launch up to 48 CUDA contexts (GPU kernels launched by MPI tasks) when using MPS. MPS can be used with OpenACC and OpenMP offload codes as well, as the compiler generates CUDA code from your directives at compile time.</p> <p>Jobs may not request MPS activation on nodes with GP100 GPUs.</p> <p>In this example, we run a CUDA Fortran program that also uses MPI. The application was compiled using the NVIDIA HPC SDK compilers, the CUDA toolkit, and Open MPI. We request all GPUs on each node and use NVIDIA MPS to use multiple MPI tasks on CPU nodes for each GPU.</p> bash <pre><code>#!/bin/bash\n#PBS -A &lt;project_code&gt;\n#PBS -N gpu_mps_job\n#PBS -q casper@casper-pbs\n#PBS -l walltime=01:00:00\n#PBS -l select=2:ncpus=36:mpiprocs=36:ngpus=4:mem=300GB:mps=1\n#PBS -l gpu_type=v100\n\n# Use scratch for temporary files to avoid space limits in /tmp\nexport TMPDIR=${SCRATCH}/temp\nmkdir -p ${TMPDIR}\n\n# Load modules to match compile-time environment\nmodule purge\nmodule load ncarenv nvhpc/22.5 cuda/11.4 openmpi/4.1.4\n\n# Run application using Open MPI\nmpirun ./executable_name\n</code></pre>"},{"location":"compute-systems/cheyenne/","title":"Cheyenne supercomputer","text":"<p>Cheyenne is a 5.34-petaflops, high-performance computer built for NCAR by SGI. The system was released for production work on January 12, 2017.</p> <p></p> <p>An SGI ICE XA Cluster, the Cheyenne supercomputer features 145,152 Intel Xeon processor cores in 4,032 dual-socket nodes (36 cores/node) and 313 TB of total memory.</p> <p>Cheyenne's login nodes give users access to the GLADE shared-disk resource and other storage systems.</p> <p>Data storage components provided by DataDirect Networks (DDN) give the GLADE system a total usable capacity of 38 PB. The DDN system transfers data at the rate of 200 GBps, more than twice as fast as the previous file system\u2019s rate of 90 GBps.</p> <p>Go to Quick start on Cheyenne</p>"},{"location":"compute-systems/cheyenne/#logging-in","title":"Logging in","text":"<p>To log in, start your terminal or Secure Shell client and run an <code>ssh</code> command as shown here: <pre><code>ssh -X username@system_name.ucar.edu\n</code></pre> OR <pre><code>ssh -X username@system_name.ucar.edu\n</code></pre></p> <p>Some users (particularly on Macs) need to use <code>-Y</code> instead of <code>-X</code> when calling <code>ssh</code> to enable <code>X11 forwarding</code>.</p> <p>You can use this shorter command if your username for the system is the same as your username on your local computer: <pre><code>ssh -X system_name.ucar.edu\n</code></pre> OR <pre><code>ssh -X system_name.ucar.edu\n</code></pre> After running the ssh command, you will be asked to authenticate to finish logging in.</p>"},{"location":"compute-systems/cheyenne/#hardware","title":"Hardware","text":"145,152\u00a0processor cores 2.3-GHz Intel\u00a0Xeon E5-2697V4 (Broadwell) processors16 flops per clock 4,032 computation nodes Dual-socket nodes, 18 cores per socket 6 login nodes Dual-socket nodes, 18 cores per socket256 GB memory/node 313 TB total system memory 64 GB/node on 3,168 nodes, DDR4-2400128 GB/node on 864 nodes, DDR4-2400 Mellanox\u00a0EDR InfiniBandhigh-speed interconnect Partial 9D Enhanced Hypercube single-plane interconnect topologyBandwidth: 25 GBps bidirectional per linkLatency: MPI ping-pong &lt; 1 \u00b5s; hardware link 130 ns 3 times Yellowstone computational capacity Comparison based on the relative performance of\u00a0CISL High Performance Computing Benchmarks\u00a0run on each system. &gt; 3.5 times Yellowstone peak performance 5.34 peak petaflops (vs. 1.504)"},{"location":"compute-systems/cheyenne/#estimating-core-hours-needed","title":"Estimating core-hours needed","text":"<p>Cheyenne allocations are made in core-hours. The recommended method for estimating your resource needs for an allocation request is to perform benchmark runs. Some guidance is provided here.</p> <p>The core-hours used for a job are calculated by multiplying the number of processor cores used by the wall-clock duration in hours. Cheyenne core-hour calculations should assume that all jobs will run in the regular queue and that they are charged for use of all 36 cores on each node.</p> <p>Just a copy of https://arc.ucar.edu/knowledge_base/70549542</p>"},{"location":"compute-systems/cheyenne/cheyenne/","title":"Cheyenne supercomputer","text":"<p>Cheyenne is a 5.34-petaflops, high-performance computer built for NCAR by SGI. The system was released for production work on January 12, 2017.</p> <p></p> <p>An SGI ICE XA Cluster, the Cheyenne supercomputer features 145,152 Intel Xeon processor cores in 4,032 dual-socket nodes (36 cores/node) and 313 TB of total memory.</p> <p>Cheyenne's login nodes give users access to the GLADE shared-disk resource and other storage systems.</p> <p>Data storage components provided by DataDirect Networks (DDN) give the GLADE system a total usable capacity of 38 PB. The DDN system transfers data at the rate of 200 GBps, more than twice as fast as the previous file system\u2019s rate of 90 GBps.</p> <p>Go to Quick start on Cheyenne</p>"},{"location":"compute-systems/cheyenne/cheyenne/#logging-in","title":"Logging in","text":"<p>To log in, start your terminal or Secure Shell client and run an <code>ssh</code> command as shown here: <pre><code>ssh -X username@system_name.ucar.edu\n</code></pre> OR <pre><code>ssh -X username@system_name.ucar.edu\n</code></pre></p> <p>Some users (particularly on Macs) need to use <code>-Y</code> instead of <code>-X</code> when calling <code>ssh</code> to enable <code>X11 forwarding</code>.</p> <p>You can use this shorter command if your username for the system is the same as your username on your local computer: <pre><code>ssh -X system_name.ucar.edu\n</code></pre> OR <pre><code>ssh -X system_name.ucar.edu\n</code></pre> After running the ssh command, you will be asked to authenticate to finish logging in.</p>"},{"location":"compute-systems/cheyenne/cheyenne/#hardware","title":"Hardware","text":"145,152\u00a0processor cores 2.3-GHz Intel\u00a0Xeon E5-2697V4 (Broadwell) processors16 flops per clock 4,032 computation nodes Dual-socket nodes, 18 cores per socket 6 login nodes Dual-socket nodes, 18 cores per socket256 GB memory/node 313 TB total system memory 64 GB/node on 3,168 nodes, DDR4-2400128 GB/node on 864 nodes, DDR4-2400 Mellanox\u00a0EDR InfiniBandhigh-speed interconnect Partial 9D Enhanced Hypercube single-plane interconnect topologyBandwidth: 25 GBps bidirectional per linkLatency: MPI ping-pong &lt; 1 \u00b5s; hardware link 130 ns 3 times Yellowstone computational capacity Comparison based on the relative performance of\u00a0CISL High Performance Computing Benchmarks\u00a0run on each system. &gt; 3.5 times Yellowstone peak performance 5.34 peak petaflops (vs. 1.504)"},{"location":"compute-systems/cheyenne/cheyenne/#estimating-core-hours-needed","title":"Estimating core-hours needed","text":"<p>Cheyenne allocations are made in core-hours. The recommended method for estimating your resource needs for an allocation request is to perform benchmark runs. Some guidance is provided here.</p> <p>The core-hours used for a job are calculated by multiplying the number of processor cores used by the wall-clock duration in hours. Cheyenne core-hour calculations should assume that all jobs will run in the regular queue and that they are charged for use of all 36 cores on each node.</p> <p>Just a copy of https://arc.ucar.edu/knowledge_base/70549542</p>"},{"location":"compute-systems/cheyenne/starting-cheyenne-jobs/cheyenne-job-script-examples/","title":"Cheyenne job script examples","text":"<p>When you use any of these examples, remember to substitute your own job name and project code, and customize the other directives and commands as necessary. That includes the commands shown for setting TMPDIR in your batch scripts as recommended here:\u00a0Storing temporary files with TMPDIR.</p> <p>Specify an\u00a0ompthreads\u00a0value in your script to help ensure that any parallel job will run properly. This is particularly important in a couple of cases:</p> <ul> <li>If your code was compiled with the GNU or Intel compiler using the\u00a0<code>-f openmp</code>\u00a0option.</li> <li>If your code was compiled with a PGI compiler with option\u00a0<code>-mp</code>.</li> </ul> <p>Tip</p> <p>For a parallel job that does not use OpenMP threads \u2013 for a pure MPI job, for example \u2013 specify <code>ompthreads=1</code>\u00a0in your\u00a0PBS select statement as shown below. Failure to do so may result in the job oversubscribing its nodes, resulting in poor performance or puzzling behavior such as exceeding its wallclock limit.</p> <p>Load all modules that are necessary to run your program at the start of your batch scripts by including a line like this:</p> <pre><code>module load intel mpt\n</code></pre> <p>If you think you might run a particular compiled executable well into the future, we advise that you load specific versions of desired modules to ensure reproducibility. Follow this example: <pre><code>module load intel/19.1.1 mpt/2.25\n</code></pre></p> <p>These examples are based on the following assumptions:</p> <ol> <li>You are using HPE's Message Passing Toolkit (MPT) MPI library.</li> <li>The\u00a0programs being run were compiled with Intel 16.0.3 or a later version.</li> </ol> <p>Contact the\u00a0NCAR Research Computing help desk\u00a0for assistance with adapting them for other cases.</p> <p>When your script is ready, submit your batch job for scheduling as shown\u00a0here.</p>"},{"location":"compute-systems/cheyenne/starting-cheyenne-jobs/cheyenne-job-script-examples/#page-contents","title":"Page contents","text":"<ul> <li>Batch script to run an MPI job</li> <li>Batch script to run a pure OpenMP job</li> <li>Batch script to run a hybrid MPI/OpenMP job</li> <li>Batch script to run a job array</li> <li>Batch script to run a command file (MPMD) job</li> <li>Pinning tasks/threads to CPUs</li> <li>Dependent jobs</li> </ul>"},{"location":"compute-systems/cheyenne/starting-cheyenne-jobs/cheyenne-job-script-examples/#batch-script-to-run-an-mpi-job","title":"Batch script to run an MPI job","text":"tcshbash <pre><code>#!/bin/tcsh\n\n### Job Name\n#PBS -N mpi_job\n### Project code\n#PBS -A project_code\n#PBS -l walltime=01:00:00\n#PBS -q queue_name\n### Merge output and error files\n#PBS -j oe\n#PBS -k eod\n### Select 2 nodes with 36 CPUs each for a total of 72 MPI processes\n#PBS -l select=2:ncpus=36:mpiprocs=36:ompthreads=1\n### Send email on abort, begin and end\n#PBS -m abe\n### Specify mail recipient\n#PBS -M email_address\n\nsetenv TMPDIR /glade/scratch/$USER/temp\nmkdir -p $TMPDIR\n\n### Run the executable\nmpiexec_mpt ./executable_name.exe\n</code></pre> <pre><code>#!/bin/bash\n\n### Job Name\n#PBS -N mpi_job\n### Project code\n#PBS -A project_code\n#PBS -l walltime=01:00:00\n#PBS -q queue_name\n### Merge output and error files\n#PBS -j oe\n#PBS -k eod\n### Select 2 nodes with 36 CPUs each for a total of 72 MPI processes\n#PBS -l select=2:ncpus=36:mpiprocs=36:ompthreads=1\n### Send email on abort, begin and end\n#PBS -m abe\n### Specify mail recipient\n#PBS -M email_address\n\nexport TMPDIR=/glade/scratch/$USER/temp\nmkdir -p $TMPDIR\n\n### Run the executable\nmpiexec_mpt ./executable_name.exe\n</code></pre>"},{"location":"compute-systems/cheyenne/starting-cheyenne-jobs/cheyenne-job-script-examples/#batch-script-to-run-a-pure-openmp-job","title":"Batch script to run a pure OpenMP job","text":"<p>To run a pure OpenMP job, specify the number of CPUs you want from the node (<code>ncpus</code>). Also specify the number of threads (<code>ompthreads</code>) or <code>OMP_NUM_THREADS</code> will default to the value of ncpus, possibly resulting in poor performance.</p> <p>Warning</p> <p>You will be charged for use of all CPUs on the node when using an exclusive queue.</p> tcshbash <pre><code>#!/bin/tcsh\n#PBS -A project_code\n#PBS -N OpenMP_job\n#PBS -j oe\n#PBS -k eod\n#PBS -m abe\n#PBS -M email_address\n#PBS -q queue_name\n#PBS -l walltime=01:00:00\n### Request 10 CPUS for 10 threads\n#PBS -l select=1:ncpus=10:ompthreads=10\n\nsetenv TMPDIR /glade/scratch/$USER/temp\nmkdir -p $TMPDIR\n\n### Run OpenMP program\n./executable_name\n</code></pre> <pre><code>#!/bin/bash\n#PBS -A project_code\n#PBS -N OpenMP_job\n#PBS -j oe\n#PBS -k eod\n#PBS -m abe\n#PBS -M email_address\n#PBS -q queue_name\n#PBS -l walltime=01:00:00\n### Request 10 CPUS for 10 threads\n#PBS -l select=1:ncpus=10:ompthreads=10\n\nexport TMPDIR=/glade/scratch/$USER/temp\nmkdir -p $TMPDIR\n\n### Run OpenMP program\n./executable_name\n</code></pre>"},{"location":"compute-systems/cheyenne/starting-cheyenne-jobs/cheyenne-job-script-examples/#batch-script-to-run-a-hybrid-mpiopenmp-job","title":"Batch script to run a hybrid MPI/OpenMP job","text":"<p>If you want to run a hybrid MPI/OpenMP configuration where each node uses threaded parallelism while the nodes communicate with each other using MPI, activate NUMA mode and run using the MPI launcher.</p> <p>Specify the number of CPUs you want from each node (<code>ncpus</code>). Also specify the number of threads (<code>ompthreads</code>) or <code>OMP_NUM_THREADS</code> will default to the value of ncpus, possibly resulting in poor performance.</p> <p>Warning</p> <p>You will be charged for use of all CPUs on the node when using an exclusive queue.</p> tcshbash <pre><code>#!/bin/tcsh\n#PBS -A project_code\n#PBS -N hybrid_job\n#PBS -j oe\n#PBS -k eod\n#PBS -m abe\n#PBS -M email_address\n#PBS -q queue_name\n#PBS -l walltime=01:00:00\n### Request two nodes, each with one MPI task and 36 threads\n#PBS -l select=2:ncpus=36:mpiprocs=1:ompthreads=36\n\nsetenv TMPDIR /glade/scratch/$USER/temp\nmkdir -p $TMPDIR\n\n### Run the hybrid OpenMP/MPI program\nmpiexec_mpt omplace ./executable_name\n</code></pre> <pre><code>#!/bin/bash\n#PBS -A project_code\n#PBS -N hybrid_job\n#PBS -j oe\n#PBS -k eod\n#PBS -m abe\n#PBS -M email_address\n#PBS -q queue_name\n#PBS -l walltime=01:00:00\n### Request two nodes, each with one MPI task and 36 threads\n#PBS -l select=2:ncpus=36:mpiprocs=1:ompthreads=36\n\nexport TMPDIR=/glade/scratch/$USER/temp\nmkdir -p $TMPDIR\n\n### Run the hybrid OpenMP/MPI program\nmpiexec_mpt omplace ./executable_name\n</code></pre>"},{"location":"compute-systems/cheyenne/starting-cheyenne-jobs/cheyenne-job-script-examples/#batch-script-to-run-a-job-array","title":"Batch script to run a job array","text":"<p>Job arrays are useful when you want to run the same program repeatedly on different input files. PBS can process a job array more efficiently than it can process the same number of individual non-array jobs. The elements in a job array are known as \"sub-jobs.\"</p> <p>Before submitting this batch script:\u00a0Place files input.1 through input.18 in the same directory where you have the sequential\u00a0cmd\u00a0command. The batch job specifies 18 sub-jobs indexed 1-18 that will run in the \"share\" queue. The Nth sub-job uses file input.N to produce file output.N. The \"share\" queue is recommended for running job arrays of sequential sub-jobs, or parallel sub-jobs each having from two to nine tasks. The share queue has a per-user limit of 18 sub-jobs per array.</p> tcshbash <pre><code>#!/bin/tcsh\n### Job Name\n#PBS -N job_arrays\n### Project code\n#PBS -A project_code\n#PBS -l walltime=01:00:00\n#PBS -q share\n### Merge output and error files\n#PBS -j oe\n#PBS -k eod\n### Select one CPU\n#PBS -l select=1:ncpus=1\n### Specify index range of sub-jobs\n#PBS -J 1-18\n\nsetenv TMPDIR /glade/scratch/$USER/temp\nmkdir -p $TMPDIR\n\n# Execute subjob for index PBS_ARRAY_INDEX\ncmd input.$PBS_ARRAY_INDEX &gt; output.$PBS_ARRAY_INDEX\n</code></pre> <pre><code>#!/bin/bash\n### Job Name\n#PBS -N job_arrays\n### Project code\n#PBS -A project_code\n#PBS -l walltime=01:00:00\n#PBS -q share\n### Merge output and error files\n#PBS -j oe\n#PBS -k eod\n### Select one CPU\n#PBS -l select=1:ncpus=1\n### Specify index range of sub-jobs\n#PBS -J 1-18\n\nexport TMPDIR=/glade/scratch/$USER/temp\nmkdir -p $TMPDIR\n\n# Execute subjob for index PBS_ARRAY_INDEX\ncmd input.$PBS_ARRAY_INDEX &gt; output.$PBS_ARRAY_INDEX\n</code></pre> <p>Tip</p> <p>If you need to include a job ID in a subsequent\u00a0<code>qsub</code>\u00a0command as in the following example, be sure to use quotation marks to preserve the\u00a0<code>[]</code>\u00a0brackets:</p> <p>``     qsub -W \"depend=afterok:317485[]\" postprocess.pbs ```</p>"},{"location":"compute-systems/cheyenne/starting-cheyenne-jobs/cheyenne-job-script-examples/#batch-script-to-run-a-command-file-mpmd-job","title":"Batch script to run a command file (MPMD) job","text":"<p>Multiple Program, Multiple Data (MPMD) jobs run multiple independent, sequential executables simultaneously. The executable commands appear in the command file (<code>cmdfile</code>) on separate lines. The command file, the executable files, and the input files should reside in the directory from which the job is submitted. If they don't, you need to specify adequate relative or full pathnames in both your command file and job scripts.</p> <p>The command file used in the example job scripts has these four lines. <pre><code>./cmd1.exe &lt; input1 &gt; output1\n\n./cmd2.exe &lt; input2 &gt; output2\n\n./cmd3.exe &lt; input3 &gt; output3\n\n./cmd4.exe &lt; input4 &gt; output4\n</code></pre> The job will produce output files that reside in the directory in which the job was submitted.</p> <p>In place of executables, you can specify independent shell scripts, MATLAB scripts, or others, or you can mix and match executables with scripts. Each task should execute in about the same wall-clock time as the others.</p> <p>Tip</p> <p>If any of your command file lines invoke a utility such as IDL, MATLAB, NCL, R and so on, invoke it in batch mode rather than interactive mode or your job will hang until it reaches the specified walltime limit. See the user guide for the utility for how to invoke it in batch mode.</p> tcshbash <pre><code>#!/bin/tcsh\n#PBS -A project_code\n#PBS -N cmd_file\n#PBS -j oe\n#PBS -k eod\n#PBS -m abe\n#PBS -M email_address\n#PBS -q queue_name\n#PBS -l walltime=01:00:00\n### Request one chunk with ncpus and mpiprocs set to\n### the number of lines in the command file\n#PBS -l select=1:ncpus=4:mpiprocs=4:ompthreads=1\n\nsetenv TMPDIR /glade/scratch/$USER/temp\nmkdir -p $TMPDIR\n\n# yyyy-mm-dd Context: Cheyenne MPT command file job.\n# Do not propagate this use of MPI_SHEPHERD\n# to other MPT jobs as it may cause\n# significant slowdown or timeout.\n# Contact the CISL Consulting Services Group\n# if you have questions about this.\nsetenv MPI_SHEPHERD true\n\nmpiexec_mpt launch_cf.sh cmdfile\n</code></pre> <pre><code>#!/bin/bash\n#PBS -A project_code\n#PBS -N cmd_file\n#PBS -j oe\n#PBS -k eod\n#PBS -m abe\n#PBS -M email_address\n#PBS -q queue_name\n#PBS -l walltime=01:00:00\n### Request one chunk with ncpus and mpiprocs set to\n### the number of lines in the command file\n#PBS -l select=1:ncpus=4:mpiprocs=4:ompthreads=1\n\nexport TMPDIR=/glade/scratch/$USER/temp\nmkdir -p $TMPDIR\n\n# yyyy-mm-dd Context: Cheyenne MPT command file job.\n# Do not propagate this use of MPI_SHEPHERD\n# to other MPT jobs as it may cause\n# significant slowdown or timeout.\n# Contact the CISL Consulting Services Group\n# if you have questions about this.\nexport MPI_SHEPHERD=true\n\nmpiexec_mpt launch_cf.sh cmdfile\n</code></pre> <p>Not a complete copy of : https://arc.ucar.edu/knowledge_base/72581486</p>"},{"location":"compute-systems/derecho/","title":"Derecho","text":"<p>Installed in 2023, Derecho is NCAR's latest supercomputer.  Derecho features 2,488 compute nodes with 128 AMD Milan cores per node and 82 nodes with four NVIDIA A100 GPUs each. The HPE Cray EX cluster is a 19.87-petaflops system that is expected to deliver about 3.5 times the scientific throughput of the Cheyenne system. Additional hardware details are available below.</p> <p></p> <p>Derecho Monthly Maintenance Windows</p> <p>The first Tuesday of each month is generally be reserved for Derecho system maintenance, as required to incorporate updates to Derecho's configuration and software during the early phase of the system lifespan. Scheduled outages may occasionally be extended (or canceled) as necessary on a case by case basis.</p> <p>NCAR/CISL works closely with the system vendor to determine the availability and criticality of software updates, and will re-evaluate the necessity of these monthly outages as the system and supporting software matures.</p> <p>Estimating Derecho Allocation Needs</p> <p>Derecho users can expect to see a 1.3x improvement over the Cheyenne system's performance on a core-for-core basis. Therefore, to estimate how many CPU core-hours will be needed for a project on Derecho, multiply the total for a Cheyenne project by 0.77.</p> <p>When requesting an allocation for Derecho GPU nodes, please make your request in terms of GPU-hours (number of GPUs used x wallclock hours). We encourage researchers to estimate GPU-hour needs by making test/benchmark runs on Casper GPUs, but will accept estimates based on runs on comparable non-NCAR, GPU-based systems.</p>"},{"location":"compute-systems/derecho/#quick-start","title":"Quick Start","text":""},{"location":"compute-systems/derecho/#logging-in","title":"Logging in","text":"<p>Once you have an account, have reviewed the Derecho Use Policies, and have a Derecho resource allocation you can log in and run jobs on the Derecho data analysis and visualization cluster.</p> <p>To log in, start your terminal or Secure Shell client and run an ssh command as shown here: <pre><code>ssh -X username@derecho.hpc.ucar.edu\n</code></pre></p> <p>Some users (particularly on Macs) need to use <code>-Y</code> instead of <code>-X</code> when calling SSH to enable X11 forwarding.</p> <p>You can omit <code>username</code> in the command above if your Derecho username is the same as your  username on your local computer.</p> <p>After running the <code>ssh</code> command, you will be asked to authenticate to finish logging in.</p> <p>Derecho has full access to NCAR storage resources, including GLADE. Users can transfer data to and from Derecho.</p> <p>To run data analysis and visualization jobs on the Derecho system's nodes, follow the procedures described here. There is no need to transfer output files from Derecho for this since Derecho and Casper mount the same <code>GLADE</code> file systems.</p> <p>Don\u2019t run <code>sudo</code> on NCAR systems!</p> <p>If you need help with tasks that you think require <code>sudo</code> privileges, or if you aren\u2019t sure, please contact HPC User Support before trying to run <code>sudo</code> yourself. The command fails when unauthorized users run it and sends a security alert to system administrators.</p>"},{"location":"compute-systems/derecho/#environment","title":"Environment","text":"<p>The Derecho HPC system uses a Cray variant of SUSE Enterprise Linux and supports widely used shells on its login and compute nodes. Users also have several compiler and MPI library choices.</p>"},{"location":"compute-systems/derecho/#shells","title":"Shells","text":"<p>The default login shell for new Derecho users is <code>bash</code>. You can change the default after logging in to the Systems Accounting Manager (SAM). It may take several hours for a change you make to take effect. You can confirm which shell is set as your default by entering <code>echo $SHELL</code> on your Derecho command line.</p>"},{"location":"compute-systems/derecho/#environment-modules","title":"Environment modules","text":"<p>The Derecho <code>module</code> utility enables users to easily load and unload compilers and compatible software packages as needed, and to create multiple customized environments for various tasks. See the Environment modules page for a general discussion of <code>module</code> usage.  Derecho's default module environment is listed here.</p>"},{"location":"compute-systems/derecho/#accessing-software-and-compiling-code","title":"Accessing software and compiling code","text":"<p>Derecho users have access to Intel, NVIDIA, and GNU compilers. The Intel compiler and OpenMPI modules are loaded by default and provide access to pre-compiled HPC Software and Data Analysis and Visualization Resources.</p> <p>See this page for a full discussion of compiling on Derecho.</p> <p>Many Derecho data analysis and AI/ML workflows benefit instead from using Conda, especially NCAR's Python Library (NPL) or to gain access to several Machine Learning Frameworks.</p>"},{"location":"compute-systems/derecho/#running-jobs-on-derecho","title":"Running jobs on Derecho","text":"<p>Users can run a variety of types of jobs on Derecho, including both traditional batch jobs submitted through PBS and also interactive and/or graphics-intensive analysis, often through remote desktops on Derecho.</p>"},{"location":"compute-systems/derecho/#job-scripts","title":"Job scripts","text":"<p>Job scripts are discussed broadly here. Users already familiar with PBS and batch submission may find Derecho-specific PBS job scripts helpful in porting their work.</p>"},{"location":"compute-systems/derecho/#derecho-hardware","title":"Derecho Hardware","text":"323,712 processor cores 3<sup>rd</sup>\u00a0Gen AMD EPYC\u2122 7763 Milan processors 2,488 CPU-only computation nodes Dual-socket nodes, 64 cores per socket256\u00a0GB DDR4 memory per node 82 GPU nodes Single-socket nodes, 64 cores per socket512\u00a0GB DDR4 memory per node4 NVIDIA 1.41 GHz A100 Tensor Core GPUs per node600 GB/s NVIDIA NVLink GPU interconnect 328 total A100 GPUs 40GB HBM2 memory per GPU600 GB/s NVIDIA NVLink GPU interconnect 6 CPU login nodes Dual-socket nodes with AMD EPYC\u2122 7763 Milan CPUs64 cores per socket512 GB DDR4-3200 memory 2 GPU development and testing nodes Dual-socket nodes with AMD EPYC\u2122 7543 Milan CPUs32 cores per socket2\u00a0NVIDIA 1.41 GHz A100 Tensor Core GPUs per node512 GB DDR4-3200 memory 692 TB total system memory 637 TB DDR4 memory on 2,488 CPU nodes42 TB DDR4 memory on 82 GPU nodes13 TB HBM2 memory on 82 GPU nodes HPE Slingshot v11 high-speed interconnect Dragonfly topology,\u00a0200 Gb/sec per port per direction1.7-2.6 usec\u00a0MPI latencyCPU-only nodes - one Slingshot injection portGPU nodes - 4 Slingshot injection ports per node ~3.5 times Cheyenne computational capacity Comparison based on the relative performance of CISL's\u00a0High Performance Computing Benchmarks\u00a0run on each system. &gt; 3.5 times\u00a0Cheyenne peak performance 19.87 peak petaflops (vs 5.34)"},{"location":"compute-systems/derecho/#status","title":"Status","text":""},{"location":"compute-systems/derecho/#nodes","title":"Nodes","text":""},{"location":"compute-systems/derecho/#queues","title":"Queues","text":""},{"location":"compute-systems/derecho/derecho-modules-list/","title":"Derecho modules list","text":""},{"location":"compute-systems/derecho/derecho-modules-list/#derecho-default-module-list","title":"Derecho default module list","text":"<pre><code>-------------------------- Module Stack Environments ---------------------------\n   ncarenv/23.06 (S,L,D)    ncarenv/23.09 (S)\n\n------------------------- Compilers and Core Software --------------------------\n   apptainer/1.1.7            intel-classic/2023.0.0\n   arm-forge/22.1.3           intel-oneapi/2023.0.0\n   atp/3.14.18                intel/2023.0.0         (L)\n   atp/3.15.0          (D)    linaro-forge/23.0\n   cce/15.0.1                 matlab/R2023a\n   cdo/2.1.1                  nccmp/1.9.0.1\n   charliecloud/0.32          ncl/6.6.2\n   cmake/3.26.3               nco/5.1.4\n   conda/latest               ncview/2.1.8\n   cray-ccdb/4.12.13          ncvis/2022.08.28\n   cray-dyninst/12.1.1        nvhpc/21.3\n   cray-mrnet/5.0.4           nvhpc/23.1             (D)\n   cray-stat/4.11.13          nvhpc/23.5\n   craype/2.7.20       (L)    papi/7.0.0.1\n   cuda/11.7.1                pcre/8.45\n   cudnn/8.5.0.96-11.7        peak-memusage/3.0.1\n   cutensor/1.7.0.1           perftools-base/23.03.0\n   darshan-util/3.4.2         perl/5.36.0\n   gcc/12.2.0                 podman/4.3.1\n   gdb4hpc/4.14.7             sanitizers4hpc/1.0.4\n   go/1.20.3                  texlive/20220321\n   grads/2.2.1                valgrind4hpc/2.12.11\n   idl/8.9.0                  vtune/2023.0.0\n\n-------------------- Compiler-dependent Software - [oneapi] --------------------\n   cray-libsci/23.02.1.1        intel-mpi/2021.8.0\n   cray-mpich/8.1.25     (L)    mkl/2023.0.0\n   eccodes/2.25.0               mpi-serial/2.3.0\n   fftw/3.3.10                  ncarcompilers/1.0.0 (L)\n   geos/3.9.1                   netcdf/4.9.2        (L)\n   hdf/4.2.15                   proj/8.2.1\n   hdf5/1.12.2           (L)    udunits/2.2.28\n\n---------------- MPI-dependent Software - [oneapi + cray-mpich] ----------------\n   darshan-runtime/3.4.2        hdf5-mpi/1.12.2           parallelio/2.6.0\n   esmf/8.4.2                   netcdf-mpi/4.9.2          parallelio/2.6.1\n   esmf/8.5.0            (D)    parallel-netcdf/1.12.3    parallelio/2.6.2 (D)\n   fftw-mpi/3.3.10              parallelio/1.10.1\n   gptl/8.1.1                   parallelio/2.5.10\n\n  Where:\n   D:  Default Module\n   L:  Module is loaded\n   S:  Module is Sticky, requires --force to unload or purge\n\nIf the avail list is too long consider trying:\n\n\"module --default avail\" or \"ml -d av\" to just list the default modules.\n\"module overview\" or \"ml ov\" to display the number of modules for each name.\n\nUse \"module spider\" to find all possible modules and extensions.\nUse \"module keyword key1 key2 ...\" to search for all possible modules matching\nany of the \"keys\".\n</code></pre>"},{"location":"compute-systems/derecho/derecho-modules-list/#derecho-complete-module-listing","title":"Derecho complete module listing","text":"<pre><code>----------------------------------------------------------------------------\nThe following is a list of the modules and extensions currently available:\n----------------------------------------------------------------------------\n  apptainer: apptainer/1.1.7, apptainer/1.1.9\n  arm-forge: arm-forge/22.1.3\n  atp: atp/3.14.18, atp/3.15.0\n  cce: cce/15.0.1\n  cdo: cdo/2.1.1, cdo/2.2.2\n  charliecloud: charliecloud/0.32, charliecloud/0.33\n  cmake: cmake/3.26.3\n  conda: conda/latest\n  cp2k: cp2k/2023.2\n  cray-ccdb: cray-ccdb/4.12.13\n  cray-dyninst: cray-dyninst/12.1.1\n  cray-libsci: cray-libsci/23.02.1.1\n  cray-mpich: cray-mpich/8.1.25\n  cray-mrnet: cray-mrnet/5.0.4\n  cray-stat: cray-stat/4.11.13\n  craype: craype/2.7.20\n  cuda: cuda/11.7.1, cuda/11.8.0, cuda/12.2.1\n  cudnn: cudnn/8.5.0.96-11.7, cudnn/8.7.0.84-11.8\n  cutensor: cutensor/1.7.0.1\n  darshan-runtime: darshan-runtime/3.4.2\n  darshan-util: darshan-util/3.4.2\n  eccodes: eccodes/2.25.0\n  ecflow: ecflow/5.8.3\n  esmf: esmf/8.4.2, esmf/8.5.0\n  fftw: fftw/3.3.10\n  fftw-mpi: fftw-mpi/3.3.10\n  gcc: gcc/12.2.0\n  gdal: gdal/3.6.4, gdal/3.7.1\n  gdb4hpc: gdb4hpc/4.14.7\n  geos: geos/3.9.1\n  go: go/1.20.3, go/1.20.6\n  gptl: gptl/8.1.1\n  grads: grads/2.2.1, grads/2.2.3\n  hdf: hdf/4.2.15\n  hdf5: hdf5/1.12.2\n  hdf5-mpi: hdf5-mpi/1.12.2\n  idl: idl/8.9.0\n  intel: intel/2023.0.0, intel/2023.2.1\n  intel-classic: intel-classic/2023.0.0, intel-classic/2023.2.1\n  intel-mpi: intel-mpi/2021.8.0, intel-mpi/2021.10.0\n  intel-oneapi: intel-oneapi/2023.0.0, intel-oneapi/2023.2.1\n  julia: julia/1.9.2\n  linaro-forge: linaro-forge/23.0\n  matlab: matlab/R2023a\n  mkl: mkl/2023.0.0, mkl/2023.2.0\n  mpi-serial: mpi-serial/2.3.0\n  mpifileutils: mpifileutils/0.11.1\n  mvapich: mvapich/3.0b\n  ncarcompilers: ncarcompilers/1.0.0\n  ncarenv: ncarenv/23.06, ncarenv/23.09\n  nccmp: nccmp/1.9.0.1, nccmp/1.9.1.0\n  ncl: ncl/6.6.2\n  nco: nco/5.1.4, nco/5.1.6\n  ncview: ncview/2.1.8, ncview/2.1.9\n  ncvis: ncvis/2022.08.28\n  netcdf: netcdf/4.9.2\n  netcdf-mpi: netcdf-mpi/4.9.2\n  nvhpc: nvhpc/21.3, nvhpc/23.1, nvhpc/23.5, nvhpc/23.7\n  osu-micro-benchmarks: osu-micro-benchmarks/7.1-1, ...\n  papi: papi/7.0.0.1\n  parallel-netcdf: parallel-netcdf/1.12.3\n  parallelio: parallelio/1.10.1, parallelio/2.5.10, parallelio/2.6.0, ...\n  pcre: pcre/8.45\n  peak-memusage: peak-memusage/3.0.1\n  perftools: perftools\n  perftools-base: perftools-base/23.03.0\n  perftools-lite: perftools-lite\n  perftools-lite-events: perftools-lite-events\n  perftools-lite-gpu: perftools-lite-gpu\n  perftools-lite-hbm: perftools-lite-hbm\n  perftools-lite-loops: perftools-lite-loops\n  perftools-preload: perftools-preload\n  perl: perl/5.36.0, perl/5.38.0\n  podman: podman/4.3.1, podman/4.5.1\n  proj: proj/8.2.1\n  rstudio: rstudio/2023.09.0\n  sanitizers4hpc: sanitizers4hpc/1.0.4\n  superlu: superlu/5.3.0\n  superlu-dist: superlu-dist/8.1.2\n  texlive: texlive/20220321\n  udunits: udunits/2.2.28\n  valgrind4hpc: valgrind4hpc/2.12.11\n  vtune: vtune/2023.0.0, vtune/2023.2.0\n----------------------------------------------------------------------------\nTo learn more about a package execute:\n   $ module spider Foo\nwhere \"Foo\" is the name of a module.\nTo find detailed information about a particular package you\nmust specify the version if there is more than one version:\n   $ module spider Foo/11.1\n----------------------------------------------------------------------------\n</code></pre>"},{"location":"compute-systems/derecho/derecho-modules/","title":"Derecho Modules","text":""},{"location":"compute-systems/derecho/derecho-modules/#derecho-default-module-list","title":"Derecho default module list","text":"<pre><code>-------------------------- Module Stack Environments ---------------------------\n   ncarenv/23.06 (S,L,D)    ncarenv/23.09 (S)\n\n------------------------- Compilers and Core Software --------------------------\n   apptainer/1.1.7            intel-classic/2023.0.0\n   arm-forge/22.1.3           intel-oneapi/2023.0.0\n   atp/3.14.18                intel/2023.0.0         (L)\n   atp/3.15.0          (D)    linaro-forge/23.0\n   cce/15.0.1                 matlab/R2023a\n   cdo/2.1.1                  nccmp/1.9.0.1\n   charliecloud/0.32          ncl/6.6.2\n   cmake/3.26.3               nco/5.1.4\n   conda/latest               ncview/2.1.8\n   cray-ccdb/4.12.13          ncvis/2022.08.28\n   cray-dyninst/12.1.1        nvhpc/21.3\n   cray-mrnet/5.0.4           nvhpc/23.1             (D)\n   cray-stat/4.11.13          nvhpc/23.5\n   craype/2.7.20       (L)    papi/7.0.0.1\n   cuda/11.7.1                pcre/8.45\n   cudnn/8.5.0.96-11.7        peak-memusage/3.0.1\n   cutensor/1.7.0.1           perftools-base/23.03.0\n   darshan-util/3.4.2         perl/5.36.0\n   gcc/12.2.0                 podman/4.3.1\n   gdb4hpc/4.14.7             sanitizers4hpc/1.0.4\n   go/1.20.3                  texlive/20220321\n   grads/2.2.1                valgrind4hpc/2.12.11\n   idl/8.9.0                  vtune/2023.0.0\n\n-------------------- Compiler-dependent Software - [oneapi] --------------------\n   cray-libsci/23.02.1.1        intel-mpi/2021.8.0\n   cray-mpich/8.1.25     (L)    mkl/2023.0.0\n   eccodes/2.25.0               mpi-serial/2.3.0\n   fftw/3.3.10                  ncarcompilers/1.0.0 (L)\n   geos/3.9.1                   netcdf/4.9.2        (L)\n   hdf/4.2.15                   proj/8.2.1\n   hdf5/1.12.2           (L)    udunits/2.2.28\n\n---------------- MPI-dependent Software - [oneapi + cray-mpich] ----------------\n   darshan-runtime/3.4.2        hdf5-mpi/1.12.2           parallelio/2.6.0\n   esmf/8.4.2                   netcdf-mpi/4.9.2          parallelio/2.6.1\n   esmf/8.5.0            (D)    parallel-netcdf/1.12.3    parallelio/2.6.2 (D)\n   fftw-mpi/3.3.10              parallelio/1.10.1\n   gptl/8.1.1                   parallelio/2.5.10\n\n  Where:\n   D:  Default Module\n   L:  Module is loaded\n   S:  Module is Sticky, requires --force to unload or purge\n\nIf the avail list is too long consider trying:\n\n\"module --default avail\" or \"ml -d av\" to just list the default modules.\n\"module overview\" or \"ml ov\" to display the number of modules for each name.\n\nUse \"module spider\" to find all possible modules and extensions.\nUse \"module keyword key1 key2 ...\" to search for all possible modules matching\nany of the \"keys\".\n</code></pre>"},{"location":"compute-systems/derecho/derecho-modules/#derecho-complete-module-listing","title":"Derecho complete module listing","text":"<pre><code>----------------------------------------------------------------------------\nThe following is a list of the modules and extensions currently available:\n----------------------------------------------------------------------------\n  apptainer: apptainer/1.1.7, apptainer/1.1.9\n  arm-forge: arm-forge/22.1.3\n  atp: atp/3.14.18, atp/3.15.0\n  cce: cce/15.0.1\n  cdo: cdo/2.1.1, cdo/2.2.2\n  charliecloud: charliecloud/0.32, charliecloud/0.33\n  cmake: cmake/3.26.3\n  conda: conda/latest\n  cp2k: cp2k/2023.2\n  cray-ccdb: cray-ccdb/4.12.13\n  cray-dyninst: cray-dyninst/12.1.1\n  cray-libsci: cray-libsci/23.02.1.1\n  cray-mpich: cray-mpich/8.1.25\n  cray-mrnet: cray-mrnet/5.0.4\n  cray-stat: cray-stat/4.11.13\n  craype: craype/2.7.20\n  cuda: cuda/11.7.1, cuda/11.8.0, cuda/12.2.1\n  cudnn: cudnn/8.5.0.96-11.7, cudnn/8.7.0.84-11.8\n  cutensor: cutensor/1.7.0.1\n  darshan-runtime: darshan-runtime/3.4.2\n  darshan-util: darshan-util/3.4.2\n  eccodes: eccodes/2.25.0\n  ecflow: ecflow/5.8.3\n  esmf: esmf/8.4.2, esmf/8.5.0\n  fftw: fftw/3.3.10\n  fftw-mpi: fftw-mpi/3.3.10\n  gcc: gcc/12.2.0\n  gdal: gdal/3.6.4, gdal/3.7.1\n  gdb4hpc: gdb4hpc/4.14.7\n  geos: geos/3.9.1\n  go: go/1.20.3, go/1.20.6\n  gptl: gptl/8.1.1\n  grads: grads/2.2.1, grads/2.2.3\n  hdf: hdf/4.2.15\n  hdf5: hdf5/1.12.2\n  hdf5-mpi: hdf5-mpi/1.12.2\n  idl: idl/8.9.0\n  intel: intel/2023.0.0, intel/2023.2.1\n  intel-classic: intel-classic/2023.0.0, intel-classic/2023.2.1\n  intel-mpi: intel-mpi/2021.8.0, intel-mpi/2021.10.0\n  intel-oneapi: intel-oneapi/2023.0.0, intel-oneapi/2023.2.1\n  julia: julia/1.9.2\n  linaro-forge: linaro-forge/23.0\n  matlab: matlab/R2023a\n  mkl: mkl/2023.0.0, mkl/2023.2.0\n  mpi-serial: mpi-serial/2.3.0\n  mpifileutils: mpifileutils/0.11.1\n  mvapich: mvapich/3.0b\n  ncarcompilers: ncarcompilers/1.0.0\n  ncarenv: ncarenv/23.06, ncarenv/23.09\n  nccmp: nccmp/1.9.0.1, nccmp/1.9.1.0\n  ncl: ncl/6.6.2\n  nco: nco/5.1.4, nco/5.1.6\n  ncview: ncview/2.1.8, ncview/2.1.9\n  ncvis: ncvis/2022.08.28\n  netcdf: netcdf/4.9.2\n  netcdf-mpi: netcdf-mpi/4.9.2\n  nvhpc: nvhpc/21.3, nvhpc/23.1, nvhpc/23.5, nvhpc/23.7\n  osu-micro-benchmarks: osu-micro-benchmarks/7.1-1, ...\n  papi: papi/7.0.0.1\n  parallel-netcdf: parallel-netcdf/1.12.3\n  parallelio: parallelio/1.10.1, parallelio/2.5.10, parallelio/2.6.0, ...\n  pcre: pcre/8.45\n  peak-memusage: peak-memusage/3.0.1\n  perftools: perftools\n  perftools-base: perftools-base/23.03.0\n  perftools-lite: perftools-lite\n  perftools-lite-events: perftools-lite-events\n  perftools-lite-gpu: perftools-lite-gpu\n  perftools-lite-hbm: perftools-lite-hbm\n  perftools-lite-loops: perftools-lite-loops\n  perftools-preload: perftools-preload\n  perl: perl/5.36.0, perl/5.38.0\n  podman: podman/4.3.1, podman/4.5.1\n  proj: proj/8.2.1\n  rstudio: rstudio/2023.09.0\n  sanitizers4hpc: sanitizers4hpc/1.0.4\n  superlu: superlu/5.3.0\n  superlu-dist: superlu-dist/8.1.2\n  texlive: texlive/20220321\n  udunits: udunits/2.2.28\n  valgrind4hpc: valgrind4hpc/2.12.11\n  vtune: vtune/2023.0.0, vtune/2023.2.0\n----------------------------------------------------------------------------\nTo learn more about a package execute:\n   $ module spider Foo\nwhere \"Foo\" is the name of a module.\nTo find detailed information about a particular package you\nmust specify the version if there is more than one version:\n   $ module spider Foo/11.1\n----------------------------------------------------------------------------\n</code></pre>"},{"location":"compute-systems/derecho/derecho-use-policies/","title":"System use policies","text":""},{"location":"compute-systems/derecho/derecho-use-policies/#appropriate-use-of-login-nodes","title":"Appropriate use of login nodes","text":"<p>Users may run short, non-memory-intensive processes interactively on the Derecho\u00a0system's login nodes. These include tasks such as text editing or running small serial scripts or programs.</p> <p>However, the login nodes\u00a0may not\u00a0be used to run processes that consume excessive resources. This is to ensure an appropriate balance between user convenience and login node performance.</p> <p>This applies to individual processes that consume excessive amounts of CPU time, more than a few GB of memory, or excessive I/O resources. It also applies collectively to multiple concurrent tasks that an individual user runs.</p> <p>Processes that use excessive resources on the login nodes are terminated automatically. Affected users are informed by email that their sessions were terminated. They are also advised to run such processes in batch or interactive jobs on the Casper cluster.</p>"},{"location":"compute-systems/derecho/derecho-use-policies/#fair-share-policy","title":"Fair share policy","text":"<p>CISL manages scheduling priorities to ensure fair access to the system by all of these stakeholder groups: the university community, the NCAR community, the Community Earth System Model (CESM) community, the\u00a0Antarctic Mesoscale Prediction System (AMPS) community,\u00a0and the Wyoming community.</p> <p>The\u00a0fair-share policy\u00a0takes the community-wide usage balance into account along with several additional factors. These include the submitting users' currently running jobs and recently completed jobs. The scheduling system uses a dynamic-priority formula to weigh these factors, calculate each job's priority, and make scheduling decisions.</p> <p></p>"},{"location":"compute-systems/derecho/derecho-use-policies/#job-scheduling-priorities","title":"Job scheduling priorities","text":"<p>The PBS Pro workload management system scheduling policy for running jobs in the Derecho environment requires balancing several factors. Jobs generally are sorted based on the following:</p> <ol> <li> <p>Job priority (user selectable)</p> </li> <li> <p>Fair share factor</p> </li> <li> <p>Eligible time in queue</p> </li> <li> <p>Job size</p> </li> </ol> <p>Job sorting is adjusted frequently in response to varying demands and workloads. PBS examines the jobs in sorted order in each scheduling cycle and starts those that it can. Jobs that cannot be started immediately are either scheduled to run at a future time or bypassed for the current cycle. Under typical system usage, multiple scheduling cycles are initiated every minute.</p> <p>The scheduler may not start a job for a number of reasons, including:</p> <ul> <li> <p>The necessary resources are not yet available.</p> </li> <li> <p>The system has been reserved for a scheduled outage.</p> </li> <li> <p>The job has been placed on hold.</p> </li> <li> <p>You have reached your concurrent core-usage limit when using the   develop queue.</p> </li> </ul> <p>A high-priority job might be delayed by one of the limits on the list, while a lower-priority job from a different user or a job requesting fewer resources might not be blocked.</p> <p>If your job is waiting in the queue, you can run the <code>qstat</code> command as shown to obtain information that can indicate why it has not started running. (Use this command sparingly.) <pre><code>qstat -s jobID\n</code></pre></p> <p>Note</p> <p>To prevent jobs from languishing in the queues indefinitely, PBS reserves resources for the top-priority jobs and doesn't allow lower-priority jobs to start if they would delay the start time of a higher-priority job.</p>"},{"location":"compute-systems/derecho/derecho-use-policies/#pbs-sorting-factors","title":"PBS sorting factors","text":""},{"location":"compute-systems/derecho/derecho-use-policies/#stakeholder-shares-and-fair-share-factor","title":"Stakeholder shares and fair-share factor","text":"<p>CISL manages scheduling priorities to ensure fair access to the system by these stakeholder groups: the university community, the NCAR community, the CESM community,\u00a0and the Wyoming community.</p> <p>Each stakeholder group is allocated a certain percentage of the available processors. A job cannot start if that action would cause the group to exceed its share, unless another group is using less than its share and has no jobs waiting. In such a case, the high-use group can \"borrow\" processors from the lower-use stakeholder group for a short time.</p> <p>When jobs are sorted, jobs from groups that are using less of their share are picked before jobs from groups using more of their shares. Shares are evaluated based on usage over the past week with usage the prior week being decayed by half.</p>"},{"location":"compute-systems/derecho/derecho-use-policies/#job-priority","title":"Job priority","text":"<p>Users can set job priority to one of three values. Jobs with higher priority are charged against the user's allocation at higher rates than others.</p> Job priority Priority order Priority factor Description premium 1 1.5 Jobs are charged at 150% of the regular rate. regular 2 1 All production jobs default to this priority. economy 3 0.7 Production batch jobs are charged at 70% of regular rate. preempt 4 0.2 Automatically selected when job is submitted to <code>preempt</code> queue."},{"location":"compute-systems/derecho/derecho-use-policies/#job-size","title":"Job size","text":"<p>Jobs asking for more nodes are favored over jobs asking for fewer. The reasoning is that while it is easier for small jobs to fill gaps in the schedule, larger jobs need help collecting enough CPUs or GPUs to start.</p>"},{"location":"compute-systems/derecho/derecho-use-policies/#gpu-usage","title":"GPU usage","text":"<p>In order to submit jobs that will use GPUs, you must be associated with a project that has an allocation of GPU hours. If you submit a job with a project code that does not have an allocation of GPU hours, your job will be rejected.</p>"},{"location":"compute-systems/derecho/derecho-use-policies/#backfilling","title":"Backfilling","text":"<p>When a job cannot start immediately, PBS sets aside resources for it before examining other jobs to see if any of them can run as backfill. That is, PBS looks at running jobs to determine when they will finish based on wall-time requested. From those finish times, PBS decides when enough resources (such as CPUs, memory, and job limits) will become available to run the top job. PBS then reserves the resources that the job requests at that identified time.</p> <p>When PBS looks at other jobs to see if they can start immediately, it also checks whether starting any of them would collide with one of these resource reservations. Only if there are no collisions will PBS start the lower-priority jobs.</p>"},{"location":"compute-systems/derecho/derecho-use-policies/#preemption","title":"Preemption","text":"<p>Derecho has a preemption routing queue that can be used to submit jobs that will run when the required resources are not in use for higher-priority work in the main or develop execution queues. In order to take advantage of preemption, submit your job to the preempt routing queue and it job will run when the necessary resources are available.</p> <p>When resources for any preempt jobs are needed by higher-priority work, the scheduler sends a <code>SIGTERM</code> signal that can be detected by your job. After the <code>SIGTERM</code> signal is sent to the job, there is a five-minute window in which the job has a chance to checkpoint or save any work that was accomplished. After the five-minute window, the job will be killed by the scheduler and deleted. See this page for additional preemption details.</p>"},{"location":"compute-systems/derecho/moving-from-cheyenne/","title":"Moving to Derecho from Cheyenne","text":"<p>This page is intended to provide a high-level comparison of Derecho to Cheyenne, particularly for users comfortable with Cheyenne operations and looking for a reference guide for transitioning workflows.</p>"},{"location":"compute-systems/derecho/moving-from-cheyenne/#processes-procedures-for-derecho-vs-cheyenne","title":"Processes &amp; Procedures for Derecho vs. Cheyenne","text":"<ul> <li> <p>Allocations: Users must have a project allocation to use Derecho.  Derecho CPU and GPU resources are allocated as separate entities.</p> </li> <li> <p>Logging in: <code>ssh</code> is used similarly to access both systems.     Please note for Derecho the host name is derecho.hpc.ucar.edu</p> <p><code>ssh</code> access to Derecho vs. Cheyenne</p> DerechoCheyenne <p>The fully-qualified domain name (FQDN) for Derecho is <code>derecho.hpc.ucar.edu</code>.  <pre><code>ssh username@derecho.hpc.ucar.edu\n</code></pre></p> <p>The fully-qualified domain name (FQDN) for Cheyenne is <code>cheyenne.ucar.edu</code>.  <pre><code>ssh username@cheyenne.ucar.edu\n</code></pre></p> </li> <li> <p>Home and Work file spaces: Derecho and Cheyenne share the work spaces <code>/glade/u/home/${USER}</code> and <code>/glade/work/${USER}</code>.  No action is necessary to migrate or synchronize these file spaces.</p> </li> <li> <p>Scratch file space:  Users are allocated 30TB data / 12 Million files of scratch quota at <code>/glade/derecho/scratch/${USER}</code>.</p> <ul> <li>Moving Files from Cheyenne: Derecho and Cheyenne have distinct scratch files systems. Users may access their Cheyenne scratch contents on Derecho from <code>/glade/cheyenne/scratch/${USER}</code>. No attempt is made to automatically transfer contents between the systems, as scratch storage is intended to be temporary and tied to specific job workflows.  (Users interested in duplicating contents between systems on their own may follow instructions here.)</li> </ul> </li> <li> <p>Project file space:  The Campaign Storage file system is mounted across all of Derecho and Cheyenne, and should be used for shared project storage.  The Cheyenne project spaces <code>/glade/p/</code> and <code>/glade/collections/</code> are deprecated and are being migrated into <code>/glade/campaign/</code> as part of the Cheyenne decommissioning process.  NCAR Labs are responsible for moving their contents from these deprecated spaces, while migration for active University projects will be performed automatically.</p> </li> <li> <p>Compiling: Derecho maintains the <code>intel</code> compilers as default. Additional compilers are available through the module system, occasionally with different naming conventions.</p> <p>In general the compilers on Derecho are much newer than their counterparts on Cheyenne, for example the default version of the GNU Compiler Collection on Derecho is accessible via the <code>gcc/12.2.0</code> module, whereas Cheyenne's default is <code>gnu/10.1.0</code>.</p> <p>Check CPU architecture flags!!</p> <p>When recompiling users should pay special attention to any architecture-specific CPU optimization flags. Derecho uses AMD Milan processors, which perform well with code compiled with <code>-march=core-avx2</code> under Intel and GCC.</p> <p>DO use on Derecho: <code>-march=core-avx2</code></p> <p>Be especially careful not to use some other flags commonly used on Cheyenne, as these will create sub-optimal code that will run slower than otherwise possible, or may fail to execute altogether.</p> <p>Do NOT use on Derecho: <code>-xHost</code> , <code>-axHost</code> , <code>-xCORE-AVX2</code> , <code>-axCORE-AVX2</code></p> What's the difference between the <code>intel</code>, <code>intel-oneapi</code>, <code>intel-classic</code> modules? <p>Users migrating from Cheyenne and previous Casper deployments may note there are several \"flavors\" of the Intel compiler available through the module system.</p> <p>Intel is currently moving from their \"classic\" compiler suite to the new \"OneAPI\" family.  During this process both sets of compilers are available, but through different commands under different <code>module</code> selections:</p> Module Fortran C C++ <code>intel-classic</code> <code>ifort</code> <code>icc</code> <code>icpc</code> <code>intel-oneapi</code> <code>ifx</code> <code>icx</code> <code>icpx</code> <code>intel</code>(default) <code>ifort</code> <code>icx</code> <code>icpx</code> <p>The <code>intel-classic</code> module makes the familiar <code>ifort/icc/icpc</code> compilers available, however it is expected these will be deprecated during Casper's lifetime.  At this stage we expect to keep existing compiler versions available, however there will be no further updates.</p> <p>The <code>intel-oneapi</code> module uses the new <code>ifx/icx/icpx</code> compilers.</p> <p>The default <code>intel</code> module presently uses the older <code>ifort</code> Fortran compiler along with the newer <code>icx/icpx</code> C/C++ compilers. This choice is intentional as the newer <code>ifx</code> does not reliably match the performance of <code>ifort</code> in all cases.  We will continue to monitor the progress of the OneAPI compilers and will change this behavior in the future.</p> <p>Users migrating particularly old code bases from Cheyenne may wish to try the <code>intel-classic</code> compilers if <code>intel</code> causes excessive warnings that are not easily resolved.  The <code>intel-classic</code> compilers are less stringent with certain standards compliance checks. Note however this is not a permanent solution, as Intel will deprecate the Classic series at some point in Derecho's lifespan.</p> <ul> <li> <p><code>ncarcompilers</code> and linking with libraries: We continue to recommend the use of the <code>ncarcompilers</code> module to facilitate compilation, particularly when using 3<sup>rd</sup>-party libraries through the <code>module</code> system.  One notable change on Derecho is that <code>ncarcompilers</code> no longer assumes which combination of <code>-l&lt;package_libraries&gt;</code> the user desires, necessitating manual selection.  For example, to link a simple Fortran application with NetCDF, the process is slightly changed on Derecho vs. Cheyenne:</p> <p><code>ncarcompilers</code> on Derecho requires specifying particular package libraries</p> <p>The <code>ncarcompilers</code> module greatly facilitates linking to 3<sup>rd</sup> party libraries by injecting <code>module</code>-specific linker library search paths into executables.  This results in executables that can find their dependencies without relying on environment variables at execution time.</p> DerechoCheyenne <p>Compiling a simple Fortran program that uses NetCDF requires specifying which <code>-l&lt;package_libs&gt;</code> are required:  <pre><code>module load netcdf\nifort -o foo.exe foo.f90 -lnetcdf\n</code></pre>  This change allows for broader support of packages that introduce multiple libraries, resolving ambiguity in the users' intent.</p> <p>Compiling a simple Fortran program that uses NetCDF on Cheyenne assumes which <code>-l&lt;package_libs&gt;</code> are required, and this detail is hidden from the user:</p> <pre><code>module load netcdf\nifort -o foo.exe foo.f90\n</code></pre> </li> </ul> </li> <li> <p>PBS Job Submission:  PBS is use to launch jobs on Derecho similar to Cheyenne.  Job submission and monitoring via <code>qsub</code>, <code>qstat</code>, <code>qdel</code> etc... are similar between the systems.</p> <ul> <li> <p>Queues: On Derecho the default PBS queue <code>main</code> takes the place of the three queues <code>regular</code>, <code>premium</code>, and <code>economy</code> on Cheyenne.</p> </li> <li> <p>Job Priority: On Derecho users request a specific job priority via the <code>#PBS -l job_priority=&lt;regular|premium|economy&gt;</code> resource directive, instead of through distinct queues.</p> </li> <li> <p><code>select</code> statements for CPU jobs: Derecho CPU nodes have 128 cores.  A typical PBS resource selection statement is <code>#PBS -l select=10:ncpus=128:mpiprocs=128:ompthreads=1</code>.</p> <p>Update your <code>select</code> statements!</p> <p>Derecho CPU nodes have 128 cores, and are generally assigned exclusively.  This means you will be charged for all 128 cores on the node, regardless of how many you use.  Do not simply copy your old <code>select</code> statements from Cheyenne - doing so will under-utilize the CPU nodes, and you will be charged for the full resource regardless of your usage!</p> </li> <li> <p>Memory: Each CPU node has a maximum of 235GB of RAM available for user jobs.  Derecho CPU nodes are all identical - there is no notion of <code>largemem</code> or <code>smallmem</code> CPU nodes.  Users requiring more memory per core than the 235GB/128 default configuration allows will need to under-subscribe CPU nodes, that is, leave some cores idle in order to increase the effective memory-per-utilized-core.</p> </li> </ul> </li> <li> <p>MPI Environment: Derecho and Cheyenne differ significantly in their default MPI configurations.  Derecho uses <code>cray-mpich</code> by default, vs. Cheyenne's Message Passing Toolkit (MPT) implementation.</p> <ul> <li> <p>Launching jobs with <code>mpiexec</code>: Derecho uses the <code>mpiexec</code> launcher specified by the MPI standard.  (There is no <code>mpiexec_mpt</code> on Derecho.) Additional command-line arguments are required to specify processor layout for <code>mpiexec</code>:   <pre><code>mpiexec -n &lt;# Total Ranks&gt; -ppn &lt;# Ranks Per Node &gt; ./executable\n</code></pre>   see <code>man mpiexec</code> on Derecho for additional details.</p> </li> <li> <p>MPI Environment Variables: Any users leveraging MPT-specific environment variables in their job scripts to change default behavior should first test their application with default configurations to determine if such approaches are still necessary, and if so will need to find <code>cray-mpich</code> equivalents - see <code>man intro_mpi</code> on Derecho or reach out to consulting for assistance.</p> </li> <li> <p>Process binding: Derecho does not use the <code>dmplace</code> or <code>omplace</code> utilities found on Cheyenne for process binding, requiring instead binding selections to be specified through <code>mpiexec</code>.  For additional details and examples see Derecho PBS Script Examples and the discussion of the <code>--cpu-bind</code> option in the <code>mpiexec</code> manual page (<code>man mpiexec</code> on Derecho).</p> </li> </ul> </li> <li> <p>cron automation: Some users leverage <code>cron</code> on Cheyenne to automate workflows. NCAR/CISL has deployed a new <code>cron</code> service independent of the HPC systems. This separated, high-availability solution allows us to perform maintenance on the HPC resources while not interrupting <code>cron</code> workflows that can tolerate the downtime. Additional details are here.</p> </li> </ul>"},{"location":"compute-systems/derecho/moving-from-cheyenne/#going-further","title":"Going Further","text":"<p>Much more information on Derecho hardware, software, and general user environment can be found in the Introduction to the Derecho Supercomputer training slides.</p>"},{"location":"compute-systems/derecho/moving-from-cheyenne/#comparison-references","title":"Comparison References","text":""},{"location":"compute-systems/derecho/moving-from-cheyenne/#user-environment-comparison","title":"User Environment Comparison","text":"<p>Click on the image below for a detailed comparison of Cheyenne and Derecho key user environment differences. </p>"},{"location":"compute-systems/derecho/moving-from-cheyenne/#hardware-comparison","title":"Hardware Comparison","text":"<p>Click on the image below for a detailed comparison of Cheyenne and Derecho hardware. </p>"},{"location":"compute-systems/derecho/compiling-code-on-derecho/","title":"Compiling code on Derecho","text":""},{"location":"compute-systems/derecho/compiling-code-on-derecho/#compilers-available-on-derecho","title":"Compilers available on Derecho","text":"<p>Several\u00a0C/C++ and Fortran\u00a0compilers are available on all NCAR HPC systems. The information on this page applies to all of those systems except where noted.</p> Compiler Language Commands for serial programs Commands for programs           using MPI Flags to enable OpenMP Intel (Classic/OneAPI)* Fortran <pre>ifort / ifx</pre> <pre>mpif90</pre> <pre>-qopenmp</pre> C <pre>icc / icx</pre> <pre>mpicc</pre> C++ <pre>icpc / icpx</pre> <pre>mpicxx</pre> NVIDIA HPC SDK Fortran <pre>nvfortran</pre> <pre>mpif90</pre> <pre>-mp</pre> C <pre>nvc</pre> <pre>mpicc</pre> C++ <pre>nvc++</pre> <pre>mpicxx</pre> GNU Compiler Collection (GCC) Fortran <pre>gfortran</pre> <pre>mpif90</pre> <pre>-fopenmp</pre> C <pre>gcc</pre> <pre>mpicc</pre> C++ <pre>g++</pre> <pre>mpicxx</pre> Cray Compiler (Derecho only)** Fortran <pre>ftn</pre> <pre>mpif90</pre> <pre>-fopenmp</pre> C <pre>cc</pre> <pre>mpicc</pre> C++ <pre>CC</pre> <pre>mpicxx</pre> * Intel OneAPI is a cross-platform toolkit that           supports C, C++, Fortran, and Python programming languages           and replaces Intel             Parallel Studio. Derecho supports both Intel OneAPI and Intel           Classic Compilers. Intel is planning to retire the Intel Classic           compilers and is moving toward Intel OneAPI. Intel Classic Compiler           commands (ifort, icc, and icpc) will be replaced by the Intel OneAPI           compilers (ifx, icx, and icpx). ** Please note that mpi wrappers are not           available by default using Cray compilers but the ncarcompilers module           will translate a call to \u201cmpicc\u201d to \u201ccc\u201d (and likewise for the other           languages) as a convenience."},{"location":"compute-systems/derecho/compiling-code-on-derecho/#compiler-commands","title":"Compiler Commands","text":"<p>All supported compilers are available via the <code>module</code> utility. After loading the compiler module you want to use, refer to the table above to identify and run the appropriate compilation wrapper command.</p> <p>If your script already includes one of the following generic MPI commands, there is no need to change it:</p> <ul> <li> <p><code>mpif90</code>, <code>mpif77</code>, <code>ftn</code>*</p> </li> <li> <p><code>mpicc</code>, <code>cc</code>*</p> </li> <li> <p><code>mpiCC</code>, <code>CC</code>*</p> </li> </ul> <p>*The wrappers <code>ftn</code>, <code>cc</code>, and <code>CC</code> are Cray-specific only available on Derecho.</p> <p>Build any libraries that you need to support an application with the same compiler, compiler version, and compatible flags used to compile the other parts of the application, including the main executable(s). Also, before you run the applications, be sure you have loaded the same module/version environment in which you created the applications. This will help you avoid job failures that can result from missing MPI launchers and library routines.</p>"},{"location":"compute-systems/derecho/compiling-code-on-derecho/#compiler-man-pages","title":"Compiler <code>man</code> pages","text":"<p>To refer to the <code>man</code> page for a compiler, log in to the system where you intend to use it, load the module, then execute <code>man</code> for the compiler. For example: <pre><code>module load nvhpc\nman nvfortran\n</code></pre></p> <p>You can also use <code>-help</code> flags for a description of the command-line options for each compiler. Follow this example: <pre><code>ifort -help\n\nnvfortran -help [=option]\n</code></pre></p> <p>Tip</p> <p>Use compiler diagnostic flags to identify potential problems while compiling the code.</p>"},{"location":"compute-systems/derecho/compiling-code-on-derecho/#changing-compilers","title":"Changing compilers","text":"<p>To change from one compiler to another, use <code>module swap</code>. In this example, you are switching from Intel to NVIDIA: <pre><code>module swap intel nvhpc\n</code></pre></p> <p>When you load a compiler module or change to a different compiler, the system makes other compatible modules available. This helps you establish a working environment and avoid conflicts.</p> <p>If you need to link your program with a library, use <code>module load</code> to load the library as in this example: <pre><code>module load netcdf\n</code></pre></p> <p>Then you can invoke the desired compilation command, including any library linking options such as <code>-lnetcdf</code>. Here's an example: <pre><code>mpif90-o foo.exe foo.f90 -lnetcdf\n</code></pre></p>"},{"location":"compute-systems/derecho/compiling-code-on-derecho/#compiling-cpu-code","title":"Compiling CPU code","text":""},{"location":"compute-systems/derecho/compiling-code-on-derecho/#using-the-cray-compiler-collection","title":"Using the Cray compiler collection","text":"<p>Derecho users have access to the Cray Compiling Environment (CCE) using the <code>cce</code> module. The compiler collection includes <code>cc</code>, <code>CC</code>, and <code>ftn</code> for compiling C, C++, and Fortran codes.</p> <p>To see which versions of the compiler are available, use the <code>module avail</code> command: <pre><code>module avail cce\n</code></pre></p> <p>CCE base compilers are available by default in the <code>ncarcompilers</code> module. Loading the <code>ncarcompilers</code> module simplifies building code with dependencies such as netCDF. For example, compiling a simple Fortran code using netCDF is as follows with the compiler wrappers: <pre><code>ftn -o mybin -lnetcdff mycode.f90\n</code></pre></p> <p>Meanwhile, if you did not have the <code>ncarcompilers</code> module loaded, you would need to run the following command instead, with the linker flags and include-paths: <pre><code>ftn -I/path/to/netcdf/include -L/path/to/netcdf/lib -lnetcdff -o mybin mycode.f90\n</code></pre></p>"},{"location":"compute-systems/derecho/compiling-code-on-derecho/#using-cray-mpich-mpi","title":"Using Cray MPICH MPI","text":"<p>Unlike other MPI libraries, Cray MPICH does NOT provide MPI wrapper commands like <code>mpicc</code>, <code>mpicxx</code>, and <code>mpif90</code>. Rather, use the same <code>cc</code>, <code>CC</code>, and <code>ftn</code> commands you use to compile a serial code. The Cray Programming Environment (CPE) will add MPI build flags to your commands whenever you have the <code>cray-mpich</code> module loaded.</p> <p>As many application build systems expect the MPI wrappers, our <code>ncarcompilers</code> module will translate a call to <code>mpicc</code> to <code>cc</code> (and likewise for the other languages) as a convenience, typically eliminating the need to alter pre-existing build scripts.</p> <p>Cray MPICH also supports GPU devices. If you are using an MPI application compiled with GPU support, enable CUDA functionality by loading a cuda module and setting or exporting this environment variable before calling the MPI launcher in your job by including this in your script: <pre><code>MPICH_GPU_SUPPORT_ENABLED=1\n</code></pre></p> <p>Also, if your GPU-enabled MPI application makes use of managed memory, you also need to set this environment variable: <pre><code>MPICH_GPU_MANAGED_MEMORY_SUPPORT_ENABLED=1\n</code></pre></p> <p>At runtime, you will also need to pass information about job parallelism to the <code>mpiexec</code> (or <code>mpirun</code> / <code>aprun</code>) launcher because this information is not automatically taken from the PBS job script. You can pass this information by setting environment variables or by using <code>mpiexec</code> options. Full details of runtime settings for launching parallel programs can be found by running <code>man mpiexec</code>.</p> <p>The primary settings you will need are:</p> <ul> <li> <p>the number of mpi ranks (<code>-n / PALS_NRANKS</code>)</p> </li> <li> <p>the number of ranks per node (<code>-ppn / PALS_PPN</code>)</p> </li> <li> <p>the number of OpenMP threads or CPUs to associate with each rank (<code>-d /   PALS_DEPTH</code>)</p> </li> <li> <p>binding options (<code>--cpu-bind / PALS_CPU_BIND</code>)</p> </li> </ul> <p>Tip</p> <p>Cray MPICH has many tunable parameters you can set through environment variables. Run <code>man mpi</code> for a complete listing of these environment variables.</p> <p>Example PBS select statements and corresponding MPI launch options are shown below for binding a hybrid MPI + OpenMP application (144 MPI ranks, and 4 OpenMP threads per MPI rank, which requires 5 nodes but does not fully subscribe the last node). Examples of both methods \u2013 setting environment variables and passing options to <code>mpiexec</code> \u2013 are provided.</p>"},{"location":"compute-systems/derecho/compiling-code-on-derecho/#environment-variable-example","title":"Environment variable example","text":"bashtcsh <pre><code>#PBS -l select=5:ncpus=128:mpiprocs=32:ompthreads=4\n\nexport PALS_NRANKS=144\nexport PALS_PPN=32\nexport PALS_DEPTH=4\nexport PALS_CPU_BIND=depth\n\nmpiexec ./a.out\n</code></pre> <pre><code>#PBS -l select=5:ncpus=128:mpiprocs=32:ompthreads=4\n\nsetenv PALS_NRANKS 144\nsetenv PALS_PPN 32\nsetenv PALS_DEPTH 4\nsetenv PALS_CPU_BIND depth\n\nmpiexec ./a.out\n</code></pre>"},{"location":"compute-systems/derecho/compiling-code-on-derecho/#mpiexec-options-example","title":"<code>mpiexec</code> options example","text":"bashtcsh <pre><code>#PBS -l select=5:ncpus=128:mpiprocs=32:ompthreads=4\n\nmpiexec --cpu-bind depth -n 144 -ppn 32 -d 4 ./a.out\n</code></pre> <pre><code>#PBS -l select=5:ncpus=128:mpiprocs=32:ompthreads=4\n\nmpiexec --cpu-bind depth -n 144 -ppn 32 -d 4 ./a.out\n</code></pre>"},{"location":"compute-systems/derecho/compiling-code-on-derecho/#other-compilers","title":"Other compilers","text":"<p>These additional compilers are available on Derecho.</p> <ul> <li> <p>NVIDIA\u2019s HPC SDK</p> </li> <li> <p>Intel compilers</p> </li> <li> <p>the GNU Compiler Collection (GCC)</p> </li> </ul> <p>When using non-Cray compilers, you can use either the compiler collection\u2019s own commands (e.g., <code>ifort</code>, <code>nvfortran</code>) or the equivalent CPE command (e.g., <code>ftn</code>) as long as you have loaded your desired compiler module. If you do not have the <code>ncarcompilers</code> module loaded and you are using the <code>cray-mpich</code> MPI, you will need to use a CPE command.</p>"},{"location":"compute-systems/derecho/compiling-code-on-derecho/#using-intel-compilers","title":"Using Intel compilers","text":"<p>The Intel compiler suite is available via the <code>intel</code> module. It includes compilers for C, C++, and Fortran codes. by default.</p> <p>To see which versions are available, use the <code>module avail</code> command. <pre><code>module avail intel\n</code></pre></p> <p>To load the default Intel compiler, use <code>module load</code> without specifying a version. <pre><code>module load intel\n</code></pre></p> <p>To load a different version, specify the version number when loading the module.</p> <p>Similarly, you can swap your current compiler module to Intel by using the <code>module swap</code> command. <pre><code>module swap cce/14.0.3 intel\n</code></pre></p> <p>Extensive documentation for using the Intel compilers is available online here. To review the manual page for a compiler, run the <code>man</code> command for it as in this example: <pre><code>man ifort\n</code></pre></p> <p>What's the difference between the <code>intel</code>, <code>intel-oneapi</code>, <code>intel-classic</code> modules?</p> <p>Users migrating from Cheyenne and previous Casper deployments may note there are several \"flavors\" of the Intel compiler available through the module system.</p> <p>Intel is currently moving from their \"classic\" compiler suite to the new \"OneAPI\" family.  During this process both sets of compilers are available, but through different commands under different <code>module</code> selections:</p> Module Fortran C C++ <code>intel-classic</code> <code>ifort</code> <code>icc</code> <code>icpc</code> <code>intel-oneapi</code> <code>ifx</code> <code>icx</code> <code>icpx</code> <code>intel</code>(default) <code>ifort</code> <code>icx</code> <code>icpx</code> <p>The <code>intel-classic</code> module makes the familiar <code>ifort/icc/icpc</code> compilers available, however it is expected these will be deprecated during Casper's lifetime.  At this stage we expect to keep existing compiler versions available, however there will be no further updates.</p> <p>The <code>intel-oneapi</code> module uses the new <code>ifx/icx/icpx</code> compilers.</p> <p>The default <code>intel</code> module presently uses the older <code>ifort</code> Fortran compiler along with the newer <code>icx/icpx</code> C/C++ compilers. This choice is intentional as the newer <code>ifx</code> does not reliably match the performance of <code>ifort</code> in all cases.  We will continue to monitor the progress of the OneAPI compilers and will change this behavior in the future.</p>"},{"location":"compute-systems/derecho/compiling-code-on-derecho/#optimizing-your-code-with-intel-compilers","title":"Optimizing your code with Intel compilers","text":"<p>Intel compilers provide several different optimization and vectorization options. By default, they use the <code>-O2</code> option, which includes some optimizations.</p> <p>Using <code>-O3</code> instead will provide more aggressive optimizations that may not improve the performance of some programs, while <code>-O1</code> enables minimal optimization. A higher level of optimization might increase your compile time significantly.</p> <p>You can also disable any optimization by using <code>-O0</code>.</p> <p>Be aware that compiling CPU code with the Intel compiler on Derecho is significantly different from using the Intel compiler on the Cheyenne system. Flags that are commonly used on Cheyenne might cause Derecho jobs to fail or run much more slowly than otherwise possible.</p> <p>CPU Architecture Flags to use  on Derecho</p> <p>DO use on Derecho: <code>-march=core-avx2</code></p> <p>Do NOT use on Derecho:</p> <p>Do NOT use on Derecho: <code>-xHost</code> , <code>-axHost</code> , <code>-xCORE-AVX2</code> , <code>-axCORE-AVX2</code></p> <p>These flags will generate code that may not run, or will run suboptimally on Derecho.</p>"},{"location":"compute-systems/derecho/compiling-code-on-derecho/#examples","title":"Examples","text":"<p>To compile and link a single Fortran program and create an executable, follow this example: <pre><code>ifort filename.f90 -o filename.exe\n</code></pre></p> <p>To enable multi-threaded parallelization (OpenMP), include the <code>-qopenmp</code> flag as shown here: <pre><code>ifort -qopenmp filename.f90 -o filename.exe\n</code></pre></p>"},{"location":"compute-systems/derecho/compiling-code-on-derecho/#compiling-gpu-code","title":"Compiling GPU code","text":"<p>On Derecho, GPU applications should be built with either the Cray compilers or the NVIDIA HPC SDK compilers and libraries. In the following examples, we demonstrate the use of NVIDIA\u2019s tools.</p> <p>Additional compilation flags for GPU code will depend in large part on which GPU-programming paradigm is being used (e.g., OpenACC, OpenMP, CUDA) and which compiler collection you have loaded. The following examples show basic usage, but note that many customizations and optimizations are possible. You are encouraged to read the relevant man page for the compiler you choose.</p>"},{"location":"compute-systems/derecho/compiling-code-on-derecho/#openacc","title":"OpenACC","text":"<p>To compile with OpenACC directives, simply add the <code>-acc</code> flag to your invocation of nvc, nvc++, or nvfortan. A Fortran example: <pre><code>nvfortran -o acc_bin -acc acc_code.f90\n</code></pre></p> <p>You can gather more insight into GPU acceleration decisions made by the compiler by adding <code>-Minfo=accel</code> to your invocation. Using compiler options, you can also specify which GPU architecture to target. This example will request compilation for both V100 (as on Casper) and A100 GPUs (as on Derecho): <pre><code>nvfortran -o acc_bin -acc -gpu=cc70,cc80 acc_code.f90\n</code></pre></p> <p>Specifying multiple acceleration targets will increase the size of the binary and the time it takes to compile the code.</p>"},{"location":"compute-systems/derecho/compiling-code-on-derecho/#openmp","title":"OpenMP","text":"<p>Using OpenMP to offload code to the GPU is similar to using OpenACC. To compile a code with OpenMP offloading, use the <code>-mp=gpu</code> flag. The aforementioned diagnostic and target flags also apply to OpenMP offloading. <pre><code>nvfortran -o omp_gpu -mp=gpu omp.f90\n</code></pre></p>"},{"location":"compute-systems/derecho/compiling-code-on-derecho/#cuda","title":"CUDA","text":"<p>The process for compiling CUDA code depends on whether you are using C++ or Fortran. For C++, the process often involves multiple stages in which you first use <code>nvcc``, the NVIDIA CUDA compiler, and then your C++ compiler of choice. <pre><code>nvcc -c -arch=sm_80 cuda_code.cu\ng++ -o cuda_bin -lcuda -lcudart main.cpp cuda_code.o\n</code></pre> Using the</code>nvcc` compiler driver with a non-NVIDIA C++ compiler requires loading a cuda environment module in addition to the compiler of choice.</p> <p>The compiler handles CUDA code directly, so the compiler you use must support CUDA. This means you should use <code>nvfortran</code>. If your source code file ends with the <code>.cuf</code> extension, nvfortran will enable CUDA automatically. Otherwise, you can specify the <code>-Mcuda</code> flag to the compiler. <pre><code>nvfortran -Mcuda -o cf_bin cf_code.f90\n</code></pre></p>"},{"location":"compute-systems/derecho/compiling-code-on-derecho/#native-compiler-commands","title":"Native Compiler Commands","text":"<p>We recommend using the module wrapper commands described above. However, if you prefer to invoke the compilers directly without the <code>ncarcompilers</code> wrappers, see this note:</p> Native Compiler Commands <p>We recommend using the module wrapper commands described above. However, if you prefer to invoke the compilers directly, unload the NCAR default compiler wrapper environment by entering this on your command line: <pre><code>module unload ncarcompilers\n</code></pre></p> <p>You can still use the environment variables that are set by the modules that remain loaded, as shown in the following examples of invoking compilers directly to compile a Fortran program.</p> Intel compilerNVIDIA HPC compilerGNU compiler collection (GCC)Cray Compilers (CPE) <pre><code>ifort -o a.out $NCAR_INC_&lt;PACKAGE&gt; program_name.f $NCAR_LDFLAGS_&lt;PACKAGE&gt; -l&lt;package_library&gt;\n</code></pre> <pre><code>nvfortran -o a.out $NCAR_INC_&lt;PACKAGE&gt; program_name.f $NCAR_LDFLAGS_&lt;PACKAGE&gt; -l&lt;package_library&gt;\n</code></pre> <pre><code>gfortran -o a.out $NCAR_INC_&lt;PACKAGE&gt; program_name.f $NCAR_LDFLAGS_&lt;PACKAGE&gt; -l&lt;package_library&gt;\n</code></pre> <pre><code>ftn -o a.out $NCAR_INC_&lt;PACKAGE&gt; program_name.f $NCAR_LDFLAGS_&lt;PACKAGE&gt; -l&lt;package_library&gt;\n</code></pre>"},{"location":"compute-systems/derecho/compiling-code-on-derecho/#multiple-compiler-versions-and-user-applications","title":"Multiple Compiler Versions and User Applications","text":"<p>In addition to multiple compilers, CISL keeps available multiple versions of libraries to accommodate a wide range of users' needs. Rather than rely on the environment variable <code>LD_LIBRARY_PATH</code> to find the correct libraries dynamically, we encode library paths within the binaries when you build Executable and Linkable Format (ELF) executables. To do this, we use <code>RPATH</code> rather than <code>LD_LIBRARY_PATH</code> to set the necessary paths to shared libraries.</p> <p>This enables your executable to work regardless of updates to new default versions of the various libraries; it doesn't have to search dynamically at run time to load them. It also means you don't need to worry about setting the variable or loading another module, greatly reducing the likelihood of runtime errors.</p>"},{"location":"compute-systems/derecho/compiling-code-on-derecho/#common-compiler-options-and-diagnostic-flags","title":"Common Compiler Options and Diagnostic Flags","text":"<p>Portability and correctness both are important goals when developing code. Non-standard code may not be portable, and its execution may be unpredictable.</p> <p>Using diagnostic options when you compile your code can help you find potential problems. Since the compiler is going to analyze your code anyway, it pays to take advantage of the diagnostic options to learn as much as you can from the analysis. Please note that some compilers disable the default optimization when you switch on certain debugging flags.</p> <p>Because of differences in compilers, it also is good practice to compile your code with each compiler that is available on the system, note any diagnostic messages you get, and revise your code accordingly.</p> <p>The following options can be helpful as you compile code to run in the HPC environment that CISL manages.</p> Compiler Flag Effect CrayCray C/C++ debug options <code>-G0</code> provide complete debugging information with optimizations disabled (i.e.<code>-O0</code>,<code>-O ipa0</code>,<code>-O scale0</code>,<code>-O vector0</code>).Breakpoints can be set at different sections of the code for easier debugging. <code>-G01</code> generate debugging report with partial optimization. <code>-G02</code> generate debugging report with full optimization. <code>-g</code> generate debugging report (equivalent to<code>-G0</code>). <code>-h bounds</code> Enables checking of array bounds, pointer and array references at runtime. IntelIntel C++ diagnostic options <code>-debug all</code> provides complete debugging information. <code>-g</code> places symbolic debugging information in the executable program. <code>-check all</code> performs all runtime checks (includes bounds checking). <code>-warn all</code> enables all warnings. <code>-stand f08</code> warns of usage that does not conform to the Fortran 2008 standard. <code>-traceback</code> enables stack trace if the program crashes. GCCGCC diagnostic warning ptions <code>-ggdb</code> places symbolic debugging information in the executable program for use by GDB. <code>-fcheck=all</code> performs all runtime checks (includes bounds checking). <code>-Wall</code> enables all warnings. <code>-std=f2008</code> warns of usage that does not conform to the Fortran 2008 standard. NVIDIA HPC SDKNVIDIA HPC SDK documentation. <code>-g</code> Include symbolic debugging information in the object modules with optimization disabled (<code>-O0</code>). <code>-gopt</code> Include symbolic debugging information in the object modules without affecting any optimizations. <code>-C</code> or <code>-Mbounds</code> Add array bounds checking. <code>-Mchkptr</code> Check for unintended de-referencing of NULL pointers. <code>-Minform=inform</code> Display all the error messages of any severity (inform, warn, severe and fatal) during compilation phase."},{"location":"compute-systems/derecho/compiling-code-on-derecho/compiler-diagnostic-flags/","title":"Compiler diagnostic flags","text":"<p>Portability and correctness both are important goals when developing code. Non-standard code may not be portable, and its execution may be unpredictable.</p> <p>Using diagnostic options when you compile your code can help you find potential problems. Since the compiler is going to analyze your code anyway, it pays to take advantage of the diagnostic options to learn as much as you can from the analysis. Please note that some compilers disable the default optimization when you switch on certain debugging flags.</p> <p>Because of differences in compilers, it also is good practice to compile your code with each compiler that is available on the system, note any diagnostic messages you get, and revise your code accordingly.</p> <p>The following options can be helpful as you compile code to run in the HPC environment that CISL manages.</p>"},{"location":"compute-systems/derecho/compiling-code-on-derecho/compiler-diagnostic-flags/#cray","title":"Cray","text":"<p>The following compiler flags may be helpful for debugging your code:</p> <ul> <li> <p><code>-G0</code> \u2013\u00a0provide complete debugging information with optimizations   disabled (i.e.\u00a0<code>-O0</code>,\u00a0<code>-O ipa0</code>,\u00a0<code>-O scale0</code>,\u00a0<code>-O vector0</code>).   Breakpoints can be set at different sections of the code for easier   debugging.</p> </li> <li> <p><code>-G01</code> \u2013\u00a0generate debugging report with partial optimization.</p> </li> <li> <p><code>-G02</code> \u2013\u00a0generate debugging report with full optimization.</p> </li> <li> <p><code>-g</code>  \u2013 generate debugging report (equivalent to\u00a0<code>-G0</code>).</p> </li> <li> <p><code>-h bounds</code>\u00a0- Enables checking of array bounds, pointer and array references at runtime.</p> </li> </ul> <p>Also see\u00a0Cray C/C++ debug options.</p>"},{"location":"compute-systems/derecho/compiling-code-on-derecho/compiler-diagnostic-flags/#intel","title":"Intel","text":"<ul> <li> <p><code>-debug all</code>\u00a0\u2013 provides complete debugging information.</p> </li> <li> <p><code>-g</code>\u00a0\u2013 places symbolic debugging information in the executable   program.</p> </li> <li> <p><code>-check all</code>\u00a0\u2013 performs all runtime checks (includes bounds checking).</p> </li> <li> <p><code>-warn all</code>\u00a0\u2013 enables all warnings.</p> </li> <li> <p><code>-stand f08</code>\u00a0\u2013 warns of usage that does not conform to the Fortran 2008 standard.</p> </li> <li> <p><code>-traceback</code>\u00a0\u2013 enables stack trace if the program crashes.</p> </li> </ul> <p>Also see\u00a0Intel C++ diagnostic options.</p>"},{"location":"compute-systems/derecho/compiling-code-on-derecho/compiler-diagnostic-flags/#gnu","title":"GNU","text":"<ul> <li> <p><code>-ggdb</code>\u00a0\u2013 places symbolic debugging information in the executable   program for use by GDB.</p> </li> <li> <p><code>-fcheck=all</code>\u00a0\u2013 performs all runtime checks (includes bounds checking).</p> </li> <li> <p><code>-Wall</code>\u00a0\u2013 enables all warnings.</p> </li> <li> <p><code>-std=f2008</code>\u00a0\u2013 warns of usage that does not conform to the Fortran 2008 standard.</p> </li> </ul> <p>Also see\u00a0GCC diagnostic warning options.</p>"},{"location":"compute-systems/derecho/compiling-code-on-derecho/compiler-diagnostic-flags/#nvidia-hpc-sdk","title":"NVIDIA HPC SDK","text":"<p>The following compiler flags may be helpful for debugging your code using NVIDIA HPC SDK.</p> <ul> <li> <p><code>-g</code> \u2013 Include symbolic debugging information in the object modules with optimization disabled (<code>-O0</code>).</p> </li> <li> <p><code>-gopt</code>\u00a0\u2013\u00a0 Include symbolic debugging information in the object modules without affecting any optimizations.</p> </li> <li> <p><code>-C</code> or <code>-Mbounds</code>\u00a0\u2013 Add array bounds checking.</p> </li> <li> <p><code>-Mchkptr</code>\u00a0\u2013 Check for unintended de-referencing of NULL pointers.</p> </li> <li> <p><code>-Minform=inform</code>\u00a0- Display all the error messages of any severity (inform, warn, severe and fatal) during compilation phase.</p> </li> </ul> <p>Also see\u00a0NVIDIA HPC SDK documentation.</p>"},{"location":"compute-systems/derecho/starting-derecho-jobs/","title":"Starting Derecho jobs","text":"<p>When you use any of these examples, remember to substitute your own job name and project code, and customize the other directives and commands as necessary.</p> <p>Load all modules that are necessary to run your program at the start of your batch scripts by including a line like this:</p> <pre><code>module load intel cray-mpich\n</code></pre> <p>If you think you might run a particular compiled executable well into the future, we advise that you load specific versions of desired modules to ensure reproducibility. Follow this example: <pre><code>module load intel/2023.0.0 cray-mpich/8.1.25\n</code></pre></p>"},{"location":"compute-systems/derecho/starting-derecho-jobs/#script-examples","title":"Script examples","text":"<p>See this page for many Derecho PBS job script examples: Derecho job script examples</p> <p>When your script is ready, submit your batch job for scheduling as shown\u00a0here.</p>"},{"location":"compute-systems/derecho/starting-derecho-jobs/derecho-job-script-examples-content/","title":"Derecho job script examples content","text":"<p>Running a hybrid CPU program with MPI and OpenMP on Derecho</p> <p>In this example, we run a hybrid application that uses both MPI tasks and OpenMP threads. The executable was compiled using default modules (Intel compilers and MPI). We use a 2 nodes with 32 MPI ranks on each node and 4 OpenMP threads per MPI rank.</p> <p>Whenever you run a program that compiled with OpenMP support, it is important to provide a value for ompthreads in the select statement; PBS will use that value to define the\u00a0<code>OMP_NUM_THREADS</code>\u00a0environment variable.</p> <pre><code>#!/bin/bash\n#PBS -A &lt;project_code&gt;\n#PBS -N hybrid_job\n#PBS -q main\n#PBS -l walltime=01:00:00\n#PBS -l select=2:ncpus=128:mpiprocs=32:ompthreads=4\n\n# Use scratch for temporary files to avoid space limits in /tmp\nexport TMPDIR=${SCRATCH}/temp\nmkdir -p ${TMPDIR}\n\n# Load modules to match compile-time environment\nmodule purge\nmodule load ncarenv/22.12 oneapi/2022.2.1 craype/2.7.19 cray-mpich/8.1.21\n\n# Run application using cray-mpich with binding\nmpiexec --cpu-bind depth -n 64 -ppn 32 -d 4 ./executable_name\n</code></pre> <p>Running an MPI-enabled GPU application on Derecho</p> <p>In this example, we run an MPI CUDA program. The application was compiled using the NVIDIA HPC SDK compilers, the CUDA toolkit, and <code>cray-mpich</code> MPI. We request all four GPUs on each of two nodes.</p> <p>Please ensure that you have the <code>cuda</code> module loaded as shown below when attempting to run GPU applications or nodes may lock up and become unresponsive.</p> <pre><code>#!/bin/bash\n#PBS -A &lt;project_code&gt;\n#PBS -N gpu_job\n#PBS -q main\n#PBS -l walltime=01:00:00\n#PBS -l select=2:ncpus=64:mpiprocs=4:ngpus=4\n\n# Use scratch for temporary files to avoid space limits in /tmp\nexport TMPDIR=${SCRATCH}/temp\nmkdir -p ${TMPDIR}\n\n# Load modules to match compile-time environment\nmodule purge\nmodule load nvhpc cuda cray-mpich\n\n# (Optional: Enable GPU managed memory if required.)\n#   From \u2018man mpi\u2019: This setting will allow MPI to properly\n#   handle unify memory addresses. This setting has performance\n#   penalties as MPICH will perform buffer query on each buffer\n#   that is handled by MPI)\n# If you see runtime errors like\n# (GTL DEBUG: 0) cuIpcGetMemHandle: invalid argument,\n#  CUDA_ERROR_INVALID_VALUE\n# make sure this variable is set\nexport MPICH_GPU_MANAGED_MEMORY_SUPPORT_ENABLED=1\n\n# Run application using the cray-mpich MPI\n#   The \u2018set_gpu_rank\u2019 command is a script that sets several GPU-\n#   related environment variables to allow MPI-enabled GPU\n#   applications to run. The set_gpu_rank script is detailed\n#   in the binding section below, and is also made available\n#   via the ncarenv module.\nmpiexec -n 8 -ppn 4 set_gpu_rank ./executable_name\n</code></pre> <p>Binding MPI ranks to CPU cores and GPU devices on Derecho</p> <p>For some GPU applications, you may need to explicitly control the mapping between MPI ranks and GPU devices (see man mpi). One approach is to manually control the <code>CUDA_VISIBLE_DEVICES</code> environment variable so a given MPI rank only \u201csees\u201d a subset of the GPU devices on a node.</p> <p>Consider the following shell script: set_gpu_rank<pre><code>#!/bin/bash\n\nexport MPICH_GPU_SUPPORT_ENABLED=1\nexport LOCAL_RANK=${PMI_LOCAL_RANK}\nexport GLOBAL_RANK=${PMI_RANK}\nexport CUDA_VISIBLE_DEVICES=$(expr ${LOCAL_RANK} % 4)\n\necho \"Global Rank ${GLOBAL_RANK} / Local Rank ${LOCAL_RANK} / CUDA_VISIBLE_DEVICES=${CUDA_VISIBLE_DEVICES} / $(hostname)\"\n\nexec $*\n</code></pre> It can be used underneath mpiexec to bind an MPI process to a particular GPU:</p> <pre><code>#PBS -l select=2:ncpus=64:mpiprocs=4:ngpus=4\n...\n# Run application using the cray-mpich MPI, binding the local\n# mpi rank [0-3] to corresponding GPU index [0-3]:\nmpiexec -n 8 -ppn 4 ./set_gpu_rank ./executable_name\n</code></pre> <p>The command above will launch a total of 8 MPI ranks across 2 nodes, using 4 MPI ranks per node, and each rank will have dedicated access to one of the 4 GPUs on the node. Again, see <code>man mpi</code> for other examples and scenarios.</p> <p>Binding MPI ranks to CPU cores can also be an important performance consideration for GPU-enabled codes, and can be done with the <code>--cpu-bind</code> option to <code>mpiexec</code>. For the above example using 2 nodes, 4 MPI ranks per node, and 1 GPU per MPI rank, binding each of the MPI ranks to one of the four separate NUMA domains within a node is likely to be optimal for performance. This could be done as follows: <pre><code>mpiexec -n 8 -ppn 4 --cpu-bind verbose,list:0:16:32:48 ./set_gpu_rank ./executable_name\n</code></pre></p> <p>Running a containerized application  under MPI on GPUs</p> <pre><code>#!/bin/bash\n#PBS -q main\n#PBS -j oe\n#PBS -o fasteddy_job.log\n#PBS -l walltime=02:00:00\n#PBS -l select=6:ncpus=64:mpiprocs=4:ngpus=4\n\nmodule load ncarenv/23.09\nmodule load apptainer gcc cuda || exit 1\nmodule list\n\nnnodes=$(cat ${PBS_NODEFILE} | sort | uniq | wc -l)\nnranks=$(cat ${PBS_NODEFILE} | sort | wc -l)\nnranks_per_node=$((${nranks} / ${nnodes}))\n\ncontainer_image=\"./rocky8-openhpc-fasteddy.sif\"\n\nsingularity \\\n    --quiet \\\n    exec \\\n    ${container_image} \\\n    ldd /opt/local/FastEddy-model/SRC/FEMAIN/FastEddy\n\nsingularity \\\n    --quiet \\\n    exec \\\n    --bind ${SCRATCH} \\\n    --bind ${WORK} \\\n    --pwd $(pwd) \\\n    --bind /run \\\n    --bind /opt/cray \\\n    --bind /usr/lib64:/host/lib64 \\\n    --env LD_LIBRARY_PATH=${CRAY_MPICH_DIR}/lib-abi-mpich:/opt/cray/pe/lib64:${LD_LIBRARY_PATH}:/host/lib64 \\\n    --env LD_PRELOAD=/opt/cray/pe/mpich/${CRAY_MPICH_VERSION}/gtl/lib/libmpi_gtl_cuda.so.0 \\\n    ${container_image} \\\n    ldd /opt/local/FastEddy-model/SRC/FEMAIN/FastEddy\n\n\n\necho \"# --&gt; BEGIN execution\"; tstart=$(date +%s)\n\nmpiexec \\\n    --np ${nranks} --ppn ${nranks_per_node} --no-transfer \\\n    set_gpu_rank \\\n    singularity \\\n    --quiet \\\n    exec \\\n    --bind ${SCRATCH} \\\n    --bind ${WORK} \\\n    --pwd $(pwd) \\\n    --bind /run \\\n    --bind /opt/cray \\\n    --bind /usr/lib64:/host/lib64 \\\n    --env LD_LIBRARY_PATH=${CRAY_MPICH_DIR}/lib-abi-mpich:/opt/cray/pe/lib64:${LD_LIBRARY_PATH}:/host/lib64 \\\n    --env LD_PRELOAD=/opt/cray/pe/mpich/${CRAY_MPICH_VERSION}/gtl/lib/libmpi_gtl_cuda.so.0 \\\n    --env MPICH_GPU_SUPPORT_ENABLED=1 \\\n    --env MPICH_GPU_MANAGED_MEMORY_SUPPORT_ENABLED=1 \\\n    --env MPICH_SMP_SINGLE_COPY_MODE=NONE \\\n    ${container_image} \\\n    /opt/local/FastEddy-model/SRC/FEMAIN/FastEddy \\\n    ./Example02_CBL.in\n\necho \"# --&gt; END execution\"\necho $(($(date +%s)-${tstart})) \" elapsed seconds; $(date)\"\n</code></pre> <p>See here for a more complete discussion of the nuances of containerized applications on Derecho.</p>"},{"location":"compute-systems/derecho/starting-derecho-jobs/derecho-job-script-examples/","title":"Derecho batch job script examples","text":"<p>When using these examples to create your own job scripts to run on Derecho, remember to substitute your own job name and project code, and customize the other directives and commands as necessary.</p> <p>Running a hybrid CPU program with MPI and OpenMP on Derecho</p> <p>In this example, we run a hybrid application that uses both MPI tasks and OpenMP threads. The executable was compiled using default modules (Intel compilers and MPI). We use a 2 nodes with 32 MPI ranks on each node and 4 OpenMP threads per MPI rank.</p> <p>Whenever you run a program that compiled with OpenMP support, it is important to provide a value for ompthreads in the select statement; PBS will use that value to define the\u00a0<code>OMP_NUM_THREADS</code>\u00a0environment variable.</p> <pre><code>#!/bin/bash\n#PBS -A &lt;project_code&gt;\n#PBS -N hybrid_job\n#PBS -q main\n#PBS -l walltime=01:00:00\n#PBS -l select=2:ncpus=128:mpiprocs=32:ompthreads=4\n\n# Use scratch for temporary files to avoid space limits in /tmp\nexport TMPDIR=${SCRATCH}/temp\nmkdir -p ${TMPDIR}\n\n# Load modules to match compile-time environment\nmodule purge\nmodule load ncarenv/22.12 oneapi/2022.2.1 craype/2.7.19 cray-mpich/8.1.21\n\n# Run application using cray-mpich with binding\nmpiexec --cpu-bind depth -n 64 -ppn 32 -d 4 ./executable_name\n</code></pre> <p>Running an MPI-enabled GPU application on Derecho</p> <p>In this example, we run an MPI CUDA program. The application was compiled using the NVIDIA HPC SDK compilers, the CUDA toolkit, and <code>cray-mpich</code> MPI. We request all four GPUs on each of two nodes.</p> <p>Please ensure that you have the <code>cuda</code> module loaded as shown below when attempting to run GPU applications or nodes may lock up and become unresponsive.</p> <pre><code>#!/bin/bash\n#PBS -A &lt;project_code&gt;\n#PBS -N gpu_job\n#PBS -q main\n#PBS -l walltime=01:00:00\n#PBS -l select=2:ncpus=64:mpiprocs=4:ngpus=4\n\n# Use scratch for temporary files to avoid space limits in /tmp\nexport TMPDIR=${SCRATCH}/temp\nmkdir -p ${TMPDIR}\n\n# Load modules to match compile-time environment\nmodule purge\nmodule load nvhpc cuda cray-mpich\n\n# (Optional: Enable GPU managed memory if required.)\n#   From \u2018man mpi\u2019: This setting will allow MPI to properly\n#   handle unify memory addresses. This setting has performance\n#   penalties as MPICH will perform buffer query on each buffer\n#   that is handled by MPI)\n# If you see runtime errors like\n# (GTL DEBUG: 0) cuIpcGetMemHandle: invalid argument,\n#  CUDA_ERROR_INVALID_VALUE\n# make sure this variable is set\nexport MPICH_GPU_MANAGED_MEMORY_SUPPORT_ENABLED=1\n\n# Run application using the cray-mpich MPI\n#   The \u2018set_gpu_rank\u2019 command is a script that sets several GPU-\n#   related environment variables to allow MPI-enabled GPU\n#   applications to run. The set_gpu_rank script is detailed\n#   in the binding section below, and is also made available\n#   via the ncarenv module.\nmpiexec -n 8 -ppn 4 set_gpu_rank ./executable_name\n</code></pre> <p>Binding MPI ranks to CPU cores and GPU devices on Derecho</p> <p>For some GPU applications, you may need to explicitly control the mapping between MPI ranks and GPU devices (see man mpi). One approach is to manually control the <code>CUDA_VISIBLE_DEVICES</code> environment variable so a given MPI rank only \u201csees\u201d a subset of the GPU devices on a node.</p> <p>Consider the following shell script: set_gpu_rank<pre><code>#!/bin/bash\n\nexport MPICH_GPU_SUPPORT_ENABLED=1\nexport LOCAL_RANK=${PMI_LOCAL_RANK}\nexport GLOBAL_RANK=${PMI_RANK}\nexport CUDA_VISIBLE_DEVICES=$(expr ${LOCAL_RANK} % 4)\n\necho \"Global Rank ${GLOBAL_RANK} / Local Rank ${LOCAL_RANK} / CUDA_VISIBLE_DEVICES=${CUDA_VISIBLE_DEVICES} / $(hostname)\"\n\nexec $*\n</code></pre> It can be used underneath mpiexec to bind an MPI process to a particular GPU:</p> <pre><code>#PBS -l select=2:ncpus=64:mpiprocs=4:ngpus=4\n...\n# Run application using the cray-mpich MPI, binding the local\n# mpi rank [0-3] to corresponding GPU index [0-3]:\nmpiexec -n 8 -ppn 4 ./set_gpu_rank ./executable_name\n</code></pre> <p>The command above will launch a total of 8 MPI ranks across 2 nodes, using 4 MPI ranks per node, and each rank will have dedicated access to one of the 4 GPUs on the node. Again, see <code>man mpi</code> for other examples and scenarios.</p> <p>Binding MPI ranks to CPU cores can also be an important performance consideration for GPU-enabled codes, and can be done with the <code>--cpu-bind</code> option to <code>mpiexec</code>. For the above example using 2 nodes, 4 MPI ranks per node, and 1 GPU per MPI rank, binding each of the MPI ranks to one of the four separate NUMA domains within a node is likely to be optimal for performance. This could be done as follows: <pre><code>mpiexec -n 8 -ppn 4 --cpu-bind verbose,list:0:16:32:48 ./set_gpu_rank ./executable_name\n</code></pre></p> <p>Running a containerized application  under MPI on GPUs</p> <pre><code>#!/bin/bash\n#PBS -q main\n#PBS -j oe\n#PBS -o fasteddy_job.log\n#PBS -l walltime=02:00:00\n#PBS -l select=6:ncpus=64:mpiprocs=4:ngpus=4\n\nmodule load ncarenv/23.09\nmodule load apptainer gcc cuda || exit 1\nmodule list\n\nnnodes=$(cat ${PBS_NODEFILE} | sort | uniq | wc -l)\nnranks=$(cat ${PBS_NODEFILE} | sort | wc -l)\nnranks_per_node=$((${nranks} / ${nnodes}))\n\ncontainer_image=\"./rocky8-openhpc-fasteddy.sif\"\n\nsingularity \\\n    --quiet \\\n    exec \\\n    ${container_image} \\\n    ldd /opt/local/FastEddy-model/SRC/FEMAIN/FastEddy\n\nsingularity \\\n    --quiet \\\n    exec \\\n    --bind ${SCRATCH} \\\n    --bind ${WORK} \\\n    --pwd $(pwd) \\\n    --bind /run \\\n    --bind /opt/cray \\\n    --bind /usr/lib64:/host/lib64 \\\n    --env LD_LIBRARY_PATH=${CRAY_MPICH_DIR}/lib-abi-mpich:/opt/cray/pe/lib64:${LD_LIBRARY_PATH}:/host/lib64 \\\n    --env LD_PRELOAD=/opt/cray/pe/mpich/${CRAY_MPICH_VERSION}/gtl/lib/libmpi_gtl_cuda.so.0 \\\n    ${container_image} \\\n    ldd /opt/local/FastEddy-model/SRC/FEMAIN/FastEddy\n\n\n\necho \"# --&gt; BEGIN execution\"; tstart=$(date +%s)\n\nmpiexec \\\n    --np ${nranks} --ppn ${nranks_per_node} --no-transfer \\\n    set_gpu_rank \\\n    singularity \\\n    --quiet \\\n    exec \\\n    --bind ${SCRATCH} \\\n    --bind ${WORK} \\\n    --pwd $(pwd) \\\n    --bind /run \\\n    --bind /opt/cray \\\n    --bind /usr/lib64:/host/lib64 \\\n    --env LD_LIBRARY_PATH=${CRAY_MPICH_DIR}/lib-abi-mpich:/opt/cray/pe/lib64:${LD_LIBRARY_PATH}:/host/lib64 \\\n    --env LD_PRELOAD=/opt/cray/pe/mpich/${CRAY_MPICH_VERSION}/gtl/lib/libmpi_gtl_cuda.so.0 \\\n    --env MPICH_GPU_SUPPORT_ENABLED=1 \\\n    --env MPICH_GPU_MANAGED_MEMORY_SUPPORT_ENABLED=1 \\\n    --env MPICH_SMP_SINGLE_COPY_MODE=NONE \\\n    ${container_image} \\\n    /opt/local/FastEddy-model/SRC/FEMAIN/FastEddy \\\n    ./Example02_CBL.in\n\necho \"# --&gt; END execution\"\necho $(($(date +%s)-${tstart})) \" elapsed seconds; $(date)\"\n</code></pre> <p>See here for a more complete discussion of the nuances of containerized applications on Derecho.</p>"},{"location":"compute-systems/derecho/starting-derecho-jobs/process-binding/","title":"Process binding","text":"<p>How to pin MPI processes to CPU cores depends on if the programs are pure MPI programs, hybrid MPI, or OpenMP programs.</p>"},{"location":"compute-systems/derecho/starting-derecho-jobs/process-binding/#pure-mpi-programs-mpi-only-no-threads","title":"Pure MPI programs (MPI only, no threads)","text":""},{"location":"compute-systems/derecho/starting-derecho-jobs/process-binding/#hybrid-mpi-openmp-programs","title":"Hybrid MPI + OpenMP programs","text":""},{"location":"compute-systems/jupyterhub/","title":"JupyterHub at NCAR","text":"<p>The JupyterHub deployment that CISL manages allows \"push-button\" access to NCAR's Cheyenne supercomputing resource and the Casper cluster of nodes used for data analysis and visualization, machine learning, and deep learning.</p> <p>It gives users the ability to create, save, and share Jupyter Notebooks through the JupyterLab interface and to run interactive, web-based analysis, visualization and compute jobs on Cheyenne and Casper. JupyterHub is an alternative to X11 access for interacting with those resources to run jobs as well as for using web-based interactive shell functionality without the need to install or use software such as SSH or PuTTY.</p>"},{"location":"compute-systems/jupyterhub/#getting-started","title":"Getting started","text":"<p>Use your web browser to go to jupyterhub.hpc.ucar.edu. Chrome and Firefox are recommended for all users.</p> <p>Select Production.</p> <p></p> <p>Log in with your NCAR username and Duo two-factor authentication, just as you would when logging directly in to either system.</p> <p></p> <p>After you authenticate, you will be able to start a new default server or create a named server. (See following image.) You can have up to four named servers to use for accessing different compute resources.</p> <p>Do not start a new server simply to run additional notebooks!</p> <p>Do not start a new server simply to run additional notebooks; a single server can support multiple notebooks at once. However, executing the same notebook file in multiple servers concurrently can lead to kernel failures and other errors.</p> <p></p> <p>After starting a server, select the cluster you want to use. You can choose to work on a login node or a batch node on either Casper or Cheyenne. </p> <p>If you choose a login node, launching the server will take you to the web interface.</p> <p>If you choose a batch node, use the form provided (images below) to specify your project code, set the necessary PBS job options, and launch the appropriate server. The name of your batch job will be <code>STDIN</code>.</p> <p>For more information about the options, see: Submitting jobs with PBS.</p> <p>Launch your job when ready. This job only gives you access to the JupyterLab instance. If you need more resources, you can launch another job or jobs from within JupyterLab.</p> <p> </p> <p>After launching the job, you will have access to multiple kernels in the web interface (image below) for working with various languages and applications.</p> <p>Where are my files?</p> <p>Note that the \u201cFile browser\u201d icon (upper-left of following image) allows you to explore your home directory only. To change to your scratch or work space, create soft links in your home directory to those locations.</p> <p></p>"},{"location":"compute-systems/jupyterhub/#python-environments-and-kernels","title":"Python environments and kernels","text":"<p>The JupyterLab dashboard provides access to Notebook and Console kernels, which are programming language interpreters. Available kernels, which change periodically as new releases are installed, include:</p> <ul> <li> <p>Multiple Python 3 interpreters with varying package support including   a basic install (Python 3), the Pangeo stack installed with conda   (Pangeo), and the NCAR Python Library (NPL) that is   also provided at the command-line by the <code>conda</code> environment module.</p> </li> <li> <p>R</p> </li> <li> <p>MATLAB</p> </li> <li> <p>Julia</p> </li> <li> <p>A Bash interpreter that provides a shell environment</p> </li> </ul>"},{"location":"compute-systems/jupyterhub/#related-documentation","title":"Related documentation","text":"<p>See these related CISL documentation pages for additional support:</p> <ul> <li>Using Conda and Python</li> </ul>"},{"location":"environment-and-software/","title":"Overview","text":"<p>CISL deploys a wide range of preconfigured software packages for general use through several deployment mechanisms, accessible as described in the User Environment section.</p> <p>Specific tools and processes are further documented in the HPC Software section of this guide, with explicit instructions for using particular tools such as Matlab, Profilers &amp; Debuggers, etc...</p>"},{"location":"environment-and-software/community-models/","title":"Community models","text":"<p>The models described below are available for use on NCAR computers that CISL manages.</p> <p></p> <p>Please contact the NCAR Research Computing help desk if you need assistance.</p> <ul> <li> <p>Community Earth System Model (CESM) \u2013 A   fully coupled, global climate model developed at NCAR. CESM (formerly   CCSM) provides state-of-the-art computer simulations of the Earth's   past, present, and future climate states. The CESM simulation image   above is from the CISL Visualization Gallery.</p> </li> <li> <p>Weather Research &amp; Forecasting (WRF) model \u2013   A next-generation mesoscale, numerical weather-prediction system   designed to serve both operational forecasting and atmospheric   research needs.</p> </li> <li> <p>Whole Atmosphere Community Climate Model (WACCM) \u2013   A comprehensive numerical model spanning the range of altitude from   the Earth's surface to the thermosphere, developed by a   collaboration of NCAR\u2019s High Altitude Observatory, Atmospheric   Chemistry Observations &amp; Modeling, and Climate and Global Dynamics   division.</p> </li> <li> <p>Atmospheric chemistry models \u2013   NCAR\u2019s Atmospheric Chemistry Observations &amp; Modeling builds,   critically evaluates, and applies process, regional- and global-scale   models that address atmospheric chemistry research questions, with a   focus on couplings between different components of the Earth system.</p> </li> <li> <p>Thermospheric General Circulation Models (TGCMs) \u2013   The High Altitude Observatory (HAO) at NCAR has developed a series   of numeric simulation models \u2013 such as TIEGCM and TIME-GCM \u2013 of the   Earth's upper atmosphere, including the upper stratosphere,   mesosphere, and thermosphere.</p> </li> <li> <p>Data Assimilation Research Testbed (DART) \u2013   A community facility for ensemble data assimilation developed and   maintained by NCAR\u2019s Data Assimilation Research Section   (DAReS).</p> </li> </ul>"},{"location":"environment-and-software/data-analysis-and-visualization/","title":"Data analysis and visualization","text":"<p>Many data analysis and visualization software packages are freely available for use on CISL-managed resources. These packages include some developed and supported by NCAR and CISL.</p>"},{"location":"environment-and-software/data-analysis-and-visualization/#frequently-used-packages","title":"Frequently used packages","text":"<p>These are among the more frequently used data analysis and visualization packages available on NCAR systems. To request installation of other packages, contact the NCAR Research Computing help desk.</p>"},{"location":"environment-and-software/data-analysis-and-visualization/#grads","title":"GrADS","text":"<p>The Grid Analysis and Display System (GrADS) is an interactive desktop tool for visualizing earth science data.</p>"},{"location":"environment-and-software/data-analysis-and-visualization/#idl","title":"IDL","text":"<p>IDL is Interactive Data Language, which is used for data visualization and analysis. Documentation is available here.</p>"},{"location":"environment-and-software/data-analysis-and-visualization/#matlab","title":"MATLAB","text":"Please follow these license use guidelines for Matlab <p>The CISL user community shares a limited number of licenses for running MATLAB, MATLAB Toolboxes, and some other applications.</p> <p>Follow these guidelines to ensure fair access for all users:</p> <ul> <li> <p>Avoid monopolizing these licenses.</p> </li> <li> <p>If you need to use multiple licenses at one time, be considerate of   others and finish your session as quickly as possible.</p> </li> <li> <p>Close applications when you are done to free up licenses for others to   use.</p> </li> </ul> <p>CISL reserves the right to kill jobs/tasks of users who monopolize these licenses.</p> <p>To see how many licenses are being used, run licstatus at your command line. You will see columns showing how many licenses you're using, the total number of licenses in use, and the total number of licenses. <pre><code>licstatus\n</code></pre></p> <p>This is a high-level language and interactive environment for data analysis, statistics, and image processing. Several MATLAB toolboxes are provided (list below). See the MathWorks web site for documentation and note the information just below about Octave, an alternative to MATLAB. Related: MATLAB Parallel Computing Toolbox</p> <p>MATLAB toolboxes</p> <ul> <li>Image Processing Toolbox</li> <li>Mapping Toolbox</li> <li>MATLAB Compiler</li> <li>Neural Network Toolbox</li> <li>Optimization Toolbox</li> <li>Parallel Computing Toolbox</li> <li>Signal Processing Toolbox</li> <li>Statistics Toolbox</li> <li>Wavelet Toolbox</li> </ul>"},{"location":"environment-and-software/data-analysis-and-visualization/#matlab-alternative-octave","title":"MATLAB alternative - Octave","text":"<p>Many MATLAB codes run with very little or no modification under Octave, a free interactive data analysis software package with syntax and functionality that are similar to MATLAB's. Since using Octave is not constrained by license issues, we encourage MATLAB users to try it, particularly those who have long-running MATLAB jobs. Depending on compute intensity, Octave usually runs slower than MATLAB but it may be suitable for most data analysis work and you won't risk having jobs killed because of a lack of licenses.</p> <p>To use Octave interactively, start an interactive job and load the module. <pre><code>module load octave\n</code></pre> Run <code>octave</code> to start the command line interface, or run the following command to use the GUI. <pre><code>octave --force-gui\n</code></pre></p>"},{"location":"environment-and-software/data-analysis-and-visualization/#ncl","title":"NCL","text":"<p>NCAR Command Language is an interpreted language that CISL designed for scientific data analysis and visualization.</p>"},{"location":"environment-and-software/data-analysis-and-visualization/#paraview","title":"ParaView<code>*</code>","text":"<p>This is an open-source application for building visualizations and analyzing data, either interactively in 3D or through batch processing. See ParaView.org for documentation.</p>"},{"location":"environment-and-software/data-analysis-and-visualization/#pyngl-and-pynio","title":"PyNGL and PyNIO","text":"<p>Python packages that CISL developed for scientific visualization, file input/output, and data analysis.</p>"},{"location":"environment-and-software/data-analysis-and-visualization/#vapor","title":"VAPOR<code>*</code>","text":"<p>The Visualization and Analysis Platform for Ocean, Atmosphere, and Solar Researchers is a desktop platform that provides an interactive 3D visualization environment for exploring geosciences CFD data sets. See VAPOR.</p> <p>Many additional applications and tools that are commonly used by atmospheric and Earth system scientists are available on NCAR HPC resources and through the CISL Research Data Archive. These include Mathematica, Vis5d, and VTK.</p> <p>Those marked with an asterisk should be run only on the Casper nodes because of their graphics and GPU requirements. Others can be used on Cheyenne. Check the man pages for any program to get additional information. </p>"},{"location":"environment-and-software/data-analysis-and-visualization/#advanced-visualization-support","title":"Advanced visualization support","text":"<p>Researchers who need help visualizing data to demonstrate the results of their scientific computing can request expert assistance and collaboration from CISL. This service is available to researchers who want help using specialized applications on data analysis and visualization resources that CISL manages.</p> <p></p> <p>CISL visualization staff have particular expertise in CISL-developed software such as VAPOR and NCL but are open to assisting with other applications. They have helped science teams produce numerous visualizations for conferences, publications, and scientific journals as well as science and broadcast news programs.</p> <p>See these sites for some examples:</p> <ul> <li> <p>CISL Visualization Gallery</p> </li> <li> <p>NCAR VisLab YouTube Channel</p> </li> </ul>"},{"location":"environment-and-software/data-analysis-and-visualization/#requesting-support","title":"Requesting support","text":"<p>To ask for assistance, please use this request form. Indicate clearly that the request is for \"Advanced Visualization Support\" and describe how the proposed visualization project meets the following criteria:</p> <ul> <li> <p>The assistance will culminate in a visualization deliverable or   deliverables within a defined time period.</p> </li> <li> <p>The deliverable(s) will help demonstrate science results and findings.</p> </li> <li> <p>A member or members of the science team will participate actively in   the production of the deliverable(s).</p> </li> <li> <p>Data to be visualized must have been generated on a CISL   high-performance computing system.</p> </li> <li> <p>CISL data analysis and visualization resources will be used in   creating the deliverables.</p> </li> </ul> <p>Requests for advanced visualization support are evaluated based on those criteria to ensure the most productive use of the limited available staff time. To further help us evaluate your request, please briefly address the following questions when submitting the request form:</p> <ol> <li> <p>What does the scientist want to communicate about the data with a     visualization?</p> </li> <li> <p>What type of visualization is needed (2D or 3D animation, still     images)?</p> </li> <li> <p>What model or instrument produced the data?</p> </li> <li> <p>How big are the data?</p> </li> <li> <p>In what file format are the data stored?</p> </li> </ol> <p>The CISL visualization staff will follow up with requesters directly if additional details are needed.</p>"},{"location":"environment-and-software/data-analysis-and-visualization/#starting-visualization-applications","title":"Starting visualization applications","text":"<p>CISL visualization experts can help as described above with using applications such as NCL and VAPOR.</p> <p>See the Casper documentation to learn how to submit the necessary jobs and start the applications on that data analysis and visualization cluster.</p>"},{"location":"environment-and-software/machine-learning-and-deep-learning/","title":"Machine learning and deep learning","text":"<p>CISL provides several libraries for users' machine learning and deep learning (ML/DL) work.</p> <p>These libraries have been compiled from source to use native CUDA (GPU) and MPI libraries, increasing the capabilities over downloadable distributions that are available online. The ML/DL library installations can be found in the NCAR Python Library.</p> <p>The libraries available are:</p> <ul> <li> <p>TensorFlow machine learning library   v2.3.1</p> </li> <li> <p>PyTorch machine learning library v1.7.1</p> </li> <li> <p>scikit-learn machine learning library   v0.5.3</p> </li> <li> <p>Horovod deep learning framework   v0.21.0</p> </li> <li> <p>Keras deep learning library v2.4.3</p> </li> </ul>"},{"location":"environment-and-software/matlab-parallel-computing-toolbox/","title":"MATLAB Parallel Computing Toolbox on Casper and Cheyenne","text":"<p>The MATLAB Parallel Computing Toolbox (PCT) allows you to run MATLAB code in parallel across multiple workers, which are analogous to MPI tasks or OpenMP threads. Additionally, NCAR has a license for the MATLAB Parallel Server (MPS) \u2013 formerly the MATLAB Distributed Computing Server \u2013 which allows you to run a MATLAB script using workers from multiple compute nodes via a batch job you submit with PBS.</p> <p>Using MPS is critical for workflows that result in MATLAB processing running on multiple nodes. Here's why:</p> <p>Any MATLAB component \u2013 MATLAB itself or one of the many toolboxes \u2013 follows the same rules regarding license use. When you load MATLAB or activate a toolbox, you check out a single license. If you launch the same product on the same node, no additional licenses are used. However, if you run the same product on a different node, you consume another license. These rules mean that running multiple batch jobs, with each executing a MATLAB script, consumes many licenses if they are scheduled on different compute nodes.</p> <p>The MPS alleviates this issue. If you start a PBS job via the MPS, you consume only a single MATLAB license (and single licenses of any other toolboxes loaded including the PCT itself) no matter how many nodes you request. Instead, MATLAB will use MPS worker licenses for each worker requested. Therefore, if you have a workflow that will result in MATLAB processes running on multiple nodes, always look to use the MATLAB Parallel Server.</p> <p>In summary, here are basic guidelines for when you should use each product:</p> <ul> <li> <p>MATLAB only - when you need only a single worker/task (e.g.,   running a script with only serial computation)</p> </li> <li> <p>PCT (local) - when you want to run a script or function using a   parallel pool of workers on a single node</p> </li> <li> <p>MPS (distributed PCT) - when you need to run a script/function   across multiple nodes of workers OR you want to run multiple scripts   on multiple nodes (similar to a command-file PBS   job)</p> </li> </ul>"},{"location":"environment-and-software/matlab-parallel-computing-toolbox/#running-a-simple-parallel-code-on-one-node-using-the-toolbox","title":"Running a simple parallel code on one node using the toolbox","text":"<p><code>fun_local</code></p> <p>In this example, a simple code uses multiple workers on a single node to compute a sum in parallel. Here is the sample code: <pre><code># parallel_sum.m:\nfunction s = parallel_sum(N)\n  s = 0;\n  parfor i = 1:N\n    s = s + i;\n  end\n\n  fprintf('Sum of numbers from 1 to %d is %d.n', N, s);\nend\n</code></pre></p> <p>This function is executed by MATLAB code that reads in a few parameters from your user environment. For single-node parallel jobs, this example uses the \u201clocal\u201d cluster profile that is available by default when using the toolbox.</p> <p>Warning</p> <p>Do not run the local toolbox profile on login nodes; excessive CPU and/or memory use will result in your script being terminated. Instead, use a PBS batch job as in the following example to run a single-node cluster on a compute node.</p> <p>PBS Submission Script <code>submit_local.pbs</code></p> <p>This PBS job also specifies the number of workers, which corresponds to the number of CPUs requested in the job script.</p> <pre><code>#!/bin/bash\n#PBS -N matlab_pct\n#PBS -A &lt;PROJECT&gt;\n#PBS -l walltime=05:00\n#PBS -q casper\n#PBS -j oe\n#PBS -o local.log\n#PBS -l select=1:ncpus=4:mpiprocs=4:mem=10GB\n\n# The following script is not distributed; it uses threads\n# and so this PBS job should only ever request a single node\n\nmodule load matlab\n\n# Derive the number of workers to use in the toolbox run script\nexport NUMWORKERS=$(wc -l $PBS_NODEFILE | cut -d' ' -f1)\n\nSECONDS=0\nmatlab -nodesktop -nosplash &lt;&lt; EOF\n% Start local cluster and submit job with custom number of workers\nc = parcluster('local')\nj = c.batch(@parallel_sum, 1, {100}, 'pool', $((NUMWORKERS - 1)));\n\n% Wait for the job to finish, then get output\nwait(j);\ndiary(j);\nexit;\nEOF\necho \"Time elapsed = $SECONDS s\"\n</code></pre>"},{"location":"environment-and-software/matlab-parallel-computing-toolbox/#mps-cluster-profiles","title":"MPS cluster profiles","text":"<p>When using the PCT, you are expected to create and use cluster profiles that manage either node-local tasks or batch-scheduler tasks. While there is a preconfigured profile for single-node use (local), you will need to do some setup before you can use the MPS. CISL provides a distributed cluster profile for PBS on both Casper and Cheyenne for all versions of MATLAB starting with R2020a.</p> <p>You can import an existing cluster profile using the wizard in the graphical interface, or you can do it programmatically as follows. If you use our sample distributed script provided in the following section, we include the MPS cluster profile setup for you, so you can skip the commands in this section.</p> <p>At the MATLAB command line, enter the following line to import the MPS profile: <pre><code>ncar_mps = parallel.importProfile('/glade/u/apps/opt/matlab/parallel/ncar_mps.mlsettings');\n</code></pre></p> <p>You need to import the profile only once; MATLAB will remember it in future sessions. If you anticipate using the parallel server profile frequently, you may want to make it your default parallel profile as shown here: <pre><code>parallel.defaultClusterProfile(ncar-mps);\n</code></pre></p>"},{"location":"environment-and-software/matlab-parallel-computing-toolbox/#using-the-matlab-parallel-server-mps-to-span-multiple-nodes","title":"Using the MATLAB parallel server (MPS) to span multiple nodes","text":"<p>The configuration above will limit your job to the number of CPUs on a single node; on Casper and Cheyenne this means 36 workers, or 72 if you use hyperthreads. However, you can use the parallel server to span multiple nodes. When using MPS, MATLAB itself will submit a job to the batch scheduler and use an internal MPI library to enable communication between remote workers.</p> <p><code>func_mps</code> Using the MATLAB parallel server</p> <p>Here again, use a MATLAB script to set up your parallel cluster as in this example, which embeds MATLAB code into a driver script <code>submit_server.sh</code>:</p> <pre><code>#!/bin/bash\n\n# This script doesn't need to run on a batch node... we can simply submit\n# the parallel job by running this script on the login node\n\nmodule rm ncarenv\nmodule load matlab\n\nmkdir -p output\n\n# Job parameters\nMPSNODES=2\nMPSTASKS=4\nMPSACCOUNT=&lt;PROJECT&gt;\nMPSQUEUE=casper@casper-pbs\nMPSWALLTIME=300\nSECONDS=0\n\nmatlab -nodesktop -nosplash &lt;&lt; EOF\n% Add cluster profile if not already present\nif ~any(strcmp(parallel.clusterProfiles, 'ncar_mps'))\n    ncar_mps = parallel.importProfile('/glade/u/apps/opt/matlab/parallel/ncar_mps.mlsettings');\nend\n\n% Start PBS cluster and submit job with custom number of workers\nc = parcluster('ncar_mps');\n\n% Matlab workers will equal nodes * tasks-per-node - 1\njNodes = '$MPSNODES';\njTasks = '$MPSTASKS';\njWorkers = str2num(jNodes) * str2num(jTasks) - 1;\n\nc.ClusterMatlabRoot = getenv('NCAR_ROOT_MATLAB');\nc.ResourceTemplate = append('-l select=', jNodes, ':ncpus=', jTasks, ':mpiprocs=', jTasks);\nc.SubmitArguments = append('-A $MPSACCOUNT -q $MPSQUEUE -l walltime=$MPSWALLTIME');\nc.JobStorageLocation = append(getenv('PWD'), '/output');\n\n% Output cluster settings\nc\n\n% Submit job to batch scheduler (PBS)\nj = batch(c, @parallel_sum, 1, {100}, 'pool', jWorkers);\n\n% Wait for job to finish and get output\nwait(j);\ndiary(j);\nexit;\nEOF\n\necho \"Time elapsed = $SECONDS s\"\n</code></pre>"},{"location":"environment-and-software/matlab-parallel-computing-toolbox/#sample-pct-scripts","title":"Sample PCT scripts","text":"<p>Including the scripts shown above, there are four sets of example scripts that you can use, modify, and extend to fit your purposes. All four examples can be copied from <code>/glade/u/apps/opt/matlab/parallel/examples</code>.</p> <ul> <li> <p><code>func_local</code> - Run a specified function using a pool of local   (single-node) workers. This is Example 1 above.</p> </li> <li> <p><code>func_mps</code> - Run a specified function using a pool of workers   distributed across multiple nodes. This is Example 2 above.</p> </li> <li> <p><code>multi_script_mps</code> - Run a specified collection of MATLAB functions   in separate script files using a pool of workers. Configured for MPS   use but can be modified to use the local profile.</p> </li> <li> <p><code>spmd_mps</code> - Run a single MATLAB function in a specified script   using many input parameters.</p> </li> </ul> <p>The last two examples are functionally similar to a command-file PBS job, but with the licensing benefits of using MPS.</p>"},{"location":"environment-and-software/ncar-classic-libraries-for-geophysics/","title":"NCAR Classic Libraries for Geophysics","text":"<p>Several mathematical libraries developed in the years 1970-1990 remain popular in the geophysics community. These libraries, listed below, are available for downloading here on GitHub: NCAR Classic Libraries for-Geophysics.</p> <p></p> <ul> <li> <p><code>FFTPACK</code>: A library of fast Fourier transforms</p> </li> <li> <p><code>FISHPACK</code>: Fortran subprograms for solving separable elliptic   partial differential equations (PDEs)</p> </li> <li> <p><code>FISHPACK 90</code>: FISHPACK subprograms with a Fortran 90 interface</p> </li> <li> <p><code>MUDPACK</code>: Multigrid Fortran subprograms for solving separable and   non-separable elliptic PDEs</p> </li> <li> <p><code>SPHEREPACK</code>: A Fortran library for modeling geophysical processes</p> </li> </ul> <p>All of these library routines are written primarily in Fortran 77. Their internal implementation does not always conform to the Fortran Standard. FISHPACK90 provides a Fortran 90 interface to the FISHPACK routines. Only MUDPACK is written with parallelism in mind; it uses OpenMP directives for shared-memory parallelism. The other libraries were designed to run on a single processor.</p> <p>These libraries represent many person-years of development, and though they are no longer under development, NCAR continues to make them available to the public at no cost under a software licensing agreement. The libraries are best suited to Linux and UNIX environments and require a directory structure, <code>tar</code>, and <code>gmake</code> commands.</p>"},{"location":"environment-and-software/ncl/","title":"Using NCL in the NCAR HPC environment","text":"<p>The NCAR HPC environment supports the use of NCAR Command Language (NCL) both interactively and in batch mode to analyze and visualize data.</p> <p>As described below, to use NCL in the Cheyenne environment you will log in to Derecho or Casper, then:</p> <ul> <li> <p>Start an interactive job on Casper and execute the NCL script from   that window, or</p> </li> <li> <p>Submit a batch job to execute an NCL script.</p> </li> </ul> <p>Follow the instructions below to get started, and customize the scripts and commands as necessary to work with your own data.</p>"},{"location":"environment-and-software/ncl/#other-resources","title":"Other resources","text":"<p>See the NCL web site for complete documentation of the language's extensive analysis and visualization capabilities.</p> <p>See the NCL Applications page for links to hundreds of complete NCL scripts that you can download and modify as needed.</p>"},{"location":"environment-and-software/ncl/#interactive-use","title":"Interactive use","text":"<p>To start an interactive window from which to modify and execute NCL scripts, log in to Casper or Derecho.</p> <p>Start a job on Casper as described in this documentation.</p> <p>When your job starts, load the default module for NCL. <pre><code>module load ncl\n</code></pre></p> <p>Modify your NCL script if necessary using a UNIX editor, and execute it as shown here, substituting the name of your own NCL script for script_name.ncl. <pre><code>ncl script_name.ncl\n</code></pre></p>"},{"location":"environment-and-software/ncl/#submitting-a-batch-script","title":"Submitting a batch script","text":"<p>If you expect running your NCL script to take longer than you would want to work interactively \u2014 overnight, for example \u2014 submit your NCL script in a batch job so it can run unattended. See Starting jobs on Casper nodes for batch job script examples and other details.</p>"},{"location":"environment-and-software/ncl/#visualization-examples","title":"Visualization examples","text":""},{"location":"environment-and-software/ncl/#example-1","title":"Example 1","text":"<p>Make an NCL script file named <code>contour_ts_line.ncl</code> using the sample script below.</p> <p>When you run it on Casper, it will create a simple line contour plot using a sample CMIP5 NetCDF data file in the <code>/glade/u/sampledata/ncl/CESM/CAM5</code> directory. The output to your working directory will be a graphic file called <code>contour_ts_line.png</code>.</p> <p></p> <pre><code>;----------------------------------------------------------------------\n; This script creates a simple line contour plot of the first timestep\n; of the \"ts\" variable on the given NetCDF file.\n;----------------------------------------------------------------------\n\nload \"$NCARG_ROOT/lib/ncarg/nclscripts/csm/gsn_code.ncl\"\nload \"$NCARG_ROOT/lib/ncarg/nclscripts/csm/gsn_csm.ncl\"\n\nbegin\n;---Open file and read data\n  dir      = \"/glade/u/sampledata/ncl/CESM/CAM5/\"\n  filename = \"ts_Amon_CESM1-CAM5_historical_r1i1p1_185001-200512.nc\"\n  a        = addfile(dir+filename,\"r\")\n\n  ts       = a-&gt;ts(0,:,:)            ; Read first time step\n  ts       = ts-273.15               ; convert from Kelvin-&gt;Celsius\n  ts@units = \"degC\"\n\n;---Look at the variable's metadata, if desired\n  printVarSummary(ts)\n\n;---Open file or window to send graphical output to.\n  wks = gsn_open_wks(\"png\",\"contour_ts_line\")   ; \"png\", \"ps\", \"pdf\", \"x11\"\n\n;---Create a default line contour plot.\n  res  = True\n  plot = gsn_csm_contour_map(wks,ts,res)\n\nend\n</code></pre>"},{"location":"environment-and-software/ncl/#example-2","title":"Example 2","text":"<p>Using a different script, you can create a more interesting visualization with the data that was used in the first example.</p> <p>Make an NCL script file named <code>contour_ts_color.ncl</code> using the sample script below.</p> <p>When you run it on Casper, the output to your working directory will be a color-filled contour called <code>contour_ts_color.png</code>.</p> <p></p> <pre><code>;----------------------------------------------------------------------\n; This script creates filled contour plot of the first timestep of\n; the \"ts\" variable on the given NetCDF file.\n;----------------------------------------------------------------------\n\nload \"$NCARG_ROOT/lib/ncarg/nclscripts/csm/gsn_code.ncl\"\nload \"$NCARG_ROOT/lib/ncarg/nclscripts/csm/gsn_csm.ncl\"\n\nbegin\n;---Open file and read data\n  dir      = \"/glade/u/sampledata/ncl/CESM/CAM5/\"\n  filename = \"ts_Amon_CESM1-CAM5_historical_r1i1p1_185001-200512.nc\"\n  a        = addfile(dir+filename,\"r\")\n\n  ts       = a-&gt;ts(0,:,:)        ; Read first time step.\n  ts       = ts-273.15           ; Convert from Kelvin -&gt; Celsius.\n  ts@units = \"degC\"\n\n;---Look at the variable's metadata, if desired\n  printVarSummary(ts)\n\n;---Open file or window to send graphical output to.\n  wks = gsn_open_wks(\"png\",\"contour_ts_color\")   ; \"png\", \"ps\", \"pdf\", \"x11\"\n\n;---Set some graphical resources to customize the contour plot.\n  res                   = True\n\n  res@gsnMaximize       = True        ; Maximize plot in frame\n\n  res@cnFillOn          = True        ; Turn on contour fill\n  res@cnLinesOn         = False       ; Turn off contour lines\n  res@cnLineLabelsOn    = False       ; Turn off line labels\n\n  res@tiMainString      = filename    ; Add a main title\n\n  res@gsnAddCyclic      = True        ; Add longitude cyclic point\n\n;--Set the contour levels using \"nice_mnmxintvl\" function.\n  mnmxint = nice_mnmxintvl( min(ts), max(ts), 18, False)\n  res@cnLevelSelectionMode = \"ManualLevels\"\n  res@cnMinLevelValF       = mnmxint(0)\n  res@cnMaxLevelValF       = mnmxint(1)\n  res@cnLevelSpacingF      = mnmxint(2)/4.  ; Decrease spacing for more levels\n\n;---Create and draw the plot.\n  plot = gsn_csm_contour_map(wks,ts,res)\nend\n</code></pre>"},{"location":"environment-and-software/numerical-libraries/","title":"Numerical Libraries","text":""},{"location":"environment-and-software/numerical-libraries/#intels-math-kernel-library","title":"Intel's Math Kernel Library","text":"<p>The Intel Math Kernel Library (Intel MKL) contains highly optimized, extensively threaded math routines for science, engineering, and financial applications. Core math functions include BLAS, LAPACK, ScaLAPACK, Sparse Solvers, Fast Fourier Transforms, Vector Math, and more.</p> <p>On NCAR systems the MKL is available through the <code>mkl</code> module.  (See here for more discussion on interacting with the module system.)</p> <p>Intel MKL has the following functional categories:</p> <ul> <li> <p>Linear algebra:</p> <ul> <li>BLAS routines are vector-vector (Level 1), matrix-vector (Level 2) and matrix-matrix (Level 3) operations for real and complex single and double precision data.</li> <li>LAPACK consists of tuned LU, Cholesky and QR factorizations, eigenvalue and least squares solvers.</li> <li>Sparse BLAS, ScaLAPACK, Sparse Solver, Extended Eigensolver (FEAST, PARDISO), PBLAS and BLACS.</li> </ul> </li> <li> <p>Fast Fourier Transforms (FFTs) from 1D to multidimensional, complex to complex, real to complex, and real to real transforms of arbitrary lengths. Applications written with the open source FFTW can be easily ported to MKL by linking with interface wrapper libraries provided as part of MKL for easy migration.</p> <ul> <li>Cluster versions of LAPACK and FFTs are also available as part of MKL to take advantage of MPI parallelism in addition to single node parallelism from multithreading.</li> </ul> </li> <li> <p>Vector math functions include computationally intensive core mathematical operations for single and double precision real and complex data types. These are similar to <code>libm</code> functions from compiler libraries but operate on vectors rather than scalars to provide better performance. There are various controls for setting accuracy, error mode and denormalized number handling to customize the behavior of the routines.</p> </li> <li> <p>Statistics functions include random number generators and probability distributions, optimized for multicore processors. Also included are compute-intensive in and out-of-core routines to compute basic statistics, estimation of dependencies etc.</p> </li> <li> <p>Data fitting functions include splines (linear, quadratic, cubic, look-up, stepwise constant) for 1-dimensional interpolation that can be used in data analytics, geometric modeling and surface approximation applications.</p> </li> </ul>"},{"location":"environment-and-software/numerical-libraries/#linking-with-the-mkl","title":"Linking with the MKL","text":"<p>The MKL ships with both serial and parallel versions of many of its core components, and with support for normal and \"long\" (64-bit) indexing integers. See the Link Line Advisor for guidance on how to select precise combinations of the MKL functionality when linking into your application.</p>"},{"location":"environment-and-software/numerical-libraries/#cray-libsci","title":"Cray LibSci","text":"<p>Derecho Only!</p> <p>Most Cray software such as <code>LibSci</code> is only available on Derecho.</p> <p>Cray's <code>LibSci</code> is a collection of numerical routines tuned for performance on Cray systems.</p> <p>Most <code>LibSci</code> components contain both serial and and parallel routines optimized specifically to make best use of Cray processors and interconnect architectures. The general components of Cray <code>LibSci</code> are:</p> <ul> <li> <p>BLAS (Basic Linear Algebra Subroutines)</p> </li> <li> <p>CBLAS (C interface to the legacy BLAS)</p> </li> <li> <p>BLACS (Basic Linear Algebra Communication Subprograms)</p> </li> <li> <p>LAPACK (Linear Algebra Package)</p> </li> <li> <p>ScaLAPACK (Parallel Linear Algebra routines)</p> </li> <li> <p>IRT (Iterative Refinement Toolkit) \u2010 a library of solvers and tools   that provides solutions to linear systems using single\u2010precision   factorizations while preserving accuracy through mixed\u2010precision   iterative refinement.</p> </li> <li> <p>CrayBLAS \u2010 a library of BLAS routines highly optimized for Cray    systems. (For further information, see <code>man intro_blas3</code>.)</p> </li> </ul> <p>For additional details see: <pre><code>$ module load cray-libsci\n$ man intro_libsci\n</code></pre></p>"},{"location":"environment-and-software/numerical-libraries/#fftw","title":"FFTW","text":"<p>FFTW is a C subroutine library for computing the discrete Fourier transform (DFT) in one or more dimensions, of arbitrary input size, and of both real and complex data (as well as of even/odd data, i.e. the discrete cosine/sine transforms or DCT/DST).</p> <p>On NCAR systems single-node and distributed-memory parallel implementations of FFTW are available through the <code>fftw</code> and <code>fftw-mpi</code> modules, respectively. (See here for more discussion on interacting with the module system.)</p>"},{"location":"environment-and-software/benchmarks/","title":"Benchmarks","text":"<p>Ensuring that real applications perform well on NCAR computing platforms is critical for getting the best value out of complex and costly high-performance computing and storage resources. Climate and weather applications are large, often with millions of lines of code, and are generally difficult to configure in a way that permits ease of use for things such as system deployments, upgrades, and procurements.</p> <p>NCAR has developed a suite of application kernels, micro-benchmarks, and full applications with moderate input cases that can be used as proxies for the full applications and still provide meaningful information and insights into system performance. A few of these are well-known benchmarks that are commonly used in HPC for characterizing system performance.</p> <p>NCAR's benchmarking application packages include source files, build scripts, and input data sets required to compile and run the applications. In cases where the benchmarks depend on applications and libraries that are not part of the package distributions, version number and download details are provided in the README files.</p>"},{"location":"environment-and-software/benchmarks/#releases","title":"Releases","text":"<p>2019/2020 Release</p> <p>2015 Release</p>"},{"location":"environment-and-software/benchmarks/ncar-benchmarking-2015/","title":"NCAR benchmarking applications- 2015 release","text":"<p>Ensuring that real applications perform well on NCAR computing platforms is critical for getting the best value out of complex and costly high-performance computing and storage resources. Climate and weather applications are large, often with millions of lines of code, and are generally difficult to configure in a way that permits ease of use for things such as system deployments, upgrades, and procurements.</p> <p>Thus, NCAR has developed a suite of application kernels, micro-benchmarks, and full applications with moderate input cases that can be used as proxies for the full applications and still provide meaningful information and insights into system performance. A few of these are well-known benchmarks that are commonly used in HPC for characterizing system performance.</p> <p>NCAR's benchmarking applications are listed in the tables below, along with file names, sizes, and checksums. These packages include source files, build scripts, and input data sets required to compile and run the applications. In cases where the benchmarks depend on applications and libraries that are not part of the package distributions, you will find version number and download details in the README files.</p> <p>Release Date: February 2, 2015</p> <p>Last Updated: August 29, 2018</p> <p>The benchmark download packages are available through the Globus-based NCAR Data Sharing Service. Instructions are given below for obtaining a Globus account, installing the required Globus software, and downloading the benchmark packages via the NCAR HPC Benchmarks endpoint. See the Globus instructions below for more information.</p> <p>Instructions for all benchmarks are available via Google Docs or direct download at this link:</p>"},{"location":"environment-and-software/benchmarks/ncar-benchmarking-2015/#application-benchmark-instructions","title":"Application Benchmark Instructions","text":"<p>These are the instructions for each of the application benchmarks in the table below.</p> Application Benchmarks Description File Size (Bytes) MD5 Checksum HOMME HOMME benchmark and HOMME_COMM communication kernel HOMME_v1.tar.gz 2728264 b35d135f52b488d0bf9c1a07f2d02a93 HPCG High Performance Conjugate Gradient Solver hpcg-2.4_v1.tar.gz 69974 fef8b6614ddaf3c45b8dd1b8fb867df7 LES Large Eddy Simulation benchmark LES_v1.tar.gz 73200 f9017e36b1ea0f02a2169770b37fad54 MG2 Morrison Gettelman cloud microphysics kernel MG2kernel_v1.tar.gz 282822 53befeb7e418c074c80f6a5ad025144c MPAS-A MPAS Atmosphere benchmark MPAS_3.2_v1.tar.gz 2259609261 e9736920454952afb7e13c2e4f859457 POPperf POP Ocean model benchmark POPperf_v1.tar.gz 66480926 0fd078478dc6b5f326701ac09713fa49 WRF Weather Research and Forecasting model WRFV3_BENCH_v1.tar.gz 13260795166 4d5a7c02656dca8042cebe1e656c793b CESM Community Earth System Model Used in numerical correctness and system acceptance testing. http://www.cesm.ucar.edu/ I/O and Microbenchmarks Description File Size (Bytes) MD5 Checksum STREAM Node level memory benchmark STREAM_v1.tar.gz 10268 ee520d700a1fef3f746b9a8117952635 SHOC Scalable HeterOgeneous Computing (GPU benchmark) shoc_v1.tar.gz 10418387 f3a4146180cb720a04104ee40bd161ea OSU-MPI MPI communication benchmarks osu-micro-benchmarks-4.4.1_v1.tar.gz 151586 4bae164fc0aecd955adae1e9a9dc48d9 IOR I/O bandwidth and latency test ncar_ior_v1.tgz 144,608 dc91a37af717005c87ec1752524ef67b pyReshaper Application I/O kernel pyResBench_v1.tgz 1,938,268,372 67c9231e8bacb644d1a952b8793dc609 mdtest Metadata performance test ncar_mdtest_v1.tgz 93,074 c9f69c6cdc335409f96ebce7764babad"},{"location":"environment-and-software/benchmarks/ncar-benchmarking-2015/#globus-instructions","title":"Globus instructions","text":""},{"location":"environment-and-software/benchmarks/ncar-benchmarking-2015/#step-1-obtain-a-globus-account","title":"Step 1: Obtain a Globus account","text":"<p>Go to www.globus.org and click the Sign Up button in the upper-right corner.</p>"},{"location":"environment-and-software/benchmarks/ncar-benchmarking-2015/#step-2-install-globus-connect-personal","title":"Step 2: Install Globus Connect Personal","text":"<p>Go to www.globus.org. Under Products select Globus Connect and then Get Globus Connect Personal. Versions are available for Mac OS X, Linux and Windows.</p>"},{"location":"environment-and-software/benchmarks/ncar-benchmarking-2015/#step-3-use-globus-to-download-benchmarks","title":"Step 3: Use Globus to download benchmarks","text":"<ol> <li> <p>Log in at www.globus.org with your Globus     account</p> </li> <li> <p>Select Transfer Files</p> </li> <li> <p>In the left-hand window, enter NCAR HPC Benchmarks as the     endpoint</p> </li> <li> <p>In the right-hand window, enter an endpoint at your own site or the     endpoint that you established with Globus Connect</p> </li> <li> <p>Select the benchmark files you wish to download and click the     right-hand arrow</p> </li> </ol> <p>Your download will be submitted through the Globus Transfer service. You will receive an email when your transfer has completed or you can monitor from the Transfer window by selecting refresh list in the right-hand window.</p>"},{"location":"environment-and-software/benchmarks/ncar-benchmarking-2019-2020/","title":"NCAR benchmarking applications - 2019-2020 release","text":"<p>31 March 2020 - Benchmark Q&amp;As updated</p> <p>For questions and answers regarding the NWSC-3 HPC Benchmarks, refer to the updated NWSC-3 Benchmarks Q&amp;As document.</p> <p>19 March 2020 - Updated benchmarks released</p> <p>Please note that the NWSC-3 HPC Benchmarks have been updated to include changes to the GOES and OSU MPI benchmarks. Prospective Offerors for the NWSC-3 Request for Proposal (RFP), which will be released 2 April 2020, should download the updated benchmark code, input cases, and instructions (see below). NCAR does not plan to make any additional changes to the HPC Benchmarks unless there are issues with the ones provided here.</p> <p>19 July 2019 - Benchmarks released</p> <p>The NWSC-3 HPC Benchmarks are available ahead of an anticipated release of the NWSC-3 Request for Proposal (RFP) in Q1 of 2020. NCAR does not plan to make any changes to the HPC Benchmarks unless there are issues with the ones provided here.</p> <p>Release Date: July 19, 2019</p> <p>Last Updated: March 19, 2020</p> <p>Ensuring that real applications perform well on NCAR computing platforms is critical for getting the best value out of complex and costly high-performance computing and storage resources. Climate and weather applications are large, often with millions of lines of code, and are generally difficult to configure in a way that permits ease of use for things such as system deployments, upgrades, and procurements.</p> <p>Thus, NCAR has developed a suite of application kernels, micro-benchmarks, and full applications with moderate input cases that can be used as proxies for the full applications and still provide meaningful information and insights into system performance. A few of these are well-known benchmarks that are commonly used in HPC for characterizing system performance.</p> <p>NCAR's benchmarking applications are listed in the table below, along with file names, sizes, and checksums. These packages include source files, build scripts, and input data sets required to compile and run the applications. In cases where the benchmarks depend on applications and libraries that are not part of the package distributions, you will find version number and download details in the README files.</p> <p>Documentation and benchmark download packages are available through the Globus-based NCAR Data Sharing Service. Instructions are given below for obtaining a Globus account, installing the required Globus software, and downloading the benchmark packages via the NCAR HPC Benchmarks endpoint. See the Globus instructions below for more information.</p>"},{"location":"environment-and-software/benchmarks/ncar-benchmarking-2019-2020/#benchmarks","title":"Benchmarks","text":"Name Description File Contents File Name  Size           (Bytes)   MD5 Checksum CLUBB Physics Kernel Instructions CLUBB_2019-05-19.pdf 63246 006fb83f72d3cc4042c361701f837ea4 Benchmark Files CLUBB_2019-05-19.tar.gz 184931304 626d0c5108f76c662d8e722fa64cfe82 DART_WRF Model Kernel Instructions DART_WRF_2019-05-20.pdf 59749 cbe78c13a80073910c0275467c52ee2c Benchmark Files DART_WRF_2019-05-20.tar.gz 258607013 a388e961fce0960d096417cd45e1f00f GOES ML Benchmark Instructions GOES16_2020-04-27.pdf  71665 1d9ba301526d25cd768304e14fa5ab14 Benchmark Files GOES16_2020-04-27.tar.gz 3578032111 5517e495689c75c4f71478d5d3f45e7e MG2 Physics Kernel Instructions MG2_2019-05-20.pdf 61804 c13a288f425993504ab9ce5db692c008 Benchmark Files MG2_2019-05-20.tar.gz 85366943 25ebc145a374d3ccd1d410f9d495261a MPAS-A* GPU-capable Atmospheric Model Instructions MPAS_2019-06-26.pdf 274813 cf523aa8e3a9d889d11817d0d07edca9 Benchmark Files (For access, follow the instructions below) Input Data MPAS_2020-04-27_data.tar.gz 17270465681 0ced450ce164b86cb9a6c82a5dcfd966 Stream Memory Bandwidth Instructions Stream_2019-05-22.pdf 69436 577d9da38eed93d782d6a046c36d7353 Benchmark Files Stream_2019-05-22.tar.gz 20743 23d9d58f8d709553c7e409ab1b1e44cc WACCM Physics Kernel Instructions WACCM_2019-05-19.pdf 62391 6bb2d8bd1df3471ad93a17540e2c2c17 Benchmark Files WACCM_2019-05-19.tar.gz 18289529 b31c36c58f5ecbbb613caaa39b663b32 WRF Weather Research and Forecasting (WRF) Model Instructions WRF_2019-09-06.pdf 93213 2a47071030e4417d4873938333a60af9 Benchmark Files WRF_2019-09-06.tar.gz 9778202346 bec5bf5cc682b14ebb30a2da51d381ab OSU MPI MPI Communications Benchmark Instructions osu-micro-benchmarks-5.5_2020-03-12.pdf 62732 4baea167d0698973e751b75e578ac6bb Benchmark Files osu-micro-benchmarks-5.5_2020-03-12.tar.gz 765369 bcb970d5a1f3424e2c7302ff60611008"},{"location":"environment-and-software/benchmarks/ncar-benchmarking-2019-2020/#globus-instructions","title":"Globus instructions","text":""},{"location":"environment-and-software/benchmarks/ncar-benchmarking-2019-2020/#step-1-obtain-a-globus-account","title":"Step 1: Obtain a Globus account","text":"<p>Go to www.globus.org and click the Sign Up button in the upper-right corner.</p>"},{"location":"environment-and-software/benchmarks/ncar-benchmarking-2019-2020/#step-2-install-globus-connect-personal","title":"Step 2: Install Globus Connect Personal","text":"<p>Go to https://www.globus.org/globus-connect-personal and install the version of Globus Connect Personal appropriate for your computer. Versions are available for Mac OS X, Linux, and Windows.</p>"},{"location":"environment-and-software/benchmarks/ncar-benchmarking-2019-2020/#step-3-use-globus-to-download-benchmarks","title":"Step 3: Use Globus to download benchmarks","text":"<ol> <li> <p>Access the NCAR HPC Benchmarks folder on Globus. (You will need     to log in to Globus with the account created in Step 1.)</p> </li> <li> <p>Select the files you wish to download and click Transfer or     Sync to in the right-hand pane.</p> </li> <li> <p>Select the endpoint you wish to transfer files to. This can be the     computer where you installed Globus Connect Personal in Step 2, or     another Globus endpoint to which you have access.</p> </li> <li> <p>Click on the Start button below the file manager to initiate the     transfer.</p> </li> </ol> <p>Your download will be submitted through the Globus Transfer service. You will receive an email when your transfer has completed. You can monitor the transfer by clicking Activity in the left-hand menu to bring up the Globus Activity view.</p> <p>MPAS-A benchmark access</p> <p>Access to the MPAS-A benchmark code is restricted. To obtain access, follow the instructions below.</p> <p>Instructions for obtaining NWSC-3 MPAS-A benchmark source code</p> <p>Code releases for the MPAS-A GPU project will occur through the open-source GitHub site. However, before you may access the site, you are required to sign the MPAS-A Confidentiality Agreement. To obtain access to the MPAS-A GPU GitHub site, send both your signed MPAS-A Confidentiality Agreement and your GitHub account/login to Alison Propes, UCAR's Subcontract/Procurement Manager.</p> <p>Note that all materials (including source code, products derived from source code, and documents) related to NWSC-3 MPAS should not be distributed, either formally or informally, in any form. Publishing any kind of results obtained from the NWSC-3 MPAS-A source code requires written consent from UCAR.</p>"},{"location":"environment-and-software/hpc-software/","title":"Software for HPC users","text":"<p>NCAR and CISL have developed and support a number of freely available software packages for visualization, data analysis, weather prediction, and high-performance computation. These include models that help researchers understand the impact of regional and global climate change, and tools and libraries for analyzing and visualizing\u00a0data.</p> <p>Software available on the HPC and data analysis systems that CISL operates also includes\u00a0open-source and commercial third-party products used for programming, analysis, and file management tasks.</p> <p></p> <p>Image from the CISL Visualization Gallery</p>"},{"location":"environment-and-software/hpc-software/#full-software-list","title":"Full Software List","text":"DerechoCasper <pre><code>----------------------------------------------------------------------------\nThe following is a list of the modules and extensions currently available:\n----------------------------------------------------------------------------\n  apptainer: apptainer/1.1.7, apptainer/1.1.9\n  arm-forge: arm-forge/22.1.3\n  atp: atp/3.14.18, atp/3.15.0\n  cce: cce/15.0.1\n  cdo: cdo/2.1.1, cdo/2.2.2\n  charliecloud: charliecloud/0.32, charliecloud/0.33\n  cmake: cmake/3.26.3\n  conda: conda/latest\n  cp2k: cp2k/2023.2\n  cray-ccdb: cray-ccdb/4.12.13\n  cray-dyninst: cray-dyninst/12.1.1\n  cray-libsci: cray-libsci/23.02.1.1\n  cray-mpich: cray-mpich/8.1.25\n  cray-mrnet: cray-mrnet/5.0.4\n  cray-stat: cray-stat/4.11.13\n  craype: craype/2.7.20\n  cuda: cuda/11.7.1, cuda/11.8.0, cuda/12.2.1\n  cudnn: cudnn/8.5.0.96-11.7, cudnn/8.7.0.84-11.8\n  cutensor: cutensor/1.7.0.1\n  darshan-runtime: darshan-runtime/3.4.2\n  darshan-util: darshan-util/3.4.2\n  eccodes: eccodes/2.25.0\n  ecflow: ecflow/5.8.3\n  esmf: esmf/8.4.2, esmf/8.5.0\n  fftw: fftw/3.3.10\n  fftw-mpi: fftw-mpi/3.3.10\n  gcc: gcc/12.2.0\n  gdal: gdal/3.6.4, gdal/3.7.1\n  gdb4hpc: gdb4hpc/4.14.7\n  geos: geos/3.9.1\n  go: go/1.20.3, go/1.20.6\n  gptl: gptl/8.1.1\n  grads: grads/2.2.1, grads/2.2.3\n  hdf: hdf/4.2.15\n  hdf5: hdf5/1.12.2\n  hdf5-mpi: hdf5-mpi/1.12.2\n  idl: idl/8.9.0\n  intel: intel/2023.0.0, intel/2023.2.1\n  intel-classic: intel-classic/2023.0.0, intel-classic/2023.2.1\n  intel-mpi: intel-mpi/2021.8.0, intel-mpi/2021.10.0\n  intel-oneapi: intel-oneapi/2023.0.0, intel-oneapi/2023.2.1\n  julia: julia/1.9.2\n  linaro-forge: linaro-forge/23.0\n  matlab: matlab/R2023a\n  mkl: mkl/2023.0.0, mkl/2023.2.0\n  mpi-serial: mpi-serial/2.3.0\n  mpifileutils: mpifileutils/0.11.1\n  mvapich: mvapich/3.0b\n  ncarcompilers: ncarcompilers/1.0.0\n  ncarenv: ncarenv/23.06, ncarenv/23.09\n  nccmp: nccmp/1.9.0.1, nccmp/1.9.1.0\n  ncl: ncl/6.6.2\n  nco: nco/5.1.4, nco/5.1.6\n  ncview: ncview/2.1.8, ncview/2.1.9\n  ncvis: ncvis/2022.08.28\n  netcdf: netcdf/4.9.2\n  netcdf-mpi: netcdf-mpi/4.9.2\n  nvhpc: nvhpc/21.3, nvhpc/23.1, nvhpc/23.5, nvhpc/23.7\n  osu-micro-benchmarks: osu-micro-benchmarks/7.1-1, ...\n  papi: papi/7.0.0.1\n  parallel-netcdf: parallel-netcdf/1.12.3\n  parallelio: parallelio/1.10.1, parallelio/2.5.10, parallelio/2.6.0, ...\n  pcre: pcre/8.45\n  peak-memusage: peak-memusage/3.0.1\n  perftools: perftools\n  perftools-base: perftools-base/23.03.0\n  perftools-lite: perftools-lite\n  perftools-lite-events: perftools-lite-events\n  perftools-lite-gpu: perftools-lite-gpu\n  perftools-lite-hbm: perftools-lite-hbm\n  perftools-lite-loops: perftools-lite-loops\n  perftools-preload: perftools-preload\n  perl: perl/5.36.0, perl/5.38.0\n  podman: podman/4.3.1, podman/4.5.1\n  proj: proj/8.2.1\n  rstudio: rstudio/2023.09.0\n  sanitizers4hpc: sanitizers4hpc/1.0.4\n  superlu: superlu/5.3.0\n  superlu-dist: superlu-dist/8.1.2\n  texlive: texlive/20220321\n  udunits: udunits/2.2.28\n  valgrind4hpc: valgrind4hpc/2.12.11\n  vtune: vtune/2023.0.0, vtune/2023.2.0\n----------------------------------------------------------------------------\nTo learn more about a package execute:\n   $ module spider Foo\nwhere \"Foo\" is the name of a module.\nTo find detailed information about a particular package you\nmust specify the version if there is more than one version:\n   $ module spider Foo/11.1\n----------------------------------------------------------------------------\n</code></pre> <pre><code>----------------------------------------------------------------------------\nThe following is a list of the modules and extensions currently available:\n----------------------------------------------------------------------------\n  adios2: adios2/2.9.1\n  apptainer: apptainer/1.1.9\n  cdo: cdo/2.2.2\n  charliecloud: charliecloud/0.33\n  clang: clang/16.0.6\n  cmake: cmake/3.26.3\n  conda: conda/latest\n  cuda: cuda/11.8.0\n  cudnn: cudnn/8.7.0.84-11.8\n  darshan-runtime: darshan-runtime/3.4.2\n  darshan-util: darshan-util/3.4.2\n  doxygen: doxygen/1.8.20\n  eccodes: eccodes/2.25.0\n  eigen: eigen/3.4.0\n  esmf: esmf/8.5.0\n  fftw: fftw/3.3.10\n  fftw-mpi: fftw-mpi/3.3.10\n  gcc: gcc/12.2.0\n  gdal: gdal/3.7.1\n  geos: geos/3.9.1\n  go: go/1.20.6\n  grads: grads/2.2.3\n  grib-util: grib-util/1.2.4\n  hdf: hdf/4.2.15\n  hdf5: hdf5/1.12.2\n  hdf5-mpi: hdf5-mpi/1.12.2\n  intel: intel/2023.2.1\n  intel-classic: intel-classic/2023.2.1\n  intel-oneapi: intel-oneapi/2023.2.1\n  julia: julia/1.9.2\n  linaro-forge: linaro-forge/23.0\n  mkl: mkl/2023.2.0\n  mpi-serial: mpi-serial/2.3.0\n  ncarcompilers: ncarcompilers/1.0.0\n  ncarenv: ncarenv/23.09\n  nccmp: nccmp/1.9.1.0\n  ncl: ncl/6.6.2\n  nco: nco/5.1.6\n  ncview: ncview/2.1.9\n  ncvis: ncvis/2022.08.28\n  netcdf: netcdf/4.9.2\n  netcdf-mpi: netcdf-mpi/4.9.2\n  nvhpc: nvhpc/23.7\n  octave: octave/8.2.0\n  openblas: openblas/0.3.23\n  openmpi: openmpi/4.1.5\n  parallel-netcdf: parallel-netcdf/1.12.3\n  parallelio: parallelio/2.6.1, parallelio/2.6.2\n  paraview: paraview/5.11.1\n  pcre: pcre/8.45\n  peak-memusage: peak-memusage/3.0.1\n  perl: perl/5.38.0\n  pocl: pocl/3.0\n  podman: podman/4.5.1\n  proj: proj/8.2.1\n  texlive: texlive/20220321\n  ucx: ucx/1.14.1\n  udunits: udunits/2.2.28\n  vapor: vapor/3.9.0\n  vexcl: vexcl/1.4.3\n  visit: visit/3.3.3\n  vtune: vtune/2023.2.0\n  wgrib2: wgrib2/3.1.1\n  xxdiff: xxdiff/latest\n----------------------------------------------------------------------------\nTo learn more about a package execute:\n   $ module spider Foo\nwhere \"Foo\" is the name of a module.\nTo find detailed information about a particular package you\nmust specify the version if there is more than one version:\n   $ module spider Foo/11.1\n----------------------------------------------------------------------------\n</code></pre> <p>For information about a system's unique user environment, consult the documentation for that resource at these links:</p> <ul> <li> <p>Derecho documentation</p> </li> <li> <p>Casper documentation</p> </li> </ul> <p>Also, see these pages for information regarding software packages and tools that are available for use on our systems:</p> <ul> <li> <p>Community models</p> </li> <li> <p>Data analysis and visualization</p> </li> <li> <p>Environment modules</p> </li> <li> <p>Tools for debugging, profiling, and optimizing code</p> </li> <li> <p>Utilities</p> </li> </ul>"},{"location":"environment-and-software/hpc-software/profiling-and-debuggers/running-ddt-map-and-pr-jobs/","title":"Debugging and profiling with Forge tools","text":"<p>The Linaro Forge tools \u2013 which include DDT, MAP, and Performance Reports \u2013 are provided for debugging, profiling, and optimizing code. Forge tools can be used with Fortran, C, C++, and Python code, and both CPU and GPU code can be analyzed.</p> <p>While Forge can be used with both serial and parallel applications, its real strength is in profiling large MPI codes that span many nodes \u2013 a task that is very challenging with traditional debugging and profiling tools like gdb and gprof.</p> <p>To get started, first configure the client interface on your local machine by following the recommended procedures below. This will allow you to begin debugging and profiling jobs.</p> <p>Note</p> <p>These tools were formerly known as Arm Forge (and Allinea Forge before that). Modules for versions before 23.0 will be found under the old <code>arm-forge</code> name.</p>"},{"location":"environment-and-software/hpc-software/profiling-and-debuggers/running-ddt-map-and-pr-jobs/#preparing-your-code","title":"Preparing your code","text":"<p>For compiled codes (e.g., Fortran/C/C++), you will need to add debug symbols to the binary to allow DDT and MAP to sample the program during execution. However, you do not need to add these symbols when using Performance Reports.</p> <ul> <li> <p>CPU code: Use the <code>-g</code> option when you compile your code before debugging or profiling.</p> </li> <li> <p>CUDA code: Include both the <code>-g</code> and <code>-G</code> options for the NVIDIA compilers to debug GPU code.</p> </li> </ul> <p>Do not move or remove the source code, binary, or executable files from the directory or directories in which you compiled them.</p>"},{"location":"environment-and-software/hpc-software/profiling-and-debuggers/running-ddt-map-and-pr-jobs/#preparing-applications-that-use-cray-mpich-mpi-for-profiling","title":"Preparing applications that use Cray-MPICH MPI for profiling","text":"<p>A limitation currently exists in Cray's Common Tools Interface which prevents preloading of the MPI sampling library at profiling time. This limitation means that any profiling done on codes that use Cray-MPICH MPI with either MAP or Performance Reports will require manual creation of the sampler library and linking of this library into your application executable. For example, let's suppose you wish to sample a simple MPI model called <code>cfd.exe</code>. Here is how you would need to link the executable: <pre><code># First, create the sampler library\nmodule load linaro-forge\nmake-profiler-libraries --platform=cray --lib-type=shared\n\n# Now, link sampler library into executable\nmpicc -L$(pwd) -lmap-sampler-pmpi -lmap-sampler -Wl,--eh-frame-hdr -Wl,-rpath=$(pwd) -o cfd.exe main.o driver.o physics.o\n</code></pre></p> <p>These extra linking steps can be challenging to incorporate into some custom build systems. In this case, you can leverage flags supported by the <code>ncarcompilers</code> wrapper to avoid injecting the necessary flags manually. For example: <pre><code># First, create the sampler library\nmodule load linaro-forge ncarcompilers\nmake-profiler-libraries --platform=cray --lib-type=shared\n\n# While building the application, link the sampler using the NCAR wrapper (bash syntax)\nNCAR_LDFLAGS_FORGE=$(pwd) NCAR_LIBS_FORGE=\"-lmap-sampler-pmpi -lmap-sampler -Wl,--eh-frame-hdr\" ./build.sh\n</code></pre></p>"},{"location":"environment-and-software/hpc-software/profiling-and-debuggers/running-ddt-map-and-pr-jobs/#using-ddt-or-map-via-a-remote-connection","title":"Using DDT or MAP via a Remote Connection","text":"<p>Both DDT and MAP feature complex graphical user interfaces (GUIs) that perform best when run on your local machine. To support this workflow, these tools provide a \"remote-connect\" option that allows your application to run on the remote system (e.g., Derecho) while the GUI runs on your workstation. (The tools also run from your command line interface).</p>"},{"location":"environment-and-software/hpc-software/profiling-and-debuggers/running-ddt-map-and-pr-jobs/#client-interface-setup","title":"Client interface setup","text":"<p>The client software version that you use locally and the server version that you use on Derecho must be the same. We recommend using the latest version available. Run <code>module avail linaro-forge</code> to identify the latest version.</p>"},{"location":"environment-and-software/hpc-software/profiling-and-debuggers/running-ddt-map-and-pr-jobs/#procedure","title":"Procedure","text":"<p>Download the client software from the Linaro site.</p> <p>Install and start the client on your local machine.</p> <p>From the \"Remote Launch\" menu (see image), select Configure.</p> <p></p> <p>Configure as shown in the following image. The configuration will apply to both DDT and MAP, so you only need to do it once.</p> <p>Enter your username followed by @ and the connection name (<code>derecho.hpc.ucar.edu</code>, for example) in the \u201cHost Name\u201d field.</p> <p>Then, fill in the \u201cRemote Installation Directory\u201d field. Once you have loaded the <code>linaro-forge</code> module, you can get the installation directory by echoing the following variable: <pre><code>echo $NCAR_ROOT_LINARO_FORGE\n</code></pre></p> <p>Click OK.</p> <p></p>"},{"location":"environment-and-software/hpc-software/profiling-and-debuggers/running-ddt-map-and-pr-jobs/#running-a-script","title":"Running a script","text":"<p>Prepare a job script. Specify the \"main\" submission queue on Derecho and customize the script with your own project code, job name, and so on.</p> <p>On the last line of your script, use <code>ddt --connect</code> (or <code>map --connect</code>) instead of <code>mpiexec</code>. <pre><code>ddt --connect ./my_executable\n</code></pre></p> <p>Submit your job when indicated below.</p>"},{"location":"environment-and-software/hpc-software/profiling-and-debuggers/running-ddt-map-and-pr-jobs/#procedure_1","title":"Procedure","text":"<p>Start the client interface on your local machine.</p> <p>From the \"Remote Launch\" menu, select the name of the host configuration you created in the previous step.</p> <p>When the following dialog box appears, authenticate as usual. (It may be necessary to click Show Terminal to see the authentication window).</p> <p></p> <p>After you log in, return to your normal terminal window and load the modules you need. (We recommend including <code>module load</code> commands in your job scripts). <pre><code>module load linaro-forge/23.0\n</code></pre></p> <p>Debugging GPU Jobs</p> <p>To debug GPU jobs, you may need to set the environment variable <code>CUDA_DEBUGGER_SOFTWARE_PREEMPTION</code> to 1. <pre><code>export CUDA_DEBUGGER_SOFTWARE_PREEMPTION=1\n</code></pre></p> <p>Submit your job script on your command line as in this example: <pre><code>qsub my-debug-script.bash\n</code></pre></p> <p>When your job starts, the GUI will show that a \u201cReverse Connect Request\u201d has been made. Accept the request to continue.</p> <p></p> <p>A \u201cRun\u201d window will open and display settings imported from your job script. Review the settings. If your program uses Cray MPICH, make sure the MPI is specified as Cray PALS where shown in the following image.</p> <p></p> <p>After reviewing the settings, click Run and the DDT or MAP window will open.</p> <p>Quit when you\u2019re finished so the license is available to other users.</p>"},{"location":"environment-and-software/hpc-software/profiling-and-debuggers/running-ddt-map-and-pr-jobs/#performance-reports","title":"Performance Reports","text":"<p>Performance Reports is another profiling tool provided by Linaro Forge. It summarizes the performance, memory usage, I/O, and more of application runs.</p> <p>To generate a performance report, submit a batch job which runs your application prepended by the <code>perf-report</code> command (there is no remote connection mode). You do not need to compile your application with the <code>-g</code> debug option first.</p> <p>Modify your batch script to load the <code>linaro-forge</code> module that you want to use and include <code>perf-report</code> as shown in the sample scripts below.</p> <p>When your job runs, the output will include both text and HTML report files.</p> <p>For additional information, see the Linaro Forge product documentation.</p>"},{"location":"environment-and-software/hpc-software/profiling-and-debuggers/running-ddt-map-and-pr-jobs/#sample-bash-script","title":"Sample bash script","text":"<pre><code>#!/bin/bash\n#PBS -N pr-job\n#PBS -A project_code\n#PBS -l walltime=01:00:00\n#PBS -q main\n#PBS -j oe\n#PBS -l select=2:ncpus=128:mpiprocs=128\n\nmodule load linaro-forge/23.0\n\nexport TMPDIR=$SCRATCH/temp\nmkdir -p $TMPDIR\n\n### Run the executable\nperf-report --mpi -n 256 ./executable_name.exe\n</code></pre>"},{"location":"environment-and-software/user-environment/","title":"Overview","text":"<p>Users have the ability to customize and tailor their software environment to meet specific needs, using three different processes depending on use case:</p> <p>Modules vs. Conda - what's right for my use case?</p> <p>The two main methods for gaining access to software are through the module systsem or conda environments. Understandably, this complexity is often the source of confusion.</p> <p>In general, users should prefer the module system when leveraging performance-critical parallel computing, particularly with compiled languages and MPI.  NCAR's modules provide access to many versions of popular compiler suites, MPI implementations, and dependent software libraries, all compiled to achieve maximum performance on a particular host system CPU architecture.</p> <p>Conversely, Conda provides a convenient mechanism for creating and sharing stand-alone, consistent software environments; particularly for Python and R usage. Conda environments are commonplace in data analysis and AI/ML workflows, where software packages often rely on a precise set of specifically configured dependencies.</p> <p>Conda packages typically work well in an HPC environment, with MPI being a common notable exception:  Conda-provided MPI implementations are usually configured for generic systems, and likely do not achieve optimal performance on \"exotic\" HPC networks.  If you run into difficulties with Conda and MPI packages, reach out to consulting.</p>"},{"location":"environment-and-software/user-environment/#modules","title":"Modules","text":"<p>Software modules  helps you identify software that is available on the system and then load compatible packages. It manages complex combinations of paths, environment variables, and dependencies automatically, allowing many versions of software packages to be available across the system, built with different combinations of compilers, communications libraries, etc...</p> <p>Modules are a standard feature on most multi-user HPC systems to allow for multiple, potentially conflicting versions of compilers and software to be available to different users simultaneously.  See our Modules page for a full discussion of module usage.</p>"},{"location":"environment-and-software/user-environment/#conda","title":"Conda","text":"<p>Conda is an open source package management system and environment management system that runs on Windows, macOS, and Linux. Conda installs, runs and updates packages and their dependencies. Conda easily creates, saves, loads and switches between environments on your local computer. It was created for Python programs, but it can package and distribute software for any language.</p> <p>Conda as a package manager helps you find and install packages. If you need a package that requires a different version of Python, you do not need to switch to a different environment manager, because <code>conda</code> is also an environment manager. With just a few commands, you can set up a totally separate environment to run that different version of Python, while continuing to run your usual version of Python in your normal environment. (For more details, see the conda project documentation.)</p>"},{"location":"environment-and-software/user-environment/#conda-within-an-hpc-environment","title":"Conda within an HPC Environment","text":"<p>NCAR system users access Python via Conda environments, which are self-contained installations of Python itself, Python packages, and the software dependencies those packages rely on. We provide a common <code>conda</code> module available through the module system, and encourage all users to leverage this common installation.</p> <p>Prefer NCAR's common <code>conda</code> environment</p> <p>While it is possible for users to install conda into their own personal workspaces, we discourage this approach.</p> <p>Instead, users should investigate the common <code>conda</code> environment module available through the module system.  Using this common installation facilitates sharing environments between colleagues and is configured internally to make optimum use of the NCAR GLADE file spaces.</p>"},{"location":"environment-and-software/user-environment/#containers","title":"Containers","text":""},{"location":"environment-and-software/user-environment/conda/","title":"Using Conda","text":"<p>Updated 12/21/2022</p> <p>This documentation has been revised to include the new update schedule for the NCAR Python Library as well as information for determining when to use managed environments vs. when to create your own environments.</p> <p>NCAR system users access Python via Conda environments, which are self-contained installations of Python itself, Python packages, and the software dependencies those packages rely on. The Conda environment manager is accessible by loading an environment module as described below.</p> <p>After loading the module, you can create your own environments or use NCAR Python Library (NPL) environments, which include Python 3.9 and a wide array of pre-installed geoscience, math, and computing packages.</p>"},{"location":"environment-and-software/user-environment/conda/#python-and-conda","title":"Python and Conda","text":"<p>Python is one of the most popular languages for scientific data analysis, visualization, and machine learning. Not only is it well-suited for such applications, one of its main strengths is the availability of supported and well-made third-party libraries for many different applications. These libraries, which include a collection of Python modules and software, can be installed into a Python environment.</p> <p>While various methods and tools are available for managing Python environments, CISL recommends using Conda to manage them on NCAR systems. If Python packages are not managed carefully, the environments may suffer from incompatibilities between chosen packages and even cause problems for other applications. One of the key concerns is dependency management.</p> <p>Many Python users are already familiar with Conda, a sophisticated package manager that eases the burden of setting Python up to do scientific analysis. While it is not difficult to install Conda yourself, we provide a Conda environment module on all of our systems to let you skip that step.</p> <p>The version of Conda that the module provides is updated frequently and also includes Mamba. (Mamba is a re-implementation of Conda with a similar interface and it is typically much faster and more robust at creating environments from complex sets of requirements). Once loaded, the Conda module:</p> <ul> <li> <p>provides access to <code>conda</code> and <code>mamba</code> commands.</p> </li> <li> <p>allows you to run <code>conda activate</code> without initializing Conda in   your startup files.</p> </li> <li> <p>adds managed environments to your list of available Python   environments.</p> </li> <li> <p>sets sensible default behaviors for Conda, such as making   <code>conda-forge</code> the default package   channel,   locating your cached package downloads   in your scratch space, and locating your personal Python environments   in your work space.</p> </li> </ul> <p>System-wide and personal <code>conda</code> installations</p> <p>If you already have a personal Miniconda installation initialized in your environment, the Conda module will take precedence only when it is loaded. To use your own install instead, simply unload the module. You can also override or augment our Conda configuration settings by defining your own in a <code>~/.condarc</code> file.</p> <p>For more information on using Conda, consider exploring the official documentation. Additionally, the Conda Cheat Sheet is a helpful quick reference that covers the commands most users will need.</p>"},{"location":"environment-and-software/user-environment/conda/#accessing-python-via-conda","title":"Accessing Python via Conda","text":"<p>Once you have the Conda module loaded, you can start using Python a number of ways. It is easy and often convenient to activate one of the environments CISL provides, such as the NCAR Python Library (NPL). However, you may wish to craft an environment that contains only the packages you care about, or use specific versions of different packages. Conda makes this process relatively straightforward. Both approaches are detailed in the following sections.</p>"},{"location":"environment-and-software/user-environment/conda/#using-managed-environments","title":"Using managed environments","text":"<p>You can access a number of pre-installed Python environments by loading the Conda module. To see available environments, run the <code>list</code> subcommand: <pre><code>conda env list\n</code></pre></p> <p>The output will include both managed environments and any personal environments you have created, provided they are in Conda\u2019s search path.</p>"},{"location":"environment-and-software/user-environment/conda/#the-ncar-python-library","title":"The NCAR Python Library","text":"<p>Our primary managed environment is the NCAR Python Library (NPL), which contains a large collection of packages related to geoscience and data processing. (It does not currently contain GPU or machine learning packages.) The NPL can be accessed as follows: <pre><code>conda activate npl\n</code></pre></p> <p>Once the environment is activated, the <code>python</code> command in your shell will use the version provided by the NPL. Currently, it is version Python 3.8, which is supported by the majority of relevant packages. Once package compatibility improves in newer versions, the NPL will be updated.</p> <p>To see the full list of packages in a managed environment, run the following command: <pre><code>conda list --name npl-2022b\n</code></pre></p>"},{"location":"environment-and-software/user-environment/conda/#update-schedule-for-the-ncar-python-library","title":"Update schedule for the NCAR Python Library","text":"<p>The NPL is updated twice a year and identified with environment names such as <code>npl-2022b</code> and <code>npl-2023a</code>. New NPL versions contain the same collection of packages as the previous environments (plus any requested and approved additions), but we let the Mamba resolver find and use the latest compatible versions of each package.</p> <p>For convenience, we also provide an environment named <code>npl</code> that always points to the most recent version of the NPL. We recommend that you load a specific version instead if you want to ensure consistent behavior from each installed package.</p>"},{"location":"environment-and-software/user-environment/conda/#creating-your-own-conda-environment","title":"Creating your own Conda environment","text":"<p>There are many reasons you might prefer to install your own Conda environment rather than use one of the managed environments mentioned above. These may include:</p> <ul> <li> <p>Preferring a small collection of relevant packages</p> </li> <li> <p>Existing workflows that create environments</p> </li> <li> <p>Faster or irregular package update cadence</p> </li> <li> <p>Desiring your own specified matrix of Python and package versions</p> </li> <li> <p>Maximizing stability in a package development environment</p> </li> </ul> <p>To install your own environment using Conda:</p> <ol> <li> <p>Load our Conda module (or use your own Miniconda install)</p> </li> <li> <p>Use <code>mamba create</code> to create an     environment     from a set of requirements, as in the following example: <pre><code>mamba create -n my-env python=3.9 numpy scipy matplotlib pandas\ngeocat-comp wrf-python\n</code></pre> That command will create a new environment called <code>my-env</code> with a recent version of Python 3.9.x. (you can choose a different Python version than the NPL when creating your own environments) and a small number of useful geoscience packages (plus any dependencies that Mamba determines are needed). The environment would be located in <code>/glade/work/$USER/conda-envs/my-env</code> if you are using the CISL Conda module.</p> </li> </ol> <p>Mamba and Conda interoperability</p> <p>In general, any Conda command can be executed with Mamba instead. In practice, it is only beneficial to use Mamba when creating environments and installing packages. The Mamba developers currently advise against using it to activate environments.</p>"},{"location":"environment-and-software/user-environment/conda/#using-conda-in-batch-jobs","title":"Using Conda in batch jobs","text":"<p>The Conda module enables Python environment activation by setting a shell alias that calls initialization scripts. However, the alias does not carry over into batch jobs or other subshells. You can address this one of two ways:</p> <ol> <li> <p>Explicitly load the Conda module at the start of your batch job.     This will restore the alias in the shell environment and allow you     to activate Conda environments.</p> </li> <li> <p>Run <code>conda init</code> (or <code>conda init tcsh</code> for tcsh users) after     loading the module on a login node. That will add initialization     commands to your <code>~/.bashrc</code> or <code>~/.tcshrc</code> file. Be aware that     <code>~/.bashrc</code> is not always sourced at the start of batch jobs, so     this approach may require some trial and error to cover your     individual use case.</p> </li> </ol>"},{"location":"environment-and-software/user-environment/conda/#reproducing-conda-environments","title":"Reproducing Conda environments","text":""},{"location":"environment-and-software/user-environment/conda/#cloning-an-environment","title":"Cloning an environment","text":"<p>It is possible to clone any environment in your <code>conda env list</code> using Mamba. For example, to clone the personal environment we created above, use the following command: <pre><code>mamba create -n my-clone --clone my-env\n</code></pre></p> <p>Assuming the clone is created on the same file system (e.g., your <code>conda-envs</code>), it will use very little space as Mamba will attempt to use hard links to avoid duplication.</p>"},{"location":"environment-and-software/user-environment/conda/#using-yaml-files","title":"Using YAML files","text":"<p>Sometimes it is useful to create an environment from the environment file of another environment. This allows Mamba to resolve all package requirements at the same time, which can produce more coherent environments and avoid situations in which the dependency resolver fails to find a solution. If the cloning method described above fails or seems to be taking too long, try this approach instead. Other benefits include having a hard copy of the specifications of an environment that you can share with colleagues, install on other systems, and version track with software like Git.</p> <p>First, use Conda to produce the YAML environment file from an existing environment. This example uses the NPL: <pre><code>conda env export [--from-history] -n npl &gt; npl-environment.yml\n</code></pre></p> <p>If you use the <code>--from-history</code> option, only packages that you explicitly request will be specified in the file, letting Conda choose newer dependencies for future environments. If you do not specify that option, all packages including dependencies will be documented explicitly in the YAML file. After creating the YAML file, you can use a text editor to make additions and changes to the package list; package additions and changes will be reflected in any new environments you create from the modified YAML file.</p> <p>You can then create new environments from the YAML file using Mamba: <pre><code>mamba env create -f npl-environment.yml -n my-npl\n</code></pre></p> <p>Warning</p> <p>Large environments like the NPL can take a long time to clone or recreate using YAML files.</p>"},{"location":"environment-and-software/user-environment/conda/#using-conda-environments-in-jupyter","title":"Using Conda environments in Jupyter","text":"<p>Managed environments can be accessed easily in a JupyterLab session using the NCAR JupyterHub service. Once you initiate a JupyterHub server, the managed kernels mentioned above (e.g., <code>NPL 2022b</code>) should be visible on the main launcher page. You can use them to run both Notebooks and Consoles.</p>"},{"location":"environment-and-software/user-environment/conda/#creating-jupyter-kernels-for-your-environments","title":"Creating Jupyter kernels for your environments","text":"<p>There are two ways to make your environments accessible in a Jupyter interface like NCAR\u2019s JupyterHub. Both involve creating a kernel for the environment.</p> <p>First, install the <code>ipykernel</code> package into your environment: <pre><code>conda activate my-env\nmamba install ipykernel\n</code></pre></p> <p>At this point, your environment should show up automatically as a kernel in any Jupyter server on Cheyenne or Casper. This method is convenient and robust, and any shell settings (e.g., environment variables) needed by packages will be set upon kernel launch.</p> <p>You can also manually create a JSON-format kernel specification file in your home directory and give it a custom name using the <code>ipykernel</code> module. This approach allows you to share environments with colleagues, as they can simply activate your Conda environment on the command line and then create their own personal Jupyter kernel. With your environment activated, run the following: <pre><code>python -m ipykernel install --user --name=my-kernel\n</code></pre></p> <p>Warning</p> <p>Kernels created using this approach will not have the previously mentioned shell settings imposed by <code>conda activate</code>, which may result in some packages (e.g., <code>pyproj</code>) not functioning properly.</p>"},{"location":"environment-and-software/user-environment/customizing/","title":"Personalizing start files","text":"<p>You can personalize your NCAR high-performance computing environment by using the sample content below in your startup files \u2013 the files that create your interactive login shell. The examples provide alternative color schemes and set some commonly used aliases.</p> <p>These examples also demonstrate how to define commands to be run for interactive sessions only and commands to be run for all new shells. This distinction can be important if you have some commands in your initialization files that would be disruptive for non-interactive connections, such as when using the <code>scp</code> command (which is not interactable and therefore will not benefit from module loads or aliases) or within a batch script.</p> <p>If you use bash or ksh, edit your <code>.profile</code> file. If you use tcsh, edit your <code>.tcshrc</code> file. PBS jobs on NCAR systems initialize your job environment with those files, so do not confuse them with <code>.bashrc</code> and <code>.login</code> files, which have different purposes and are not always sourced at login time.)</p> <p>Personalizing these files is optional; the information is provided in response to users' requests.</p> <p>Keep the following in mind when writing/revising your initialization files</p> <ul> <li> <p>Consider making backup copies of your existing startup files before   creating customized files.</p> </li> <li> <p>If you source other scripts in your initialization files, be sure to   provide an absolute path to the file (or a consistent reference path   like <code>source ~/commands</code> instead of <code>source commands</code>). That way,   the script will execute correctly whether it is run from a session   within your home directory or anywhere else on the file systems.</p> </li> <li> <p>It is best to include only those commands and settings in your   initialization files that are generic to all work you perform.   Examples of these include environment variables and <code>module   load</code> commands. Any settings specific to a particular workflow, or   settings others may need to run your jobs, should be included in   relevant shell and PBS batch scripts rather than in your   initialization files.</p> </li> <li> <p>Some programs (conda, for example) insert initialization instructions   into your startup files. These instructions will be executed for most   new shells, including at the start of your PBS jobs.</p> </li> </ul> <code>~/.profile</code> for bash and ksh users<code>~/.tcshrc</code> for tcsh users <pre><code>### Settings for interactive shells only\nif [[ $- == *i* ]]; then\n    # My Personal Prompt\n    #PS1=\"\\u@\\h:\\w&gt; \"\n    # Another Colorful Prompt\n    PS1=\"\\[\\e[1;31;40m\\]\\u@\\[\\e[1;34;40m\\]\\H:\\[\\e[1;32;40m\\]\\w&gt;\\[\\e[0m\\] \"\n    export PS1\n\n    # Generic Aliases\n    alias rm='rm -i'\n    alias cp='cp -i'\n    alias mv='mv -i'\n    alias h=\"history | grep \"\n    alias ls=\"ls --color\"\n    alias vi=\"vim\"\n    alias j='qstat'\n    alias more='less'\n    alias tail='tail ---disable-inotify -f'\n    alias diffcolor='git diff --color=always --color-words'\nfi\n\n### Settings for all shells\nexport PATH=~/bin:$PATH\nexport PBS_ACCOUNT=CHEYENNE_PROJECT_CODE\nexport DAV_PROJECT=CASPER_PROJECT_CODE\n\n### Source .bashrc file to make environment more consistent\n### (optional - some users prefer this; e.g., conda users)\nif [[ -f ~/.bashrc ]]; then\n    . ~/.bashrc\nfi\n</code></pre> <pre><code>### Settings for interactive shells only\ntty &gt; /dev/null\nif ( $status == 0 ) then\n    # My Personal Prompt\n    #set prompt = \"%n@%m:%~\"\n    # Another Colorful Prompt\n    set prompt = \"%{\\033[31;40m%}%n@%{\\033[0m%}%{\\033[1;34m%}%m:%{\\033[0m%}%{\\033[32;40m%}%~&gt;%{\\033[0m%} \"\n\n    # Generic Aliases\n    alias rm 'rm -i'\n    alias cp 'cp -i'\n    alias mv 'mv -i'\n    alias h \"history | grep \"\n    alias ls \"ls --color\"\n    alias vi \"vim\"\n    alias j 'qstat'\n    alias more 'less'\n    alias diffcolor 'git diff --color=always --color-words'\nendif\n\n### Settings for all shells\nsetenv PATH ~/bin:$PATH\nsetenv PBS_ACCOUNT CHEYENNE_PROJECT_CODE\nsetenv DAV_PROJECT CASPER_PROJECT_CODE\n</code></pre>"},{"location":"environment-and-software/user-environment/modules/","title":"Modules","text":""},{"location":"environment-and-software/user-environment/modules/#overview","title":"Overview","text":"<p>The <code>module</code> utility helps you identify software that is available on the system and then load compatible packages. It manages complex combinations of paths, variables, and dependencies so you can compile and run jobs efficiently and make the most of your allocation.</p> <p>Some modules are loaded by default. To see which modules those are, run <code>module list</code> when you log in. Depending on the work you need to do, you can load additional modules or different modules, or you can create and save multiple customized environments as described below.</p> <p>Warning</p> <p>Do not use personalized start files to load environment modules; it can cause conflicts. Instead, set up any unique environments that you need as described in the customized environments section below. Use that approach to create and save different environments for various aspects of your work \u2013 one for model runs and another for data analysis, for example.</p>"},{"location":"environment-and-software/user-environment/modules/#essential-module-commands","title":"Essential module commands","text":"<p>Following are descriptions of commonly used module commands.</p> <ul> <li><code>module av</code> \u2013 Show which modules are available for use with the currently loaded compiler.  Typical output:  </li> </ul> <p>In the example above, (<code>L</code>) indicates which modules are currently loaded.  The modules deployed at NCAR are hierarchical, with a base of common compilers and core software.  The remainder of the output is dependent on what core software (namely, compilers) are chosen.</p> <ul> <li> <p><code>module help</code> \u2013 List options and subcommands for the module utility; or specify a modulefile by name for help with an individual module. <pre><code>module help\nmodule help netcdf\n</code></pre></p> </li> <li> <p><code>module list</code> \u2013 List the modules that are loaded.</p> </li> <li> <p><code>module load</code> \u2013 Load the default version of a software package, or load a specified version. <pre><code>module load modulefile_name\nmodule load modulefile_name/n.n.n\n</code></pre></p> </li> <li> <p><code>module purge</code> \u2013 Unload all modules. Some users include this command in a batch script, followed by a sequence of <code>module load</code> commands to establish a customized environment for the job being submitted.</p> </li> <li> <p><code>module reset</code> - Reset the system default modules.</p> </li> <li> <p><code>module spider</code> \u2013 List all modules that exist on the system. This does not give you information on module dependencies or tell you which modules can be loaded without conflicts at that point.</p> </li> <li> <p><code>module spider modulefile_name/n.n.n</code> \u2013 List all occurrences of <code>modulefile_name/n.n.n</code> on the system, including its module dependences. For example, to determine which compiler/MPI combinations provide <code>netcdf-mpi/4.9.2</code>: <pre><code>module spider netcdf-mpi/4.9.2\n\n------------------------------------------------------------------------------------------------------\n  netcdf-mpi: netcdf-mpi/4.9.2\n------------------------------------------------------------------------------------------------------\n\n    You will need to load all module(s) on any one of the lines below before the \"netcdf-mpi/4.9.2\"\n    module is available to load.\n\n      ncarenv/23.09  gcc/12.2.0  cray-mpich/8.1.25\n      ncarenv/23.09  gcc/12.2.0  mvapich/3.0b\n      ncarenv/23.09  intel-classic/2023.2.1  cray-mpich/8.1.25\n      ncarenv/23.09  intel-classic/2023.2.1  intel-mpi/2021.10.0\n      ncarenv/23.09  intel-oneapi/2023.2.1  cray-mpich/8.1.25\n      ncarenv/23.09  intel-oneapi/2023.2.1  intel-mpi/2021.10.0\n      ncarenv/23.09  intel/2023.2.1  cray-mpich/8.1.25\n      ncarenv/23.09  intel/2023.2.1  intel-mpi/2021.10.0\n      ncarenv/23.09  nvhpc/23.7      cray-mpich/8.1.25\n      ...\n</code></pre> Each output line is a consistent set of modules that when loaded will provide access to the requested <code>netcdf-mpi</code> package.</p> </li> <li> <p><code>module swap</code> \u2013 Unload one module and load a different one. Example: <pre><code>module swap netcdf pnetcdf\n</code></pre></p> </li> <li> <p><code>module unload</code> \u2013 Unload the specified software package. <pre><code>module unload modulefile_name\n</code></pre></p> </li> <li> <p><code>module whatis</code> \u2013 Get a short description of a module.</p> </li> </ul>"},{"location":"environment-and-software/user-environment/modules/#customized-environments","title":"Customized environments","text":"<p>If you have created your own environment or want to have multiple collections of modules for various tasks, you can save those customized environments for easy reuse.</p> <p>To save a customized environment as your default environment, load the modules that you want to use, then simply run <code>module save</code> or <code>module s</code>.</p> <pre><code>module save\n</code></pre> <p>The cluster you are using will append a suffix to the name you provide. For example, on Casper your example will include <code>.dav</code> as a suffix.</p> <p>If you plan to set up additional custom environments for other needs, give each collection of modules a unique name.</p> <pre><code>module save environment_name\n</code></pre> <p>To use one of the custom environments from that list, use <code>module restore</code>, or <code>module r</code>, followed by the name.</p> <p><pre><code>module restore environment_name\n</code></pre> To see a list of your customized, saved environments, use <code>module savelist</code>. <pre><code>module savelist\n</code></pre></p> <p>To see which modules you've saved in a custom environment, use <code>module describe</code> as shown.</p> <pre><code>module describe environment_name.xyz\n</code></pre>"},{"location":"environment-and-software/user-environment/modules/#remove-a-customized-environment","title":"Remove a customized environment","text":"<p>To remove a customized environment that you have saved:</p> <ol> <li>Change to your <code>.lmod.d</code> directory.</li> <li>List the files.</li> <li>Use <code>rm</code> to delete what you no longer need.</li> </ol> <pre><code>cd /glade/u/home/username/.lmod.d\nls -l\nrm environment_name.xyz\n</code></pre>"},{"location":"environment-and-software/user-environment/modules/#revise-a-customized-environment","title":"Revise a customized environment","text":"<p>To revise a customized environment:</p> <ol> <li>Restore (change to) that environment.</li> <li>Unload, load, or swap modules as needed.</li> <li>Save the environment as a default environment again with the same name.</li> </ol> <pre><code>module restore myenvironment\nmodule load additional_module\nmodule save myenvironment\n</code></pre> <p>The previously saved environment will be renamed automatically with the addition of a tilde (~). In the example just above, the previously saved environment would be renamed to <code>myenvironment~</code>.</p> <p>Troubleshooting tips</p> <p>Situation: You load a custom default module collection (for example, <code>module restore myenvironment</code>). You receive a warning similar to this: <pre><code>Lmod Warning: The following modules have changed: pgi\nLmod Warning: Please re-save this collection\n</code></pre></p> <p>What to do: \u201cRe-save\u201d the customized module collection by running <code>module save</code> and using the same environment name, as follows.</p> <pre><code>module save myenvironment\n</code></pre>"},{"location":"environment-and-software/user-environment/utilities/","title":"Utilities","text":"<p>CISL provides the tools described on this page to support users' code development efforts and to facilitate good programming and file management practices.</p>"},{"location":"environment-and-software/user-environment/utilities/#compressing-and-archiving-files","title":"Compressing and archiving files","text":"<p>Because files often contain large amounts of redundant data and empty space, you can reduce their size significantly with tools such as <code>gzip</code> and <code>bzip2</code>.</p> <p>Bzip2 provides greater compression, but it is also slower. Gzip offers a good balance of compression rate and speed. Note that <code>gzip</code> and <code>bzip2</code> are able to compress only individual files. See the tools' <code>man</code> pages for details.</p> <p>To both package and compress a group of files, use the <code>tar</code> command to create an archive file \u2013 also called a tar file or tarball \u2013 as shown in the following examples. Also use the <code>tar</code> command to extract files when needed.</p> <p>Create an archive file with <code>tar</code>: <pre><code>tar -cvf archive_name.tar file1 file2 file3\n</code></pre></p> <p>Extract files from the archive file: <pre><code>tar -xvf archive_name.tar\n</code></pre> Create a <code>bzip2</code>-compressed archive file: <pre><code>tar cjf archive_name.tar.bz2 filename\n</code></pre></p> <p>Extract files from the archive: <pre><code>tar xf archive_name.tar.bz2\n</code></pre></p> <p>Create a <code>gzip</code>-compressed archive file: <pre><code>tar czf archive_name.tgz filename\n</code></pre></p> <p>Extract files from the gzipped archive: <pre><code>tar xf archive_name.tgz\n</code></pre></p> <p>The \"<code>j</code>\" and \"<code>z</code>\" options that are included when creating files with <code>bzip2</code> or <code>gzip</code> are not needed for *extracting *files; they are used automatically. (If they are included in a script you use, there is no harm in leaving them there.)</p>"},{"location":"environment-and-software/user-environment/utilities/#editing-files","title":"Editing files","text":"<p>Among the most widely used text editors are Vim, Vi and Emacs.</p> <p>Several excellent tutorials are available for text editors, including the following:</p> <ul> <li> <p>GNU Emacs   Manual \u2013 gnu.org</p> </li> <li> <p>The Emacs   Tutorial \u2013   Free Software Foundation</p> </li> <li> <p>A Tutorial Introduction to GNU   Emacs \u2013   University of Chicago Library</p> </li> </ul>"},{"location":"environment-and-software/user-environment/utilities/#making-executables","title":"Making executables","text":"<p>Using simple makefiles with the appropriate compiler is usually sufficient for making your own executables on Cheyenne.</p> <p>If you are tasked with developing cross-platform software, however, consider using the GNU Build System, also known as GNU Autotools. This is a suite of programming tools designed to assist in making source code packages portable to different UNIX-like systems. It consists of the GNU Utility programs autoconf, automake, and libtool. Man pages are available for each.</p> <p>Generating the executable usually is a two-step process using the GNU Build System:</p> <ol> <li> <p>Invoke the <code>configure</code> script, which creates a <code>makefile</code> with     some settings that are appropriate to the machine.</p> </li> <li> <p>Invoke <code>make</code>, which will use the makefile to compile the     executable, often using a complex set of rules.</p> </li> </ol> <p>GNU make (<code>gmake</code>) is the GNU incarnation of POSIX make, and as such it is more user friendly and full-featured. Usually it is used as a tool for managing compilation and builds of software packages, but it can do more. It functions consistently across platforms. See the GNU Make Manual.</p>"},{"location":"environment-and-software/user-environment/utilities/#version-control","title":"Version control","text":"<p>Git (<code>git</code>) is the most popular distributed version control system in use and is available on NCAR Systems.</p> <p>Mercurial (<code>hg</code>) is another good distributed version control system. Its functionality is similar to that of Git.</p>"},{"location":"environment-and-software/user-environment/containers/","title":"Using Containers on NCAR Systems","text":""},{"location":"environment-and-software/user-environment/containers/#introduction","title":"Introduction","text":"<p>A container is a unit of software that packages code with its required dependencies in order to run in an isolated, controlled environment. This allows you to run an application in a way that is predictable, repeatable, and without the uncertainty of inconsistent code execution across diverse development and production environments. A container will always start up and run code the same way, regardless of what machine it exists on.</p>"},{"location":"environment-and-software/user-environment/containers/#terminology","title":"Terminology","text":"<ul> <li> <p>Operating System (OS): This is the software that manages all other software on a computer, along with the hardware.</p> </li> <li> <p>Kernel: This is the core component of the OS that handles interacting with a machine\u2019s hardware. The kernel translates requests for physical resources - CPU cycles, RAM, I/O, etc... - between software processes and the hardware.</p> </li> <li> <p>Container Image: An image is a read-only template that contains all the code, dependencies, libraries, and supporting files that are required to launch a container. This is a unit of software that packages code with its required dependencies in order to run in an isolated, controlled environment. Container images virtualize an OS, but not a kernel (in contrast to heavier-weight virtual machine technology).  Images can be built manually, or retrieved from one of several Image Registries.</p> <ul> <li> <p>Image Registry: This is a solution for container image storage and sharing. Several popular examples are Docker Hub, Quay.io and GitHub's container registry.  Additionally, private image registries can be implemented.</p> </li> <li> <p>Image Repository: This is a specific location for a container image within an image registry. It contains the latest image along with the history of it within a registry.</p> </li> </ul> </li> <li> <p>Containers: A container is an instance of an image. Running a container requires a container runtime environment and a CPU instruction set architecture (e.g. <code>x86_64</code>, <code>arm64</code>) compatible with the image from which the container is instantiated. A single container image can be used to run many container instances.</p> </li> <li> <p>A Container Runtime is a software component that can run containers on a host operating system. Container runtimes are responsible for loading container images from a repository, monitoring local system resources, isolating system resources for use of a container, and managing container life cycle.  Container Engines are complete solutions for container technologies - including the container, the container runtime underlying it, the container image and the tools to build it, and potentially can include container image registries.</p> </li> </ul>"},{"location":"environment-and-software/user-environment/containers/#container-runtimes-in-an-hpc-environment","title":"Container Runtimes in an HPC Environment","text":"<p>The most poplar container runtime is Docker, and nearly all other container platforms seek to be compatible with Docker.  Unfortunately, Docker's original design required elevated privileges and therefore was incompatible with shared HPC systems from a security perspective.  (These limitations have been addressed to some extent with recent developments in \"rootless\" Docker, however it is still rare to find Docker installed on HPC systems.)  Due to Docker's ubiquity and common tooling we will discuss it later as a viable platform for local container image development, for example when running Docker on a laptop or external system.</p> <p>To address Docker's security concerns, a number of alternative runtimes better suited for HPC use have been developed.  Some notable examples include Apptainer (formerly, Singularity), Charliecloud, and Podman, which are all currently installed on NCAR's HPC resources and available through the module system.</p> Charliecloud vs. Apptainer vs. Podman - Which is right for me? <p>Unfortunately, with the current state of container technology on HPC systems it is difficult to provide a single recommended runtime.  If you are familiar with one of these tools already, by all means use it - we will strive to maintain the most popular tools that are compatible with our security requirements.</p> <p>For users beginning to work with containers, here are some general considerations:</p> <ul> <li> <p>Apptainer and the Singularity family of tools that precedes it was created from the outset to run complex applications on HPC clusters in a simple, portable, and reproducible way. It is well documented, stable, and mature.  Probably the most significant drawback to Apptainer is building images, as it relies on its own recipe definition file format - known as \"<code>def</code> files\" -  somewhat limiting build interoperability with other tools, including Docker.  If you have an existing Singularity <code>def</code> file, use Apptainer.  If you have relatively complex Docker build recipes (e.g. <code>Dockerfiles</code>) or want easy interoperability with other tools in the build stage, you may consider one of the other alternatives.</p> </li> <li> <p>Charliecloud similarly runs containers with no privileged operations or daemons and minimal configuration changes on HPC center resources, and is also designed for HPC from the outset.  Charliecloud's build steps are largely compatible with <code>Dockerfile</code> definition recipes, making it a good choice for users already invested in the Docker build process.  By default Charliecloud stores container images in a directory tree, which can be performance limiting for large scale jobs on parallel file systems (see this paper for more information), however this limitation is largely overcome when using compressed SqushFUSE image files.  Perhaps the one drawback to Charliecloud is that it appears less widely deployed across HPC sites that Apptainer, with a smaller community support base.</p> </li> <li> <p>Podman is envisioned as a complete replacement for Docker, which has benefits and drawbacks in an HPC environment.  If you have a complex Docker workflow, Podman might be a drop-in replacement.  The caveat is that some features supported in Podman are not implemented in an HPC environment due to security concerns, such as switching user IDs within a <code>Dockerfile</code>, etc...  Podman has widespread industry support beyond just the HPC community.  One downside to Podman is at present containers are straightforward to install at an individual level, but somewhat difficult to share within a group of users, as Podman does not easily support running containers from single compressed image files, limiting scalability when used under MPI.</p> </li> </ul> <p>We cannot support all use cases for each runtime</p> <p>NCAR CISL staff will attempt to support containerized workflows using at least one of the tools mentioned above, and may recommend one tool over others given a specific use case.  Due to both technical and personnel resource limitations we cannot force-fit any particular tool simply based on a users' personal preference.</p>"},{"location":"environment-and-software/user-environment/containers/#use-cases","title":"Use Cases","text":"<p>Based on feedback from NCAR HPC users, some of the more common use cases envisioned for containers on our HPC systems are:</p> <ol> <li> <p>To insulate a users' application from changes in the host environment,</p> </li> <li> <p>To run applications developed elsewhere, particularly when difficult to install directly, and</p> </li> <li> <p>To run legacy applications that are difficult to build with a modern software stack.</p> </li> </ol> <p>Additionally, users expressed a desire to:</p> <ol> <li> <p>Deploy in-house developed applications within a container to make it easier for others to install,</p> </li> <li> <p>Offer trainings outside of NCAR resources in a controlled environment, and</p> </li> <li> <p>Use cloud-based systems, but with a NCAR-HPC \"look and feel.\"</p> </li> </ol> <p>To support these needs users must:</p> <ul> <li> <p>Be able to run modeling codes across many nodes with MPI (with or without GPUs), or</p> </li> <li> <p>Run specialized containers within a single node as part of an analysis workflow.</p> </li> </ul> <p>We address all of these use cases in our discussion of working with containers.  Additionally we provide a collection of containerized workflow examples building upon this reference foundation.</p>"},{"location":"environment-and-software/user-environment/containers/#references","title":"References","text":"<ol> <li>Containers@TACC</li> <li>DigitalOcean's Introduction to Containers</li> <li>Open Container Initiative</li> </ol>"},{"location":"environment-and-software/user-environment/containers/examples/","title":"Sample Containerized Workflows","text":"<p>Warning</p> <p>This page contains a sample of containerized workflows that demonstrate various techniques built up in practice, often from resolving user issues. We do not necessarily endorse or support each use case, rather these examples are provided in hopes they may be useful to demonstrate (i) sample containerized workflows, and (ii) solutions to various problems you may encounter.</p>"},{"location":"environment-and-software/user-environment/containers/examples/#nvidias-ngc-containers","title":"NVIDIA's NGC containers","text":"<p>NVIDIA's NGC is a catalog of software optimized for GPUs. NGC containers allow you to run data science projects \"out of the box\" without installing, configuring, or integrating the infrastructure.</p>"},{"location":"environment-and-software/user-environment/containers/examples/#nvidias-modulus-physics-ml-framework","title":"NVIDIA's Modulus physics-ML framework","text":"<p>NVIDIA Modulus  is an open source deep-learning framework for building, training, and fine-tuning deep learning models using state-of-the-art Physics-ML methods. NVIDIA provides a frequently updated Docker image with a containerized PyTorch installation that can be run under Apptainer, albeit with some effort.  Because the container is designed for Docker, some additional steps are required as discussed below.</p> Running containerized NVIDIA-Modulus on a single Casper GPU <ol> <li> <p>Rather than pull the container and run as-is, we will create a derived container that allows us to encapsulate our desired changes. The primary reason for this is the Modulus container assumes the container is writable and makes changes during execution.   Since we will run under Apptainer using a compressed, read-only image, this fails. Therefore we will make our own derived image and make the requisite changes during the build process.</p> <p>This is accomplished first by creating a simple Apptainer definition file: my_modulus.def<pre><code>From: nvcr.io/nvidia/modulus/modulus:23.09\n\n%post\n    # update pip\n    python -m pip install --upgrade pip\n\n    # use pip to install additional packages needed for examples later\n    pip install warp-lang\n\n    # Remove cuda compat layer (https://github.com/NVIDIA/nvidia-docker/issues/1256)\n    # note that the source container attempts to do this at run-time, but that will\n    # fail when launched read-only.  So we do that here instead.\n    # (This issue will likely be resolved with newer versions of nvidia-modulus)\n    rm -rf /usr/local/cuda/compat/lib\n</code></pre> The definition file begins by pulling a specified version of the Modulus container, then modifying it in our <code>%post</code> step.  In <code>%post</code> we update the <code>pip</code> Python package installer, use <code>pip</code> to install some additional Python packages not in the base image but required for the examples run later, and finally removes a conflicting path from the source image.</p> <p>Using the <code>my_modulus.def</code> file we now create our derived container and store it as a SIF: <pre><code>module load apptainer\nTMPDIR=/var/tmp/ singularity build my_modulus.sif my_modulus.def\n</code></pre> Note in this step we have explicitly set <code>TMPDIR</code> to a local file system, as occasionally containers fail to build on the large parallel file systems usually used for <code>TMPDIR</code> within NCAR.  (The failure symptoms are usually fatal error messages related to <code>xattrs</code>.)</p> </li> <li> <p>Fetch some examples so we can test our installation:    <pre><code>git clone https://github.com/NVIDIA/modulus.git\n</code></pre></p> </li> <li> <p>Run the container in an interactive session on a single Casper GPU. We will launch an interactive session, then run the container interactively with the <code>singularity shell</code> command.    <pre><code># Interactive PBS submission from a login node:\nqsub -I -A &lt;ACCOUNT&gt; -q casper -l select=1:ncpus=4:mpiprocs=4:ngpus=1 -l gpu_type=v100 -l walltime=1:00:00\n\n# Then on the GPU node:\nmodule load apptainer\nsingularity shell \\\n            --nv --cleanenv \\\n            --bind /glade/work \\\n            --bind /glade/campaign \\\n            --bind /glade/derecho/scratch \\\n            ./my_modulus.sif\n</code></pre>    Note the command line arguments to <code>singularity shell</code>:</p> <ul> <li><code>--nv</code>: enable Nvidia support,</li> <li><code>--cleanenv</code>: clean environment before running container, causing the container to be launched with no knowledge of environment variables set on the host.  This is default behavior for Docker, and is required in this case to prevent conflicting <code>CUDA_*</code> and other environment variable settings from confusing the contanierized PyTorch.</li> <li><code>--bind /glade/work</code> etc...:  binds host file systems into the container, allowing us to read and write from GLADE.</li> </ul> </li> <li> <p>Now we are inside the container, as evidenced by the <code>Apptainer&gt;</code> command line prompt in the final step of this example.  We will run one of the sample problems checked out in step 3:    <pre><code>Apptainer&gt; cd modulus/examples/cfd/darcy_fno/\nApptainer&gt; python ./train_fno_darcy.py\nWarp 0.10.1 initialized:\n   CUDA Toolkit: 11.5, Driver: 12.3\n   Devices:\n     \"cpu\"    | x86_64\n     \"cuda:0\" | Tesla V100-SXM2-32GB (sm_70)\n   Kernel cache: /glade/u/home/benkirk/.cache/warp/0.10.1\n[21:04:13 - mlflow - WARNING] Checking MLFlow logging location is working (if this hangs its not)\n[21:04:13 - mlflow - INFO] MLFlow logging location is working\n[21:04:13 - mlflow - INFO] No Darcy_FNO experiment found, creating...\n[21:04:13 - checkpoint - WARNING] Provided checkpoint directory ./checkpoints does not exist, skipping load\n[21:04:13 - darcy_fno - WARNING] Model FourierNeuralOperator does not support AMP on GPUs, turning off\n[21:04:13 - darcy_fno - WARNING] Model FourierNeuralOperator does not support AMP on GPUs, turning off\n[21:04:13 - darcy_fno - INFO] Training started...\nModule modulus.datapipes.benchmarks.kernels.initialization load on device 'cuda:0' took 205.84 ms\nModule modulus.datapipes.benchmarks.kernels.utils load on device 'cuda:0' took 212.94 ms\nModule modulus.datapipes.benchmarks.kernels.finite_difference load on device 'cuda:0' took 670.44 ms\n[21:04:46 - train - INFO] Epoch 1 Metrics: Learning Rate =  1.000e-03, loss =  6.553e-01\n[21:04:46 - train - INFO] Epoch Execution Time:  3.241e+01s, Time/Iter:  1.013e+03ms\n[21:05:14 - train - INFO] Epoch 2 Metrics: Learning Rate =  1.000e-03, loss =  4.255e-02\n[21:05:14 - train - INFO] Epoch Execution Time:  2.812e+01s, Time/Iter:  8.786e+02ms\n[...]\n</code></pre></p> </li> </ol> <p>While this example demonstrated running the container interactively, alternatively steps 3 and 4 can be combined to be run inside a PBS batch job.</p>"},{"location":"environment-and-software/user-environment/containers/examples/#popular-aiml-tools","title":"Popular AI/ML tools","text":"<p>Optimized Tensorflow and PyTorch models are available directly from the NGC.</p> Running AI/ML tools from NGC containers <p>Building an image with Apptainer</p> <p>Anticipating that we may want to make additions to the container, we will build our own derived Apptainer image using a Definition file.</p> TensorflowPyTorch <p>ngc_tensorflow.def<pre><code>Bootstrap: docker\nFrom: nvcr.io/nvidia/tensorflow:23.11-tf2-py3\n\n%post\n    # update pip\n    python -m pip install --upgrade pip\n\n[...]\n</code></pre> <pre><code>module load apptainer\nTMPDIR=/var/tmp/ singularity build my_image.sif ngc_tensorflow.def\n</code></pre></p> <p>ngc_pytorch.def<pre><code>Bootstrap: docker\nFrom: nvcr.io/nvidia/pytorch:23.11-py3\n\n%post\n    # update pip\n    python -m pip install --upgrade pip\n\n[...]\n</code></pre> <pre><code>module load apptainer\nTMPDIR=/var/tmp/ singularity build my_image.sif ngc_pytorch.def\n</code></pre></p> <p>Run the image</p> <p><pre><code>module load apptainer\nsingularity shell \\\n            --nv --cleanenv \\\n            --bind /glade/work \\\n            --bind /glade/campaign \\\n            --bind /glade/derecho/scratch \\\n            ./my_image.sif\n[...]\nApptainer&gt;\n</code></pre> We are now inside the container. Note the command line arguments to <code>singularity shell</code>:  <code>--nv --cleanenv</code> enables NVIDIA support with a clean environment; <code>--bind /glade/work</code> etc...:  binds host file systems into the container, allowing us to read and write from GLADE.</p>"},{"location":"environment-and-software/user-environment/containers/examples/#building-and-running-containerized-fasteddy-under-mpi-on-gpus","title":"Building and running containerized FastEddy under MPI on GPUs","text":"<p>Warning</p> <p>While the result of this demonstration is a functional application, we recommend against using this container for production FastEddy workflows!</p> <p>It is much easier to simply build FasyEddy \"bare metal\" when operating inside the NCAR HPC environment!!</p> <p>This example demonstrates building a containerized version of FastEddy from the open-source variant hosted on GitHub.  It is provided for demonstration purposes because it demonstrates several common issues encountered when running GPU-aware MPI applications inside containers across multiple nodes, particularly when binding the host MPI into the container, and the source code is open for any interested user to follow along and adapt.</p>"},{"location":"environment-and-software/user-environment/containers/examples/#about-fasteddy","title":"About FastEddy","text":"<p>FastEddy is a large-eddy simulation (LES) model developed by the Research Applications Laboratory (RAL) here at NCAR. The fundamental premise of FastEddy model development is to leverage the accelerated and more power efficient computing capacity of graphics processing units (GPU)s to enable not only more widespread use of LES in research activities but also to pursue the adoption of microscale and multiscale, turbulence-resolving, atmospheric boundary layer modeling into local scale weather prediction or actionable science and engineering applications.</p>"},{"location":"environment-and-software/user-environment/containers/examples/#containerization-approach","title":"Containerization approach","text":"<p>The container is built off-premises with <code>docker</code> from three related images, each providing a foundation for the next.  We begin with a</p> <ol> <li>Rockylinux version 8 operating system with OpenHPC version 2 installed, then add</li> <li>a CUDA development environment and a CUDA-aware MPICH installation on top, and finally add</li> <li>the FastEddy source and compiled program.</li> </ol> <p>A benefit of this layered approach is that the intermediate images created in steps 1 and 2 can be beneficial in their own right, providing base layers for other projects with similar needs. Additionally, by building the image externally with Docker we are able to switch user IDs within the process (discussed further below), which has some benefits when using containers to enable development workflows.</p>"},{"location":"environment-and-software/user-environment/containers/examples/#building-the-image","title":"Building the image","text":"<p>Build framework</p> <p>For complete details of the build process, see the Docker-based container build framework described here.</p> <p>The image was built external to the HPC environment and then pushed to Docker Hub. (For users only interested in the details of running such a container, see instructions for running the container below.)</p> <p>In this case a simple Mac laptop with <code>git</code>, GNU <code>make</code>, and <code>docker</code> all installed locally was used and the process takes about an hour; any similarly configured system should suffice. No GPU devices are required to build the image.</p>"},{"location":"environment-and-software/user-environment/containers/examples/#the-base-layer","title":"The base layer","text":"The Rockylinx 8 + OpenHPC base layer <p>For the base layer we deploy an OpenHPC v2 installation on top of a Rocklinux v8 base image.  OpenHPC provides access to many pre-complied scientific libraries and applications, and supports a matrix of compilers and MPI permutations. and we will select one that works well with Derecho.  Notably, at present OpenHPC does not natively support CUDA installations, however we will address this limitation in the subsequent steps.</p> <p>rocky8/OpenHPC-mpich/Dockerfile<pre><code>FROM docker.io/rockylinux/rockylinux:8\n\nADD extras/docker-clean /usr/bin/docker-clean\n\nARG COMPILER_VERSION=gnu9\nARG MPI_FAMILY=mpich\nARG MPI_FAMILY_VARIANT=mpich-ofi\nARG MPICH_VERSION=3.4.3\nARG OSU_VERSION=7.2\n\n# Basic OpenHPC development environment setup, derived from Install_guide-Rocky8-Warewulf-SLURM-2.4\nRUN echo \"yum/dnf config\" \\\n    &amp;&amp; set -x \\\n    &amp;&amp; adduser plainuser \\\n    &amp;&amp; chmod a+rx /usr/bin/docker-clean &amp;&amp; docker-clean \\\n    &amp;&amp; yum -y update \\\n    &amp;&amp; yum -y install which git tar curl xz bzip2 patch \\\n    &amp;&amp; yum -y install http://repos.openhpc.community/OpenHPC/2/EL_8/x86_64/ohpc-release-2-1.el8.x86_64.rpm \\\n    &amp;&amp; yum -y install dnf-plugins-core \\\n    &amp;&amp; yum config-manager --set-enabled powertools \\\n    &amp;&amp; yum -y install ohpc-base \\\n    &amp;&amp; yum -y install lmod-ohpc nhc-ohpc ohpc-autotools \\\n    &amp;&amp; yum -y install ${COMPILER_VERSION}-compilers-ohpc \\\n    &amp;&amp; yum -y install hwloc-ohpc valgrind-ohpc \\\n    &amp;&amp; yum -y install ${MPI_FAMILY_VARIANT}-${COMPILER_VERSION}-ohpc \\\n    &amp;&amp; yum -y install lmod-defaults-${COMPILER_VERSION}-${MPI_FAMILY_VARIANT}-ohpc \\\n    &amp;&amp; docker-clean\n\n# Prevent mpicxx from linking -lmpicxx, which we do not need, and cannot use on our Cray-EX\nRUN sed -i 's/cxxlibs=\"-lmpicxx\"/cxxlibs= #\"-lmpicxx\"/g' /opt/ohpc/pub/mpi/${MPI_FAMILY_VARIANT}-${COMPILER_VERSION}-ohpc/3.4.2/bin/mpicxx\n\nRUN mkdir -p /opt/local \\\n    &amp;&amp; chown -R plainuser: /home/plainuser/ /opt/local\n\nCOPY extras/hello_world_mpi.C /home/plainuser/\n\nUSER plainuser\nSHELL [\"/bin/bash\", \"-lc\"]\n\nRUN echo \"Installing MPI benchmark applications\" \\\n    &amp;&amp; whoami &amp;&amp; module avail &amp;&amp; module list \\\n    &amp;&amp; echo \"OSU:\" \\\n    &amp;&amp; cd /tmp &amp;&amp; curl -Sl https://mvapich.cse.ohio-state.edu/download/mvapich/osu-micro-benchmarks-${OSU_VERSION}.tar.gz | tar xz \\\n    &amp;&amp; cd osu-micro-benchmarks-${OSU_VERSION} \\\n    &amp;&amp; ./configure --help \\\n    &amp;&amp; ./configure --prefix=/opt/local/osu-micro-benchmarks-${OSU_VERSION} \\\n                   CXX=$(which mpicxx) CC=$(which mpicc) FC=$(which mpif90) F77=$(which mpif77) \\\n    &amp;&amp; make -j 8 V=0 &amp;&amp; rm -rf /opt/local/osu-micro-benchmarks-${OSU_VERSION} &amp;&amp; make install \\\n    &amp;&amp; cd &amp;&amp; rm -rf /tmp/osu-micro-benchmarks-${OSU_VERSION} \\\n    &amp;&amp; cd /opt/local &amp;&amp; mpicxx -o hello_world_mpi /home/plainuser/hello_world_mpi.C -fopenmp \\\n    &amp;&amp; echo \"IMB:\" \\\n    &amp;&amp; cd /opt/local &amp;&amp; rm -rf imb-2021.3 &amp;&amp; git clone https://github.com/intel/mpi-benchmarks.git imb-2021.3 \\\n    &amp;&amp; cd /opt/local/imb-2021.3 &amp;&amp; git checkout 8ba5d968272b6e7b384f91b6597d1c4590faf3db \\\n    &amp;&amp; CXX=$(which mpicxx) CC=$(which mpicc) make \\\n    &amp;&amp; make -C src_cpp -f Makefile TARGET=MPI1 clean \\\n    &amp;&amp; make -C src_cpp -f Makefile TARGET=NBC clean\\\n    &amp;&amp; make -C src_cpp -f Makefile TARGET=RMA clean \\\n    &amp;&amp; make -C src_cpp -f Makefile TARGET=EXT clean \\\n    &amp;&amp; make -C src_cpp -f Makefile TARGET=IO clean \\\n    &amp;&amp; make -C src_cpp -f Makefile TARGET=MT clean \\\n    &amp;&amp; make -C src_c/P2P -f Makefile TARGET=P2P clean \\\n    &amp;&amp; docker-clean\n\n# Local Variables:\n# mode: sh\n# End:\n</code></pre> Dockerfile Steps</p> <ol> <li>The image begins with a minimal Rockylinux v8 image, and adds a utility script <code>docker-clean</code> copied from the build host.</li> <li>We parameterize several variables with build arguments using the <code>ARG</code> instructions.  (Build arguments are available within the image build process as environment variables, but not when running the resulting container image; rather <code>ENV</code> instructions can be used for those purposes. For a full discussion of <code>Dockerfiles</code> and supported instructions see here.)</li> <li> <p>We then perform a number of <code>RUN</code> steps.  When running <code>docker</code>, each <code>RUN</code> step creates a subsequent layer in the image. (We follow general Docker guidance and also strive to combine related commands inside a handful of <code>RUN</code> instructions.)</p> <ol> <li>The first <code>RUN</code> instruction takes us from the very basic Rockylinux 8 source image to a full OpenHPC installation.  We add a non-privileged user <code>plainuser</code> to leverage later, update the OS image with any available security patches, and then generally follow an OpenHPC installation recipe to add compilers, MPI, and other useful development tools.</li> <li>The second <code>RUN</code> step works around an issue we would find later when attempting to run the image on Derecho.  Specifically, the OpenHPC <code>mpich-ofi</code> package provides support for the long-deprecated MPI C++ interface.  This is not present on Derecho with the <code>cray-mpich</code> implementation we will ultimately use to run the container.  Since we do not need this support, here we hack the <code>mpicxx</code> wrapper so that it does not link in <code>-lmpicxx</code>, the problematic library.</li> <li>The third and following <code>RUN</code> instructions steps create a directory space <code>/opt/local</code> we can use from our unprivileged <code>plainuser</code> account, copy in some more files, and then switch to <code>plainuser</code> to test the development environment by installing some common MPI benchmarks.</li> </ol> </li> </ol> <p>Discussion</p> <ul> <li> <p>OpenHPC v2 supports both OpenSUSE and Rocklinux 8 as its base OS. It would be natural to choose OpenSUSE for similarity to Casper and Derecho, however by choosing instead Rocklinux we gain access to a different build environment, which has benefits for developers looking to improve portability.  This process followed here can also be thought of as a \"roadmap\" for deploying the application at similarly configured external sites.</p> </li> <li> <p>OpenHPC supports <code>openmpi</code> and <code>mpich</code> MPI implementations, with the latter in two forms: <code>mpich-ucx</code> and <code>mpich-ofi</code>.  In this example we intentionally choose <code>mpich-ofi</code> with prior knowledge of the target execution environment.  On Derecho the primary MPI implementation is <code>cray-mpich</code> (itself forked from <code>mpich</code>) which uses an HPE-proprietary <code>libfabric</code> interface to the Slingshot v11 high-speed communication fabric.</p> </li> <li> <p>Notice that each <code>RUN</code> step is finalized with a <code>docker-clean</code> command.  This utility script removes temporary files and cached data to minimize the size of the resulting image layers.  One consequence is that the first <code>dnf</code> package manager interaction in a <code>RUN</code> statement will re-cache these data.  Since cached data are not relevant in the final image - especially when run much later on - we recommend removing it to reduce image bloat.</p> </li> <li> <p>In this example we are intentional switching between <code>root</code> (the default user in the build process) and our unprivileged <code>plainuser</code> account.  Particularly in development workflows, we want to be sure compilation and installation steps work properly as an unprivileged user, and tools such as the <code>lmod</code> module system and <code>mpiexec</code> often are intended not to be used as <code>root</code>.</p> </li> <li> <p>Since MPI container runtime inregration can be a pain point at execution, we install OSU's and Intel's MPI benchmark suites to aid in deployment testing, independent of any user application.</p> </li> </ul> <p>Building the image <pre><code>docker build --tag &lt;dockerhub_username&gt;/rocky8-openhpc-mpich:latest .\n</code></pre></p>"},{"location":"environment-and-software/user-environment/containers/examples/#adding-cuda-cuda-aware-mpich","title":"Adding CUDA &amp; CUDA-aware MPICH","text":"Adding CUDA + CUDA-aware MPICH <p>Next we add CUDA and add a CUDA-aware MPI installation.  We choose a specific version of the open-source MPICH library (both to closely match what is provided by OpenHPC and for Derecho compatibility) and configure it to use the pre-existing OpenHPC artifacts (<code>hwloc</code>, <code>libfabric</code>) as dependencies. For both <code>cuda</code> and the new <code>mpich</code> we also install \"modulefiles\" so the new additions are available in the typical module environment.  Finally, we re-install one of the MPI benchmark applications, this time with CUDA support. rocky8/OpenHPC-cuda/Dockerfile<pre><code>FROM benjaminkirk/rocky8-openhpc-mpich:latest\n\nARG COMPILER_VERSION=gnu9\nARG MPI_FAMILY=mpich\nARG MPI_FAMILY_VARIANT=mpich-ofi\nARG MPICH_VERSION=3.4.3\nARG OSU_VERSION=7.2\n\nUSER root\n\n# https://developer.nvidia.com/cuda-11-7-1-download-archive\nRUN echo \"Cuda\" \\\n    &amp;&amp; curl -O https://developer.download.nvidia.com/compute/cuda/11.7.1/local_installers/cuda-repo-rhel8-11-7-local-11.7.1_515.65.01-1.x86_64.rpm \\\n    &amp;&amp; rpm -i cuda-repo-rhel8-11-7-local-11.7.1_515.65.01-1.x86_64.rpm &amp;&amp; rm -f cuda-repo-rhel8-11-7-local-11.7.1_515.65.01-1.x86_64.rpm \\\n    &amp;&amp; dnf -y install cuda &amp;&amp; rm /var/cuda-repo-rhel8-11-7-local/*.rpm &amp;&amp; dnf config-manager --disable cuda-rhel8-11-7-local \\\n    &amp;&amp; echo \"RDMA prereqs\" \\\n    &amp;&amp; dnf -y install libibverbs-devel libpsm2-devel \\\n    &amp;&amp; docker-clean\n\nRUN mkdir /opt/ohpc/pub/moduledeps/${COMPILER_VERSION}/cuda\nCOPY extras/cuda-11.7 /opt/ohpc/pub/moduledeps/${COMPILER_VERSION}/cuda/11.7\nCOPY extras/mpich-${MPICH_VERSION}-ofi-cuda /opt/ohpc/pub/moduledeps/${COMPILER_VERSION}/mpich/${MPICH_VERSION}-ofi-cuda\nCOPY extras/hello_world.cu /home/plainuser\n\nRUN mkdir -p /opt/local \\\n    &amp;&amp; chown -R plainuser: /home/plainuser/ /opt/local\n\nUSER plainuser\nSHELL [\"/bin/bash\", \"-lc\"]\n\nRUN whoami &amp;&amp; module avail \\\n    &amp;&amp; module load -mpich +hwloc +libfabric +cuda &amp;&amp; module list \\\n    &amp;&amp; cd /opt/local &amp;&amp; nvcc -o hello_cuda /home/plainuser/hello_world.cu --cudart shared \\\n    &amp;&amp; cd /tmp &amp;&amp; curl -sSL https://www.mpich.org/static/downloads/${MPICH_VERSION}/mpich-${MPICH_VERSION}.tar.gz | tar xz \\\n    &amp;&amp; cd mpich-${MPICH_VERSION} \\\n    &amp;&amp; ./configure --help \\\n    &amp;&amp; ./configure --prefix=/opt/local/mpich-${MPICH_VERSION}-cuda \\\n                   CC=$(which gcc) CXX=$(which g++) FC=$(which gfortran) F77=$(which gfortran) \\\n                   --enable-fortran \\\n                   --with-libfabric=${LIBFABRIC_DIR} \\\n                   --with-hwloc-prefix=${HWLOC_DIR} \\\n                   --with-cuda=${CUDA_HOME} \\\n    &amp;&amp; make -j 8 &amp;&amp; make install \\\n    &amp;&amp; docker-clean\n\n# Prevent mpicxx from linking -lmpicxx, which we do not need, and cannot use on our Cray-EX\nRUN sed -i 's/cxxlibs=\"-lmpicxx\"/cxxlibs= #\"-lmpicxx\"/g' /opt/local/mpich-${MPICH_VERSION}-cuda/bin/mpicxx\n\nRUN echo \"Installing MPI benchmark applications\" \\\n    &amp;&amp; whoami &amp;&amp; module avail \\\n    &amp;&amp; module load mpich/${MPICH_VERSION}-ofi-cuda &amp;&amp; module list \\\n    &amp;&amp; echo \"OSU:\" \\\n    &amp;&amp; cd /tmp &amp;&amp; curl -Sl https://mvapich.cse.ohio-state.edu/download/mvapich/osu-micro-benchmarks-${OSU_VERSION}.tar.gz | tar xz \\\n    &amp;&amp; cd osu-micro-benchmarks-${OSU_VERSION} \\\n    &amp;&amp; ./configure --help \\\n    &amp;&amp; ./configure --prefix=/opt/local/osu-micro-benchmarks-${OSU_VERSION} \\\n                   CXX=$(which mpicxx) CC=$(which mpicc) FC=$(which mpif90) F77=$(which mpif77) LIBS=\"-L${CUDA_HOME}/targets/x86_64-linux/lib -lcudart\" \\\n                   --enable-cuda --with-cuda=${CUDA_HOME} \\\n    &amp;&amp; make -j 8 V=0 &amp;&amp; rm -rf /opt/local/osu-micro-benchmarks-${OSU_VERSION} &amp;&amp; make install \\\n    &amp;&amp; cd &amp;&amp; rm -rf /tmp/osu-micro-benchmarks-${OSU_VERSION} \\\n    &amp;&amp; cd /opt/local &amp;&amp; mpicxx -o hello_world_mpi /home/plainuser/hello_world_mpi.C -fopenmp \\\n    &amp;&amp; docker-clean\n\n# Local Variables:\n# mode: sh\n# End:\n</code></pre></p> <p>Dockerfile Steps</p> <ol> <li> <p>We switch back to the <code>root</code> user so we can modify the operating system installation within the image.</p> </li> <li> <p>The first <code>RUN</code> instruction installs a full CUDA development environment and some additional development packages required to build MPI later.</p> </li> <li> <p>The next <code>RUN</code> instructions install modulefiles into the image so we can access the CUDA and (upcoming) MPICH installation, and clean up file permissions.  The remaining steps are executed again as our unprivileged <code>plainuser</code>.</p> </li> <li> <p>The fourth <code>RUN</code> instruction downloads, configures, and installs MPICH.  The version is chosen to closely match the baseline MPICH already installed in the image and uses some of its dependencies, and we also enable CUDA support.</p> </li> <li> <p>In the final <code>RUN</code> instruction we re-install one of the MPI benchmark applications, this time with CUDA support.</p> </li> </ol> <p>Discussion</p> <ul> <li> <p>There are several ways to install CUDA, here we choose a \"local repo\" installation because it allows us to control versions, but are careful also to remove the downloaded packages after installation, freeing up 3GB+ in the image.</p> </li> <li> <p>The CUDA development environment is very large and it is difficult to separate unnecessary components, so is step increases the size of the image from ~1.2GB to 8.8GB. We leave all components in the development image, including tools we will very likely not need inside a container such as <code>nsight-systems</code> and <code>nsight-compute</code>.  For applications built on top of this image, a user could optionally remove these components later to decrease their final image size (demonstrated next).</p> </li> </ul> <p>Building the image <pre><code>docker build --tag &lt;dockerhub_username&gt;/rocky8-openhpc-mpich-cuda:latest .\n</code></pre></p>"},{"location":"environment-and-software/user-environment/containers/examples/#building-fasteddy","title":"Building FastEddy","text":"Adding FastEddy <p>rocky8/OpenHPC-FastEddy/Dockerfile<pre><code>FROM benjaminkirk/rocky8-openhpc-mpich-cuda:latest\n\nARG MPICH_VERSION=3.4.3\n\nUSER root\n\nRUN echo \"netcdf: serial netcdf from the base OS\" \\\n    &amp;&amp; yum -y install \\\n           netcdf-devel \\\n    &amp;&amp; echo \"removing unnecessary NVIDIA components to shrink container image\" \\\n    &amp;&amp; rm -rf /opt/local/nvidia /usr/local/cuda-11.7/targets/x86_64-linux/lib/*_static.a \\\n    &amp;&amp; docker-clean\n\nUSER plainuser\nRUN echo \"FastEddy - source\" \\\n    &amp;&amp; cd /opt/local &amp;&amp; git clone https://github.com/NCAR/FastEddy-model.git &amp;&amp; git clone https://github.com/NCAR/FastEddy-tutorials.git \\\n    &amp;&amp; cd FastEddy-model/SRC/FEMAIN \\\n    &amp;&amp; sed -i 's/TEST_LIBS = -lm -lmpi -lstdc++ -lcurand/TEST_LIBS = -lm -lmpi -lstdc++ $(LIBS)/g' Makefile \\\n    &amp;&amp; sed -i 's/TEST_CU_LIBS = -lm -lmpi -lcurand/TEST_CU_LIBS = -lm -lmpi $(LIBS)/g' Makefile \\\n    &amp;&amp; docker-clean\n\nRUN echo \"FastEddy - build\" \\\n    &amp;&amp; module avail &amp;&amp; module load mpich/${MPICH_VERSION}-ofi-cuda &amp;&amp; module list \\\n    &amp;&amp; cd /opt/local/FastEddy-model/SRC &amp;&amp; fe_inc= &amp;&amp; for d in */ */*/ ; do fe_inc=\"-I$(pwd)/${d} ${fe_inc}\" ; done \\\n    &amp;&amp; cd FEMAIN &amp;&amp; make \\\n                        INCLUDES=\"${fe_inc} -I${MPI_DIR}/include/ -I${CUDA_HOME}/targets/x86_64-linux/include/\" \\\n                        LIBS=\"-L${CUDA_HOME}/targets/x86_64-linux/lib -lcurand -lcudart -lcuda -L/usr/lib64 -lnetcdf\" \\\n    &amp;&amp; ldd ./FastEddy \\\n    &amp;&amp; docker-clean\n\n# Local Variables:\n# mode: sh\n# End:\n</code></pre> Dockerfile Steps</p> <ol> <li>Again we switch back to <code>root</code> for performing operating system level tasks, as our base image left us as <code>plainuser</code>.</li> <li>The first <code>RUN</code> instruction installs the development package for NetCDF - an additional application dependency not already satisfied.  We also remove some particularly large CUDA components from the development image not required in the final application image.</li> <li>Then again as <code>plainuser</code>, the next <code>RUN</code> instruction downloads the FastEddy open-source variant.  We make some changes to the definition of a few hard-coded <code>make</code> variables so that we can specify installation paths during linking later.</li> <li>The final <code>RUN</code> instruction then builds FastEddy.  We build up and use custom <code>INCLUDE</code> and <code>LIBS</code> variables, specifying some unique paths for the particular build environment.</li> </ol> <p>Discussion</p> <ul> <li>When building the image locally with Docker, the space savings from step (2) are not immediately apparent.  This is a result of the Docker \"layer\" approach: the content still exists in the base layer and is only \"logically\" removed by the commands listed above.  The space savings is realized on the HPC system when we \"pull\" the image with <code>singularity</code>.</li> <li>If an even smaller container image is desired, even more components could be stripped: CUDA numerical libraries the application does not need, or even the containerized MPIs after we are done with them.  As we will see next, we replace the container MPI with the host MPI at run-time, so technically no MPI is required inside the container when we are done using it for compilation.</li> </ul> <p>Building the image <pre><code>docker build --tag &lt;dockerhub_username&gt;/rocky8-openhpc-fasteddy:latest .\n</code></pre></p> <p>Pushing the image to Docker Hub <pre><code>docker push &lt;dockerhub_username&gt;/rocky8-openhpc-fasteddy:latest\n</code></pre></p>"},{"location":"environment-and-software/user-environment/containers/examples/#running-the-container-on-derecho","title":"Running the container on Derecho","text":"<p>With the container built from the steps above (or simply pulling the resulting image from Docker Hub), we are now ready to run a sample test case on Derecho.  We choose <code>Example02_CBL.in</code> from the FastEddy Tutorial and modify it to run on 24 GPUs (full steps listed here).  The PBS job script listed below shows the steps required to \"bind\" the host MPI into the container.</p> Containerized FastEddy PBS Script run_fasteddy_container.pbs<pre><code>#!/bin/bash\n#PBS -q main\n#PBS -j oe\n#PBS -o fasteddy_job.log\n#PBS -l walltime=02:00:00\n#PBS -l select=6:ncpus=64:mpiprocs=4:ngpus=4\n\nmodule load ncarenv/23.09\nmodule load apptainer gcc cuda || exit 1\nmodule list\n\nnnodes=$(cat ${PBS_NODEFILE} | sort | uniq | wc -l)\nnranks=$(cat ${PBS_NODEFILE} | sort | wc -l)\nnranks_per_node=$((${nranks} / ${nnodes}))\n\ncontainer_image=\"./rocky8-openhpc-fasteddy.sif\"\n\nsingularity \\\n    --quiet \\\n    exec \\\n    ${container_image} \\\n    ldd /opt/local/FastEddy-model/SRC/FEMAIN/FastEddy\n\nsingularity \\\n    --quiet \\\n    exec \\\n    --bind ${SCRATCH} \\\n    --bind ${WORK} \\\n    --pwd $(pwd) \\\n    --bind /run \\\n    --bind /opt/cray \\\n    --bind /usr/lib64:/host/lib64 \\\n    --env LD_LIBRARY_PATH=${CRAY_MPICH_DIR}/lib-abi-mpich:/opt/cray/pe/lib64:${LD_LIBRARY_PATH}:/host/lib64 \\\n    --env LD_PRELOAD=/opt/cray/pe/mpich/${CRAY_MPICH_VERSION}/gtl/lib/libmpi_gtl_cuda.so.0 \\\n    ${container_image} \\\n    ldd /opt/local/FastEddy-model/SRC/FEMAIN/FastEddy\n\n\n\necho \"# --&gt; BEGIN execution\"; tstart=$(date +%s)\n\nmpiexec \\\n    --np ${nranks} --ppn ${nranks_per_node} --no-transfer \\\n    set_gpu_rank \\\n    singularity \\\n    --quiet \\\n    exec \\\n    --bind ${SCRATCH} \\\n    --bind ${WORK} \\\n    --pwd $(pwd) \\\n    --bind /run \\\n    --bind /opt/cray \\\n    --bind /usr/lib64:/host/lib64 \\\n    --env LD_LIBRARY_PATH=${CRAY_MPICH_DIR}/lib-abi-mpich:/opt/cray/pe/lib64:${LD_LIBRARY_PATH}:/host/lib64 \\\n    --env LD_PRELOAD=/opt/cray/pe/mpich/${CRAY_MPICH_VERSION}/gtl/lib/libmpi_gtl_cuda.so.0 \\\n    --env MPICH_GPU_SUPPORT_ENABLED=1 \\\n    --env MPICH_GPU_MANAGED_MEMORY_SUPPORT_ENABLED=1 \\\n    --env MPICH_SMP_SINGLE_COPY_MODE=NONE \\\n    ${container_image} \\\n    /opt/local/FastEddy-model/SRC/FEMAIN/FastEddy \\\n    ./Example02_CBL.in\n\necho \"# --&gt; END execution\"\necho $(($(date +%s)-${tstart})) \" elapsed seconds; $(date)\"\n</code></pre> <p>Discussion</p> <p>The <code>mpiexec</code> command is fairly standard.  Note that we are using it to launch <code>singularity</code>, which in turn will start up the containerized <code>FastEddy</code> executable.</p> <p>The <code>singularity exec</code> command line is complex, so let's deconstruct it here:</p> <ul> <li> <p>We make use of the <code>--bind</code> argument first to mount familiar GLADE file systems within the container,</p> </li> <li> <p>and again to \"inject\" the host MPI into the container (as described here).  The <code>/run</code> directory necessity is not immediately obvious but is used by Cray-MPICH as part of the launching process.</p> </li> <li> <p>We also need to use the <code>--env</code> to set the <code>LD_LIBRARY_PATH</code> inside the image so that the application can find the proper host libraries.  Recall when we built the FastEddy executable in the containerized environment it had no knowledge of these host-specific paths.  Similarly, we use <code>--env</code> to set the <code>LD_PRELOAD</code> environment variable inside the container.  This will cause a particular Cray-MPICH library to be loaded prior to application initialization.  This step is not required for \"bare metal\" execution.</p> </li> <li> <p>We set some important Cray-MPICH specific <code>MPICH_*</code> environment variables as well to enable CUDA-awareness (<code>MPICH_GPU_*</code>)  and work around an MPI run-time error  (<code>MPICH_SMP_SINGLE_COPY_MODE</code>) that will otherwise appear.</p> </li> <li>Finally, a note on the <code>--bind /usr/lib64:/host/lib64</code> argument.  Injecting the host MPI requires that some shared libraries from the host's <code>/usr/lib64</code> directory be visible inside the image.  However, this path also exists inside the image and contains other libraries needed by the application.  We cannot simply bind the hosts directory into the same path, doing so will mask these other libraries.  So we bind the host's <code>/usr/lib64</code> into the container image at <code>/host/lib64</code>, and make sure this path is set in the <code>LD_LIBRARY_PATH</code> variable as well.  Because we want these particular host libraries found as last resort (not taking precedence over similar libraries in the container, we append <code>/host/lib64</code> to the <code>LD_LIBRARY_PATH</code> search path.</li> </ul> <p>The arguments above were determined iteratively through trial and error.  Such is the reality of containerized MPI applications and proprietary host MPI integration. Feel free to experiment with the PBS file, omitting some of the <code>--bind</code> and <code>--env</code> arguments and observing the resulting error message, however  do NOT modify the <code>MPICH_GPU_*</code> variables, doing so may trigger a very unfortunate kernel driver bug and render the GPU compute nodes unusable.</p> <p>Pulling the image</p> <p>We begin with pulling the image from Docke Hub and constructing a SIF. (If you want to test your own built/pushed image, replace <code>benjaminkirk</code> with your own <code>&lt;dockerhub_username&gt;</code> as specified in the tag/push operations listed above.) <pre><code>derecho$ singularity pull rocky8-openhpc-fasteddy.sif docker://benjaminkirk/rocky8-openhpc-fasteddy:latest\n[...]\n\nderecho$ ls -lh rocky8-openhpc-fasteddy.sif\n-rwxr-xr-x 1 someuser ncar 3.1G Dec  5 17:08 rocky8-openhpc-fasteddy.sif\n</code></pre></p> <p>Running the job <pre><code>derecho$ mkdir ./output\nderecho$ qsub -A &lt;account&gt; run_fasteddy_container.pbs\nderecho$ tail -f fasteddy_job.log\n</code></pre></p>"},{"location":"environment-and-software/user-environment/containers/examples/#faking-a-native-installation-of-containerized-applications","title":"\"Faking\" a native installation of containerized applications","text":"<p>Occasionally it can be beneficial to \"hide\" the fact that a particular application is containerized, typically to simplify the user interface and usage experience. In this section we follow a clever approach deployed by the NIH Biowulf team and outlined here to enable users to interact transparently with containerized applications without needing to know any details of the run-time (<code>singularity</code>, <code>ch-run</code>, etc...).</p> <p>The basic idea is to create a <code>wrapper.sh</code> shell script that</p> <ol> <li>Infers the name of the containerized command to run,</li> <li>Invokes the chosen run-time transparently to the user, and</li> <li>Passes along any command-line arguments to the containerized application.</li> </ol> <p>Consider the following directory tree structure, taken from a production deployment: Directory tree for 'faking' native installation of containerized applications<pre><code>/glade/u/apps/opt/leap-container/15/\n\u251c\u2500\u2500 bin/\n\u2502   \u251c\u2500\u2500 eog -&gt; ../libexec/wrap_singularity.sh\n\u2502   \u251c\u2500\u2500 evince -&gt; ../libexec/wrap_singularity.sh\n\u2502   \u251c\u2500\u2500 gedit -&gt; ../libexec/wrap_singularity.sh\n\u2502   \u251c\u2500\u2500 geeqie -&gt; ../libexec/wrap_singularity.sh\n\u2502   \u251c\u2500\u2500 gimp -&gt; ../libexec/wrap_singularity.sh\n\u2502   \u251c\u2500\u2500 gv -&gt; ../libexec/wrap_singularity.sh\n\u2502   \u251c\u2500\u2500 smplayer -&gt; ../libexec/wrap_singularity.sh\n\u2502   \u251c\u2500\u2500 vlc -&gt; ../libexec/wrap_singularity.sh\n\u2502   \u2514\u2500\u2500 xfig -&gt; ../libexec/wrap_singularity.sh\n\u2514\u2500\u2500 libexec/\n    \u251c\u2500\u2500 Makefile\n    \u251c\u2500\u2500 ncar-casper-gui_tools.sif\n    \u2514\u2500\u2500 wrap_singularity.sh\n</code></pre></p> <p>At the top level, we simply have two directories: <code>./bin/</code> (which likely will go into the user's <code>PATH</code>) and <code>./libexec/</code> (where we will hide implementation details).</p> <p>Constructing the <code>bin</code> directory</p> <p>The <code>./bin/</code> directory contains symbolic links to the <code>wrap_singularity.sh</code> script, where the name of the symbolic link is the containerized application to run.  For the example above, when a user runs <code>./bin/gv</code> for example, it will invoke the <code>wrap_singularity.sh</code> \"behind the scenes.\"  In general there can be many application symbolic links in the <code>./bin/</code> directory, so long as the desired application exists within the wrapped container image.</p> <p>The <code>wrap_singularity.sh</code> wrapper script</p> <p>The <code>wrap_singularity.sh</code> script is written such that  whatever symbolic links you create to it will run inside of the container, inferring the application name from that of the symbolic link. wrap_singularity.sh<pre><code>#!/bin/bash\n\n#----------------------------------------------------------------------------\n# environment\ntopdir=\"$(pwd)\"\nselfdir=\"$(dirname $(readlink -f ${BASH_SOURCE[0]}))\"\nrequested_command=\"$(basename ${0})\"\ncontainer_img=\"ncar-casper-gui_tools.sif\"\n#----------------------------------------------------------------------------\n\ntype module &gt;/dev/null 2&gt;&amp;1 || source /etc/profile.d/z00_modules.sh\nmodule load apptainer || exit 1\n\ncd ${selfdir} || exit 1\n[ -f ${container_img} ] || { echo \"Cannot locate ${container_img}\"; exit 1; }\n\ncd ${topdir} || exit 1\n\nsingularity \\\n    --quiet \\\n    exec \\\n    -B /glade/campaign \\\n    -B /glade/derecho/scratch \\\n    -B /glade/work \\\n    -B /glade/u \\\n    ${selfdir}/${container_img} \\\n    ${requested_command} ${@}\n</code></pre></p> <p>Specifically:</p> <ul> <li>The command to execute is inferred from the shell argument <code>${0}</code> - the name of the script being executed.  Here is where the symbolic links from <code>./bin</code> are important:  If the symbolic link <code>./bin/gv</code> is invoked, for example, the script above will execute with the name <code>gv</code>.  This is accessible within the script as <code>${0}</code>, and is stored in the <code>requested_command</code> variable on line 7.</li> <li>Any command-line arguments passed to the executable are captured in the <code>${@}</code> environment variable, and are passed directly through as command-line arguments to the containerized application (line 27).</li> <li>We bind-mount the usual GLADE file systems so that expected data are accessible (lines 22-25).</li> <li>In this example we execute all commands in the same base container <code>ncar-casper-gui_tools.sif</code> (specified on line 8).  This is the simplest approach, however strictly not required.  (A more complex treatment could \"choose\" different base containers for different commands using a <code>bash</code> <code>case</code> statement, for example, if desired.)</li> <li>The container is launched with the users' directory <code>topdir</code> as the working directory.  This is required so that any relative paths specified are handled properly.</li> <li>In order to robustly access the required <code>apptainer</code> module, we first check to see if the <code>module</code> command is recognized and if not initialize the module environment (line 11), then load the <code>apptainer</code> module (line 12).  This allows the script to function properly even when the user does not have the module system initialized in their environment - a rare but an occasional issue.</li> </ul> <p>While the example above wraps the Apptainer run-time, a similar approach works for Charliecloud and Podman as well if desired.</p>"},{"location":"environment-and-software/user-environment/containers/working_with_containers/","title":"Working with Containers","text":"<p>The first step in running a container is usually (i) to \"pull\" a pre-existing container from an external repository, or (ii) to build a container according to a recipe. When a container is built locally, it is usually desirable to then share it with a larger community, often by \"pushing\" the resulting image to an external repository.  We cover these topics generally in the next sections, which will be valuable both for users unfamiliar with containers and to others familiar with a tool such as Docker but unclear on how to execute these steps specifically in one of our supported runtimes.</p>"},{"location":"environment-and-software/user-environment/containers/working_with_containers/#general-container-usage","title":"General container usage","text":"<p>The concepts of pull, build, and push are common regardless of container run-time, however the specifics vary. We will highlight these specific steps for each supported run time in the examples below.</p> <p>Example scenario</p> <p>In the examples below we will work incrementally through a simple but realistic example use case: building a container using the latest version of a different operating system to provide tools not available on the host.  Specifically, we will:</p> <ol> <li> <p>Begin with a basic Rocky Linux 9 container image fetched from Docker Hub,</p> </li> <li> <p>Demonstrate building our own derived container image with additional packages and tools, and</p> </li> <li> <p>Demonstrate sharing the resulting image.</p> </li> </ol>"},{"location":"environment-and-software/user-environment/containers/working_with_containers/#pulling-a-container","title":"Pulling a container","text":"<p>Pulling &amp; converting a simple container image</p> ApptainerCharliecloudPodman <p>Pulling &amp; listing images</p> <p>We will use the command <code>singularity pull</code> from the <code>apptainer</code> module to pull our image and save it in Singularity Image Format (SIF): singularity pull<pre><code>casper$ singularity pull ./rocky9.sif docker://rockylinux/rockylinux:9\nINFO:    Converting OCI blobs to SIF format\nINFO:    Starting build...\nGetting image source signatures\nCopying blob 4031b0359885 done\nCopying config 175264fac6 done\nWriting manifest to image destination\nStoring signatures\n2023/11/27 15:12:32  info unpack layer: sha256:4031b03598854f77c4ae1e53c2fdca86fdb41eb95f1f051416ce2e363fc8cdd2\nINFO:    Creating SIF file...\n</code></pre></p> <p>Prefer Apptainer's SIF image format</p> <p>SIF images are much better suited for use on large parallel file systems than large directory trees, and can easily be shared with other users.</p> <p>Like most run times, Apptainer supports several image storage formats, including unpacked directory tree \"sandboxes\" and compressed read-only image bundles in Singularity Image Format (SIF). We recommend using read-only, compressed SIF images for both performance and best practices reasons. While sandboxes may be tempting to create \"writable\" containers, they create sprawling directory trees of many small files, which slow container startup time and complicate management especially on shared, parallel file systems.   Furthermore, writable container images undercut the encapsulation and repeatability benefits offered by containerization.  It is possible to run containers on top of SIF images with temporary write layers if necessary.</p> <p>Running a simple command from the container</p> <p>We cover running containers in much more detail here, however below we will use the command <code>ch-run</code> to inspect the contents of the file <code>/etc/os-release</code> inside the container: singularity exec<pre><code>casper$ singularity exec ./rocky9.sif cat /etc/os-release\nNAME=\"Rocky Linux\"\nVERSION=\"9.2 (Blue Onyx)\"\nID=\"rocky\"\nID_LIKE=\"rhel centos fedora\"\nVERSION_ID=\"9.2\"\nPLATFORM_ID=\"platform:el9\"\nPRETTY_NAME=\"Rocky Linux 9.2 (Blue Onyx)\"\nANSI_COLOR=\"0;32\"\nLOGO=\"fedora-logo-icon\"\nCPE_NAME=\"cpe:/o:rocky:rocky:9::baseos\"\nHOME_URL=\"https://rockylinux.org/\"\nBUG_REPORT_URL=\"https://bugs.rockylinux.org/\"\nSUPPORT_END=\"2032-05-31\"\nROCKY_SUPPORT_PRODUCT=\"Rocky-Linux-9\"\nROCKY_SUPPORT_PRODUCT_VERSION=\"9.2\"\nREDHAT_SUPPORT_PRODUCT=\"Rocky Linux\"\nREDHAT_SUPPORT_PRODUCT_VERSION=\"9.2\"\n</code></pre> This is functionally a <code>hello-world</code> type demonstration, and can be compared to the same file on the host to show we are indeed running in a different environment.</p> <p><code>apptainer</code> vs. <code>singularity</code></p> <p>As of version 3, the commands <code>apptainer</code> and <code>singularity</code> are synonymous.  We will use the latter as there is a wide array of existing documentation referencing the <code>singularity</code> executable across the internet.</p> <p>Pulling &amp; listing images</p> <p>We will use the command <code>ch-image</code> from the <code>charliecloud</code> module to pull and list images: ch-image pull &amp; ch-image list<pre><code># Pull the requested image, storing into Charliecloud's internal format\ncasper$ ch-image pull rockylinux/rockylinux:9\npulling image:    rockylinux/rockylinux:9\nrequesting arch:  amd64\nmanifest list: downloading: 100%\nmanifest: downloading: 100%\nconfig: downloading: 100%\nlayer 1/1: 4031b03: downloading: 63.6/63.6 MiB (100%)\npulled image: adding to build cache\nflattening image\nlayer 1/1: 4031b03: listing\nvalidating tarball members\nlayer 1/1: 4031b03: changed 34 absolute symbolic and/or hard links to relative\nresolving whiteouts\nlayer 1/1: 4031b03: extracting\nimage arch: amd64\n\n# List all known images\ncasper$ ch-image list\nrockylinux/rockylinux:9\n</code></pre> See <code>ch-image --help</code> for more details and options.</p> <p>Prefer Charliecloud's bundled SquashFUSE image format</p> <p>After running the two commands above, the requested container has been downloaded and unpacked into Charliecloud's storage directory tree in its native format.  This often is on temporary storage, and it is advisable to use <code>ch-convert</code> to convert the image to one of several other image formats before use.</p> <p>Converting the image</p> <p>On NCAR's HPC systems we strive to support the <code>squash</code> SquashFS file system archive, which allows the container to be converted to a single, compressed file.  This is much better suited for use on large parallel file systems, and can easily be shared with other users.  The command <code>ch-convert</code> can be used to convert images between Charliecloud's supported formats.</p> <p>ch-convert<pre><code># Convert from Charliecloud's internal format to a compressed SquashFUSE image\ncasper$ ch-convert rockylinux/rockylinux:9 ./rocky9.sqfs\nParallel mksquashfs: Using 72 processors\nCreating 4.0 filesystem on ./rocky9.sqfs, block size 65536.\n[=====================================================================|] 8075/8075 100%\n\nExportable Squashfs 4.0 filesystem, gzip compressed, data block size 65536\n        compressed data, compressed metadata, compressed fragments,\n        compressed xattrs, compressed ids\n        duplicates are removed\nFilesystem size 61709.90 Kbytes (60.26 Mbytes)\n        35.85% of uncompressed filesystem size (172122.07 Kbytes)\nInode table size 80336 bytes (78.45 Kbytes)\n        27.36% of uncompressed inode table size (293634 bytes)\nDirectory table size 88626 bytes (86.55 Kbytes)\n        43.45% of uncompressed directory table size (203971 bytes)\nNumber of duplicate files found 1875\nNumber of inodes 8122\nNumber of files 6194\nNumber of fragments 693\nNumber of symbolic links 909\nNumber of device nodes 0\nNumber of fifo nodes 0\nNumber of socket nodes 0\nNumber of directories 1019\nNumber of hard-links 0\nNumber of ids (unique uids + gids) 1\nNumber of uids 1\n        root (0)\nNumber of gids 1\n        root (0)\n</code></pre> See <code>ch-convert --help</code> for more details and options.</p> <p>Running a simple command from the container</p> <p>We cover running containers in much more detail here, however below we will use the command <code>ch-run</code> to inspect the contents of the file <code>/etc/os-release</code> inside the container: ch-run<pre><code>casper$ ch-run ./rocky9.sqfs -- cat /etc/os-release\nNAME=\"Rocky Linux\"\nVERSION=\"9.2 (Blue Onyx)\"\nID=\"rocky\"\nID_LIKE=\"rhel centos fedora\"\nVERSION_ID=\"9.2\"\nPLATFORM_ID=\"platform:el9\"\nPRETTY_NAME=\"Rocky Linux 9.2 (Blue Onyx)\"\nANSI_COLOR=\"0;32\"\nLOGO=\"fedora-logo-icon\"\nCPE_NAME=\"cpe:/o:rocky:rocky:9::baseos\"\nHOME_URL=\"https://rockylinux.org/\"\nBUG_REPORT_URL=\"https://bugs.rockylinux.org/\"\nSUPPORT_END=\"2032-05-31\"\nROCKY_SUPPORT_PRODUCT=\"Rocky-Linux-9\"\nROCKY_SUPPORT_PRODUCT_VERSION=\"9.2\"\nREDHAT_SUPPORT_PRODUCT=\"Rocky Linux\"\nREDHAT_SUPPORT_PRODUCT_VERSION=\"9.2\"\n</code></pre> This is functionally a <code>hello-world</code> type demonstration, and can be compared to the same file on the host to show we are indeed running in a different environment.</p> <p>Pulling &amp; listing images</p> <p>We will use the command <code>podman pull</code> and <code>podman images</code> from the <code>podman</code> module to pull and list images: podman image pull &amp; podman images<pre><code># Pull the requested image, storing into Podman's internal image format\ncasper$ podman image pull docker://rockylinux/rockylinux:9\nTrying to pull docker.io/rockylinux/rockylinux:9...\nGetting image source signatures\nCopying blob 4031b0359885 done\nCopying config 175264fac6 done\nWriting manifest to image destination\nStoring signatures\n175264fac6da4392fb2a9761583c81f509745629daee81de29beb7051f360db7\n\n# list known (downloaded) images\ncasper$ podman images\nREPOSITORY                       TAG         IMAGE ID      CREATED       SIZE\ndocker.io/rockylinux/rockylinux  9           175264fac6da  6 months ago  181 MB\n</code></pre></p> <p>Running a simple command from the container</p> <p>We cover running containers in much more detail here, however below we will use the command <code>ch-run</code> to inspect the contents of the file <code>/etc/os-release</code> inside the container: podman run<pre><code>casper$  podman run rockylinux/rockylinux:9 cat /etc/os-release\nNAME=\"Rocky Linux\"\nVERSION=\"9.2 (Blue Onyx)\"\nID=\"rocky\"\nID_LIKE=\"rhel centos fedora\"\nVERSION_ID=\"9.2\"\nPLATFORM_ID=\"platform:el9\"\nPRETTY_NAME=\"Rocky Linux 9.2 (Blue Onyx)\"\nANSI_COLOR=\"0;32\"\nLOGO=\"fedora-logo-icon\"\nCPE_NAME=\"cpe:/o:rocky:rocky:9::baseos\"\nHOME_URL=\"https://rockylinux.org/\"\nBUG_REPORT_URL=\"https://bugs.rockylinux.org/\"\nSUPPORT_END=\"2032-05-31\"\nROCKY_SUPPORT_PRODUCT=\"Rocky-Linux-9\"\nROCKY_SUPPORT_PRODUCT_VERSION=\"9.2\"\nREDHAT_SUPPORT_PRODUCT=\"Rocky Linux\"\nREDHAT_SUPPORT_PRODUCT_VERSION=\"9.2\"\n</code></pre> This is functionally a <code>hello-world</code> type demonstration, and can be compared to the same file on the host to show we are indeed running in a different environment.</p> <p>Podman vs. Docker</p> <p>Podman seeks to be functionally equivalent with Docker, so many Docker commands you may be familiar with will work the same.</p>"},{"location":"environment-and-software/user-environment/containers/working_with_containers/#building-a-container-from-a-definition-file","title":"Building a container from a definition file","text":"<p>In the examples above, we pulled a ready-made container image.  For most practical applications we will want instead to build our own container image, often beginning with a base image from a public repository as shown above but extending it to meet a specific need.  This process begins with a \"recipe\" file listing the steps required.  By way of terminology, such recipes are typically referred to as <code>Dockerfiles</code> and usually follow a common format.  Charliecloud and Podman support <code>Dockerfiles</code> directly.  Apptainer is an outlier in this regard, and supports its own \"definition\" file format (commonly referred to as <code>def</code>-files).  In this section we describe the general form of these build recipe files and provide simple build examples for the supported run-times.</p>"},{"location":"environment-and-software/user-environment/containers/working_with_containers/#anatomy-of-build-recipes","title":"Anatomy of build recipes","text":"<p><code>Dockerfiles</code> and Apptainer <code>Definition</code> files</p> <code>Dockerfile</code>Apptainer <code>Definition</code> files <p>Following from the Docker documentation, a basic <code>Dockerfile</code> is Sample Dockerfile<pre><code>FROM rockylinux/rockylinux:9\n\nRUN yum -y install dnf-plugins-core \\\n    &amp;&amp; dnf -y update \\\n    &amp;&amp; dnf config-manager --set-enabled crb \\\n    &amp;&amp; dnf -y install epel-release \\\n    &amp;&amp; dnf -y groupinstall \"Development Tools\" \\\n    &amp;&amp; dnf -y install \\\n           chrpath \\\n           bzip2 autoconf automake libtool \\\n           gcc gcc-c++ gcc-gfortran emacs make procps-ng openmpi-devel \\\n    &amp;&amp; yum clean all\n\nRUN\n   [...]\n\nENV\n   [...]\n</code></pre></p> <p>Following from the Apptainer documentation, a basic definition file is Sample Definition File<pre><code>Bootstrap: docker\nFrom: docker.io/rockylinux/rockylinux:9\n\n%post\n    yum -y install dnf-plugins-core \\\n        &amp;&amp; dnf -y update \\\n        &amp;&amp; dnf config-manager --set-enabled crb \\\n        &amp;&amp; dnf -y install epel-release \\\n        &amp;&amp; dnf -y install gimp \\\n        &amp;&amp; dnf clean all --verbose\n\n%environment\n    [...]\n</code></pre></p> <p>We can now use the general form of these definition files to demonstrate constructing our own derived container image.</p> <p>Building a container from a recipe file</p> ApptainerCharliecloudPodman <p>Deffile:<pre><code>Bootstrap: docker\nFrom: docker.io/rockylinux/rockylinux:9\n\n%post\n    yum -y install dnf-plugins-core \\\n        &amp;&amp; dnf -y update \\\n        &amp;&amp; dnf config-manager --set-enabled crb \\\n        &amp;&amp; dnf -y install epel-release \\\n        &amp;&amp; dnf -y install xpdf \\\n        &amp;&amp; dnf clean all --verbose\n\n%environment\n    export MY_CONTANER_VAR=\"foo\"\n</code></pre> We use the command <code>singularity build</code> to create a compressed SIF directly from the <code>Deffile</code>: <pre><code>casper$ TMPDIR=/var/tmp/ singularity build my_rocky9.sif Deffile\n[...]\n</code></pre></p> <p>Dockerfile:<pre><code>FROM rockylinux/rockylinux:9\n\nRUN yum -y install dnf-plugins-core \\\n    &amp;&amp; dnf -y update \\\n    &amp;&amp; dnf config-manager --set-enabled crb \\\n    &amp;&amp; dnf -y install epel-release \\\n    &amp;&amp; dnf -y groupinstall \"Development Tools\" \\\n    &amp;&amp; dnf -y install \\\n           chrpath \\\n           bzip2 autoconf automake libtool \\\n           gcc gcc-c++ gcc-gfortran emacs make procps-ng openmpi-devel \\\n    &amp;&amp; dnf clean all --verbose\n</code></pre> We use the command <code>ch-image build</code> to build a container from the <code>Dockerfile</code>: <pre><code>casper$ ch-image build --force fakeroot --tag my_rocky9 .\n[...]\n</code></pre> Charliecloud builds in its internal format, which requires conversion before running.  As shown above, we will convert the image to our preferred SquashFS format: <pre><code>casper$ ch-image list\nmy_rocky9\nrockylinux/rockylinux:9\n\nbenkirk@casper20(61)$ ch-convert my_rocky9 ./my_rocky9.sqfs\ninput:   ch-image  my_rocky9\noutput:  squash    ./my_rocky9.sqfs\npacking ...\n[...]\n</code></pre></p> <p>Dockerfile:<pre><code>FROM docker.io/rockylinux/rockylinux:9\n\nRUN yum -y install dnf-plugins-core \\\n    &amp;&amp; dnf -y update \\\n    &amp;&amp; dnf config-manager --set-enabled crb \\\n    &amp;&amp; dnf -y install epel-release \\\n    &amp;&amp; dnf -y install xpdf \\\n    &amp;&amp; dnf clean all --verbose\n</code></pre> We use the command <code>podman build</code> to build a container from the <code>Dockerfile</code>: <pre><code>casper$ podman build --tag my_rocky9 .\n[...]\n</code></pre></p> <p>Container image builds directly on the HPC systems can be fragile</p> <p>As discussed previously, security concerns in the HPC environment restrict certain container image build operations that require elevated privileges.  Simple operations such as compiling code within a container to augment with a tool, or customizing the execution environment will likely work fine.  Additionally, installing most packages through an operating system package manager usually works as well.</p> <p>A common failure, however, is building containers that switch user IDs or change ownership of files within the build process.  This can occur explicitly through a <code>USER</code> statement or through installation of some package.  In either case, the underlying issue is that on the host the user has access to only a single user ID  - their own.  Many complex containers violate this restriction.  We cannot support such build processes securely, even with so-called \"rootless\" container installations.</p> <p>An alternative and popular workflow is to build containers externally to the HPC environment on a resource where the user has elevated privileges, likely using Docker.  The finalized images is then pushed to an image repository, and then pulled into the HPC environment.  We will not demonstrate the approach here due to the variability of external environments, however the process is straightforward for the user familiar with the build steps discussed below.</p>"},{"location":"environment-and-software/user-environment/containers/working_with_containers/#interaction-with-environment-variables","title":"Interaction with environment variables","text":"<p>Different container run-times differ greatly in how they handle environment variables set externally on the host. Docker (and by similarity Podman) usually runs containers in a \"clean\" environment, that is, variables set on the host do not exist inside the container.  Charliecloud by contrast passes nearly all environment variables from the host into the container.  Apptainer takes a middle ground, by default passing along host environment variables, but this behavior can be changed with command-line arguments.</p> <p>The Docker/Podman approach makes sense when containerizing small services or pieces of code that you want to run identically everywhere, independent of the host environment.  Conversely, the Apptainer &amp; Charliecloud approach makes more sense when many useful variables are already defined in the environment.  Regardless, at some point when working with containers you will undoubtedly find an issue that is traced either to a variable being inherited from the host when you don't want it to be, or missing in the container when you really want it.</p> <p>Each run-time  allows for environment variables to be set explicitly from the run command line.  Additionally, when building containers you can generally embed environment variables as well.  Again, this behavior is run-time dependent as shown in the following example.</p> <p>Container run-times and environment variables</p> <p>Host Environment</p> <p>We set the following variables on the host system prior to launching the container: <pre><code>export HOST_VAR=\"foo\"\nexport TOGGLE_VAR=\"set_from_OUTSIDE\"\nunset RANDOM_VAR\n</code></pre></p> <p>Container Environment</p> <p>We then construct a tiny container image (&lt;10MB) from the minimal Alpine Linux distribution. When we run the container it will report the value of several environment variables: <code>HOST_VAR</code>, <code>CONTAINER_VAR</code>, <code>TOGGLE_VAR</code>, and <code>RANDOM_VAR</code> (if set).</p> ApptainerCharliecloudPodman <p>The Apptainer Definition file <code>%environment</code> section allows us to define variables that exist inside the container. Definition File<pre><code>Bootstrap: docker\nFrom: docker.io/alpine:latest\n\n%files\n   ./extras/speak.sh /opt/container/speak.sh\n   ./extras/list_glade_filesystems.sh /opt/container/list_glade_filesystems.sh\n\n%post\n   chmod +x /opt/container/*.sh\n\n%environment\n    export CONTAINER_VAR=\"bar\"\n    export TOGGLE_VAR=\"set_from_INSIDE\"\n</code></pre> Container Environment Examples</p> <p>Running the container shows that <code>HOST_VAR</code> is passed in by default, and <code>TOGGLE_VAR</code> retains its value from inside the container definition unless explicitly passed via the <code>--env</code> argument. The argument <code>--cleanenv</code> prevents external variables from being passed. <pre><code># Step #1:\nhost$ singularity run ./my_alpine.sif /opt/container/speak.sh\n------------------------------\nHOST_VAR=foo\nCONTAINER_VAR=bar\nTOGGLE_VAR=set_from_INSIDE\n------------------------------\n\n# Step #2:\nhost$ singularity run --cleanenv ./my_alpine.sif /opt/container/speak.sh\n------------------------------\nHOST_VAR=\nCONTAINER_VAR=bar\nTOGGLE_VAR=set_from_INSIDE\n------------------------------\n\n# Step #3:\nhost$ singularity run --cleanenv --env TOGGLE_VAR=set_from_OUTSIDE ./my_alpine.sif /opt/container/speak.sh\n------------------------------\nHOST_VAR=\nCONTAINER_VAR=bar\nTOGGLE_VAR=set_from_OUTSIDE\n------------------------------\n\n# Step #4:\nhost$ singularity run --cleanenv --env RANDOM_VAR=set_on_command-line ./my_alpine.sif /opt/container/speak.sh\n------------------------------\nHOST_VAR=\nCONTAINER_VAR=bar\nTOGGLE_VAR=set_from_INSIDE\nRANDOM_VAR=set_on_command-line\n------------------------------\n</code></pre></p> <p>The Charliecloud <code>Dockerfile</code> <code>ENV</code> section allows us to define variables, but its use seems inconsistent in practice. Dockerfile<pre><code>FROM alpine:latest\n\nCOPY extras/speak.sh /opt/container/speak.sh\nCOPY extras/list_glade_filesystems.sh /opt/container/list_glade_filesystems.sh\n\nRUN chmod +x /opt/container/*.sh \\\n    &amp;&amp; mkdir -p /glade/u/home /glade/work /glade/campaign /glade/derecho/scratch /random/path\n\nENV CONTAINER_VAR=\"bar\" \\\n    TOGGLE_VAR=\"set_from_INSIDE\"\n</code></pre> Container Environment Examples</p> <p>Running the container shows that <code>HOST_VAR</code> is passed in by default, and <code>TOGGLE_VAR</code> takes its value from the host. The environment variables set from the <code>ENV</code> instruction are disregarded by default, but are honored when the <code>--set-env</code> arguments used. The argument <code>--set-env=VAR=val</code> allows variables to be passed to the container.   The argument <code>--unset-env</code> can be passed to prevent certain specified host variables from passing into the container, with <code>--unset-env=\"*\"</code> preventing any. <pre><code># Step #1:\nhost$ ch-run ./my_alpine.sqfs -- /opt/container/speak.sh\n------------------------------\nHOST_VAR=foo\nCONTAINER_VAR=\nTOGGLE_VAR=set_from_OUTSIDE\n------------------------------\n\n# Step #2:\nhost$ ch-run --set-env ./my_alpine.sqfs -- /opt/container/speak.sh\n------------------------------\nHOST_VAR=foo\nCONTAINER_VAR=bar\nTOGGLE_VAR=set_from_INSIDE\n------------------------------\n\n# Step #3:\nhost$ ch-run '--unset-env=*' ./my_alpine.sqfs -- /opt/container/speak.sh\n------------------------------\nHOST_VAR=\nCONTAINER_VAR=\nTOGGLE_VAR=\n------------------------------\n\n# Step #4:\nhost$ ch-run --set-env=TOGGLE_VAR=set_from_OUTSIDE ./my_alpine.sqfs -- /opt/container/speak.sh\n------------------------------\nHOST_VAR=foo\nCONTAINER_VAR=\nTOGGLE_VAR=set_from_OUTSIDE\n------------------------------\n\n# Step #5:\nhost$ ch-run --set-env=RANDOM_VAR=set_on_command-line ./my_alpine.sqfs -- /opt/container/speak.sh\n------------------------------\nHOST_VAR=foo\nCONTAINER_VAR=\nTOGGLE_VAR=set_from_OUTSIDE\nRANDOM_VAR=set_on_command-line\n------------------------------\n</code></pre> Discussion</p> <p>Charliecloud honors the <code>ENV</code> <code>Dockerfile</code> instruction at build-time but disregards it at run-time by default, the argument <code>--set-env</code> is required to regain access to these variables.</p> <p>The Podman <code>Dockerfile</code> <code>ENV</code> section allows us to define variables that exist inside the container. Dockerfile<pre><code>FROM docker.io/alpine:latest\n\nCOPY extras/speak.sh /opt/container/speak.sh\nCOPY extras/list_glade_filesystems.sh /opt/container/list_glade_filesystems.sh\n\nRUN chmod +x /opt/container/*.sh\n\nENV CONTAINER_VAR=\"bar\" \\\n    TOGGLE_VAR=\"set_from_INSIDE\"\n</code></pre> Container Environment Examples</p> <p>Running the container shows that <code>HOST_VAR</code> is not passed in by default, and <code>TOGGLE_VAR</code> retains its value from inside the container definition unless explicitly passed via the <code>--env</code> argument. <pre><code># Step #1:\nhost$ podman run my_alpine:latest /opt/container/speak.sh\n------------------------------\nHOST_VAR=\nCONTAINER_VAR=bar\nTOGGLE_VAR=set_from_INSIDE\n------------------------------\n\n# Step #2:\nhost$ podman run --env TOGGLE_VAR=set_from_OUTSIDE my_alpine:latest /opt/container/speak.sh\n------------------------------\nHOST_VAR=\nCONTAINER_VAR=bar\nTOGGLE_VAR=set_from_OUTSIDE\n------------------------------\n\n# Step #3:\nhost$ podman run --env RANDOM_VAR=set_on_command-line my_alpine:latest /opt/container/speak.sh\n------------------------------\nHOST_VAR=\nCONTAINER_VAR=bar\nTOGGLE_VAR=set_from_INSIDE\nRANDOM_VAR=set_on_command-line\n------------------------------\n</code></pre></p> <p>Summary</p> <p>Each container run-time allows you to pass environment variables in via command line arguments, and have different default behaviors with respect to host-defined variables.  Some allow you to set default values that exist inside the container.  Our best guidance is simply be aware of what is defined in your execution environment, pass critical values via command line arguments to avoid ambiguity, and perform error checking on environment variable values inside your image to be safe.</p> <p>Full definitions of the test cases can be found here.</p>"},{"location":"environment-and-software/user-environment/containers/working_with_containers/#accessing-host-file-systems","title":"Accessing host file systems","text":"<p>Similar to treatment of environment variables, each container run-time has unique behavior with respect to home and initial working directories inside containers.  By default all provide minimal access to the host file systems, however they allow for host directories to be \"bind mounted\" into the container upon request.  Following the same approach outlined above, we use our minimal container image to illustrate default and optional file system accessibility.</p> <p>Container run-times and mounting host file systems</p> ApptainerCharliecloudPodman <p>The <code>singularity</code> <code>--bind</code> option allows host directories to be bind-mounted into the container.</p> <p>As shown below, by default the users' home directory is mounted.  The initial working directory (<code>PWD</code>) behavior depends on where the container is launched from, and if that path has been bind-mounted into the container or not (Steps 1 &amp; 3).  The initial directory can be set explicitly with the <code>--pwd</code> flag (Step 2). <pre><code># Step #1:\nhost$ singularity run ./my_alpine.sif /opt/container/list_glade_filesystems.sh\n------------------------------\nHOME=/glade/u/home/benkirk\nPWD=/glade/u/home/benkirk\nglade_user              150.0T     77.7T     72.3T  52% /glade/u/home/benkirk\n------------------------------\n\n# Step #2:\nhost$ singularity run --pwd /opt/container ./my_alpine.sif /opt/container/list_glade_filesystems.sh\n------------------------------\nHOME=/glade/u/home/benkirk\nPWD=/opt/container\nglade_user              150.0T     77.7T     72.3T  52% /glade/u/home/benkirk\n------------------------------\n\n# Step #3:\nhost$ singularity run --bind /glade/u/home/benkirk --bind /glade/derecho/scratch --bind /glade/work --bind /glade/campaign --bind /glade/work:/random/path ./my_alpine.sif /opt/container/list_glade_filesystems.sh\n------------------------------\nHOME=/glade/u/home/benkirk\nPWD=/glade/work/benkirk/repos/csg-utils/hpc-demos/containers/tutorial/apptainer\nglade_user              150.0T     77.7T     72.3T  52% /glade/u/home/benkirk\nglade_user              150.0T     77.7T     72.3T  52% /glade/u/home/benkirk\n                         54.5P     10.7P     43.3P  20% /glade/derecho/scratch\ncsfs1                     4.0P    983.2T      3.0P  24% /glade/work\ncsfs1                   120.7P     93.5P     27.2P  77% /glade/campaign\ncsfs1                     4.0P    983.2T      3.0P  24% /random/path\n------------------------------\n</code></pre></p> <p>The <code>ch-run</code> <code>--bind</code> option allows host directories to be bind-mounted into the container.  Note that Charliecloud will not create new directories for the bind-mount locations, so we need to ensure they are created within our <code>Dockerfile</code>: Dockerfile<pre><code>FROM alpine:latest\n\nCOPY extras/speak.sh /opt/container/speak.sh\nCOPY extras/list_glade_filesystems.sh /opt/container/list_glade_filesystems.sh\n\nRUN chmod +x /opt/container/*.sh \\\n    &amp;&amp; mkdir -p /glade/u/home /glade/work /glade/campaign /glade/derecho/scratch /random/path\n\nENV CONTAINER_VAR=\"bar\" \\\n    TOGGLE_VAR=\"set_from_INSIDE\"\n</code></pre> As shown below, by default the users' home directory is mounted.  The initial working directory (<code>PWD</code>) is the container file system root (<code>/</code>). In Step 2 we bind some of the common GLADE file systems into the container using the directories created in the <code>Dockerfile</code>.  Step 3 fails, showing Charliecloud refusing to bind-mount into <code>/random/other</code> since no such mount point exists in the base image. <pre><code># Step #1:\nhost$ ch-run ./my_alpine.sqfs -- /opt/container/list_glade_filesystems.sh\n------------------------------\nHOME=/glade/u/home/benkirk\nPWD=/\n------------------------------\n\n# Step #2:\nhost$ ch-run --bind=/glade/derecho/scratch --bind=/glade/work --bind=/glade/campaign --bind=/glade/work:/random/path ./my_alpine.sqfs -- /opt/container/list_glade_filesystems.sh\n------------------------------\nHOME=/glade/u/home/benkirk\nPWD=/\n                         54.5P     10.7P     43.3P  20% /glade/derecho/scratch\ncsfs1                     4.0P    983.1T      3.0P  24% /glade/work\ncsfs1                   120.7P     93.5P     27.2P  77% /glade/campaign\ncsfs1                     4.0P    983.1T      3.0P  24% /random/path\n------------------------------\n\n# Step #3:\nhost$ ch-run --bind=/glade/derecho/scratch --bind=/glade/work --bind=/glade/campaign --bind=/glade/work:/random/other/path ./my_alpine.sqfs -- /opt/container/list_glade_filesystems.sh\nch-run[73181]: error: can't mkdir: /var/tmp/benkirk.ch/mnt/random/other: Read-only file system (ch_misc.c:409 30)\n</code></pre> Discussion</p> <p>Charliecloud will not create destination paths for bind-mounts inside the container.  Make sure you create such mount points when you build the image.</p> <p>The Podman <code>--volume</code> option allows host directories to be bind-mounted into the container.</p> <p>As shown below, by default the users' is placed into <code>/root</code> as a home directory,  The initial working directory (<code>PWD</code>) is the container file system root (<code>/</code>). <pre><code># Step #1:\nhost$ podman run my_alpine:latest /opt/container/list_glade_filesystems.sh\n------------------------------\nHOME=/root\nPWD=/\n------------------------------\n\n# Step #2:\nhost$ podman run --volume /glade/u/home/benkirk --volume /glade/derecho/scratch --volume /glade/work --volume /glade/campaign --volume /glade/work:/random/path my_alpine:latest /opt/container/list_glade_filesystems.sh\n------------------------------\nHOME=/root\nPWD=/\nmergedroot              188.3G      2.0G    186.3G   1% /glade/work\ncsfs1                     4.0P    983.1T      3.0P  24% /random/path\nmergedroot              188.3G      2.0G    186.3G   1% /glade/campaign\nmergedroot              188.3G      2.0G    186.3G   1% /glade/derecho/scratch\nmergedroot              188.3G      2.0G    186.3G   1% /glade/u/home/benkirk\n------------------------------\n</code></pre></p> <p>Summary</p> <p>Each run-time has a method for accessing host directories within the container.  They are all different with how they treat the users' home and initial working directory inside the container.  Do not assume a given behavior, rather be explicit with changing directories and specifying full paths as necessary.</p> <p>Full definitions of the test cases can be found here.</p>"},{"location":"environment-and-software/user-environment/containers/working_with_containers/#running-containerized-mpi-applications","title":"Running containerized MPI applications","text":"<p>The interaction between MPI implementations and container run-times is unfortunately the single biggest pain point of running containers in the HPC environment.  If you're drawn to the \"simplicity\" promised by containerization but need to run on multiple nodes with MPI, we strongly encourage you to fully consider this section before going further. (The issues are well described here.) First let us distinguish two use cases:</p> <ol> <li> <p>Running MPI inside a container on a single node, using an MPI stack defined within the container:  While somewhat limited, this case is fairly easy to handle since the complexities of interfacing with a high-speed network are largely eliminated.</p> </li> <li> <p>Running MPI across multiple nodes, launching the run-time with MPI from the host:  This is the general case, and also where complications arise.  This is the focus of the remainder of this section.</p> </li> </ol> <p>When using MPI on the host to launch an MPI application that was compiled within the container, it is imperative the host and container MPI implementations be compatible.  In practice this means the pair should be from the same implementation (e.g. OpenMPI, or MPICH) and at similar versions - and the closer the versions the better. This means the container image can rarely be created without knowledge of the execution host environment.</p> <p>For Casper, where we deploy OpenMPI by default, this is not too terribly complicated since most containerized operating systems can easily install similar versions. For Derecho, however, the default MPI is Cray's MPICH, which is proprietary and therefore difficult in general to install into an arbitrary container image.  In this case we must choose particular versions of MPI for the container image, knowing in advance they share heritage with the target host system.  This allows us to build MPI applications inside the container with a compatible - but readily available -  MPI.</p> <p>We then follow the \"bind model\" approach when running the container in order to \"replace\" the container MPI with the host MPI, gaining access to the high-speed network and vendor MPI optimizations.  For a full demonstration of this process, see our containerized FastEddy example.</p>"},{"location":"environment-and-software/user-environment/containers/working_with_containers/#running-containerized-gpu-applications","title":"Running containerized GPU applications","text":"<p>Many GPU compute capabilities are directly available within containers to applications. CUDA, OpenMP and OpenACC offload codes will generally work without any special consideration.</p> <p>An exception is interfacing directly with the kernel driver.  If you require such functionality, Apptainer provides the easiest support path through its <code>--nv</code> command line argument.  See the Apptainer GPU documentation page for more details. <pre><code># running nvidia-smi from within the container, try 1:\ncasper$ singularity exec --cleanenv ./my_cuda_container.sif nvidia-smi\nFailed to initialize NVML: Driver/library version mismatch\n\n# running nvidia-smi from within the container, try 2:\ncasper$ singularity exec --nv --cleanenv ./my_cuda_container.sif nvidia-smi\nNVIDIA-SMI 545.23.06              Driver Version: 545.23.06    CUDA Version: 12.3     |\n|-----------------------------------------+----------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n|                                         |                      |               MIG M. |\n|=========================================+======================+======================|\n|   0  Tesla V100-SXM2-32GB           On  | 00000000:89:00.0 Off |                    0 |\n| N/A   30C    P0              39W / 300W |      0MiB / 32768MiB |      0%      Default |\n|                                         |                      |                  N/A |\n+-----------------------------------------+----------------------+----------------------+\n[...]\n</code></pre></p>"},{"location":"environment-and-software/user-environment/containers/working_with_containers/#sharing-a-container","title":"Sharing a container","text":"<p>Under Development</p> <p>The best practices for sharing container images is currently under development.  The availability and pricing models of external repositories is frequently changing, complicating a general recommendation.</p> <p>NCAR does not currently offer a custom centralized image repository for HPC user access.</p> <p>For sharing on the HPC systems, we currently recommend revision-controlled build processes and sharing resulting compressed imaged files directly.  Alternatively, a popular model for external container building is to push the resulting images to Docker Hub, where they can be pulled into the HPC environment using the techniques outlined above.</p>"},{"location":"esds/faq/","title":"Frequently Asked Questions","text":"<p>A copy of ESDS FAQ page: https://ncar.github.io/esds/faq/</p> <p>This contains relevant questions and answers from common workflow issues and questions posted on Zulip.</p> <p>Note</p> <p>This page is meant to be a list of FAQ regarding climate datasets, movivated by a variety of employees across UCAR/NCAR.</p>"},{"location":"esds/faq/#i-need-help-with-this","title":"I need help with this!","text":""},{"location":"esds/faq/#where-do-i-go-for-help","title":"Where do I go for help?","text":"<p>Try one of the following resources.</p> <ol> <li>Xarray's How Do I do X? page</li> <li>Xarray Github Discussions</li> <li>Pangeo Discourse Forum</li> <li>NCAR Zulip under #python-questions, #python-dev, or #dask.</li> </ol> <p>Avoid personal emails and prefer a public forum.</p>"},{"location":"esds/faq/#what-do-i-do-if-my-question-is-not-answered-on-this-page","title":"What do I do if my question is not answered on this page?","text":"<p>If your question is related to conda environments and you're affiliated with UCAR/NCAR, you can open a help ticket on the NCAR Research Computing Helpdesk site. If your issue is related to data science packages and workflows, you can open an issue on our GitHub here or book an office hour appointment with an ESDS core member!</p>"},{"location":"esds/faq/#someone-must-have-written-the-function-i-want-where-do-i-look","title":"Someone must have written the function I want. Where do I look?","text":"<p>See the xarray ecosystem page. Also see the xarray-contrib and pangeo-data organizations. Some NCAR relevant projects include:</p> <ol> <li>GeoCAT-comp</li> <li>GeoCAT-viz</li> <li>cf_xarray</li> <li>climpred</li> <li>eofs</li> <li>MetPy</li> <li>rechunker</li> <li>xclim</li> <li>xesmf</li> <li>xgcm</li> <li>pop-tools</li> <li>xskillscore</li> </ol>"},{"location":"esds/faq/#how-do-i-use-conda-environments","title":"How do I use conda environments?","text":""},{"location":"esds/faq/#general-advice","title":"General Advice","text":"<p>Dealing with Python environments can be tricky... a good place to start is to checkout this guide on dealing with Python environments. If you just need a refresher on the various conda commands, this conda cheet sheet is a wonderful quick reference.</p>"},{"location":"esds/faq/#using-conda-on-ncar-hpc-resources","title":"Using conda on NCAR HPC resources","text":"<p>Warning</p> <p>Since 12 December 2022, it is no longer recommended to install your own version of miniconda on the HPC system.   To export your existing environments to the recommended installation of miniconda, refer to the \"How can I export my environments?\" section.</p> <p>The NCAR High Performance Computing (HPC) system has a conda installation for you to use. The most recent and detailed instructions can be found on this Using Conda and Python page.</p> <p>If you don't want the trouble of making your own conda environment, there are managed environments available. The NCAR Package Library (NPL) is an environment containing many common scientific Python packages such as Numpy, Xarray, and GeoCAT. You can access the NPL environment through the command line and the NCAR JupyterHub.</p>"},{"location":"esds/faq/#npl-on-the-command-line","title":"NPL on the command line","text":"<ol> <li>Open up a terminal in Casper or Cheyenne</li> <li>Load the NCAR conda module:</li> </ol> <pre><code>$ module load conda/latest\n</code></pre> <ol> <li>List the available NCAR managed environments:</li> </ol> <pre><code>$ conda env list\n\n   base                  *  /glade/u/apps/opt/conda\n   npl                      /glade/u/apps/opt/conda/envs/npl\n   npl-2022b                /glade/u/apps/opt/conda/envs/npl-2022b\n   npl-2206                 /glade/u/apps/opt/conda/envs/npl-2206\n   npl-2207                 /glade/u/apps/opt/conda/envs/npl-2207\n   pygpu-dask               /glade/u/apps/opt/conda/envs/pygpu-dask\n</code></pre> <ol> <li>Activate the environment you want to use. Here we are using the <code>npl</code> environment as an example. <code>npl</code> can be replaced    with any available environment name:</li> </ol> <pre><code>$ conda activate npl\n</code></pre> <ol> <li>Now when you run a script, the modules within the <code>npl</code> environment will be available to your program.</li> </ol>"},{"location":"esds/faq/#npl-on-the-ncar-jupyterhub","title":"NPL on the NCAR JupyterHub","text":"<ol> <li>Log in to the Production NCAR JupyterHub</li> <li>Start a server</li> <li>With your Jupyter Notebook open, click on the kernel name in the upper right.    </li> <li>A dialog will appear with all the various kernels available to you. These kernels will (generally) have the same    name as the conda environment that it uses. This may not be the case if you are managing your own environments and kernels.    </li> <li>Select the \"npl (conda)\" kernel from the list if you want to use the NCAR-managed NPL environment.    </li> </ol>"},{"location":"esds/faq/#creating-and-accessing-a-new-conda-environment-on-the-ncar-jupyterhub","title":"Creating and accessing a new conda environment on the NCAR JupyterHub","text":"<p>You may want to move past using NPL, and create a new conda environment! For detailed instructions, check out the Using Conda and Python page on the NCAR Advanced Research Computing site. Heres a summary of the basic steps:</p> <ol> <li>Create the environment</li> </ol> <p>If you are creating an environment from scratch, use the following:</p> <pre><code>conda create --name my_environment\n</code></pre> <p>where <code>my_environment</code> is the name of your environment</p> <p>Ff you have an environment file (ex. <code>environment.yml</code>), use the following:</p> <pre><code>conda env create -f environment.yml\n</code></pre> <ol> <li>Activate your environment and install the <code>ipykernel</code> package</li> </ol> <pre><code>conda activate my_environment.yml\nconda install ipykernel\n</code></pre> <pre><code>The [`ipykernel`](https://github.com/ipython/ipykernel) package is required for your environment to be available from the NCAR [JupyterHub](https://jupyterhub.hpc.ucar.edu/)\n</code></pre> <ol> <li>Accessing your conda environment</li> </ol> <p>Your environment should now automatically show up as an available kernel in any Jupyter server on the NCAR HPC systems.    If you want to give your kernel a name that is different from the environment name, you can use the following command:</p> <pre><code>python -m ipykernel install --user --name=my-kernel\n</code></pre> <p>Where <code>my-kernel</code> is the kernel name.</p>"},{"location":"esds/faq/#conda-is-taking-too-long-to-solve-environment-use-mamba","title":"Conda is taking too long to solve environment: use mamba","text":"<p>This is a very common issue when installing a new package or trying to update a package in an existing conda environment. This issue is usually manifested in a conda message along these lines:</p> <pre><code>environment Solving environment: failed with initial frozen solve. Retrying with flexible solve.\n</code></pre> <p>One solution to this issue is to use <code>mamba</code> which is a drop-in replacement for conda. Mamba aims to greately speed up and improve conda functionality such as solving environment, installing packages, etc...</p> <ul> <li>Installing Mamba</li> </ul> <pre><code>conda install -n base -c conda-forge mamba\n</code></pre> <ul> <li>Set <code>conda-forge</code> and <code>nodefaults</code> channels</li> </ul> <pre><code>conda config --add channels nodefaults\nconda config --add channels conda-forge\n</code></pre> <ul> <li>To install a package with mamba, you just run</li> </ul> <pre><code>mamba install package_name\n</code></pre> <ul> <li>To create/update an environment from an environment file, run:</li> </ul> <pre><code>mamba env update -f environment.yml\n</code></pre> <pre><code>We do not recommend using `mamba` to activate and deactivate environments as this can cause packages to misbehave/not load correctly.\n</code></pre> <p>See mamba documentation for more.</p>"},{"location":"esds/faq/#how-can-i-export-my-environments","title":"How can I export my environments?","text":"<p>If you made an environment on one machine or using a different conda installation, you can export that environment and use it elsewhere. These are the basic steps:</p> <ol> <li>Export your environment</li> </ol> <p>With the environment you want to export activated, run the following command:</p> <pre><code>conda env export --from-history &gt; environment.yml\n</code></pre> <p>where <code>environment</code> can be replaced with the file name of your choice. The <code>--from-history</code> flag allows you to    recreate your environment on any system. It is the cross-platform compatible way of exporting an environment.</p> <ol> <li> <p>Move the <code>environment.yml</code> to the system you want to use it on / activate the appropriate conda installation you wish to use.</p> </li> <li> <p>Use the <code>.yml</code> file to create your environment    <pre><code>conda env create -f environment.yml\n</code></pre></p> </li> </ol>"},{"location":"esds/faq/#xarray-and-dask","title":"Xarray and Dask","text":""},{"location":"esds/faq/#general-tips","title":"General tips","text":"<ol> <li>Read the xarray documentation on optimizing workflow with dask.</li> <li>Read the Best practices for dask array</li> <li>Keep track of chunk sizes throughout your workflow. This is especially important when reading in data using <code>xr.open_mfdataset</code>. Aim for 100-200MB size    chunks.</li> <li>Choose chunking appropriate to your analysis. If you're working with time series then chunk more in space and less along time.</li> <li>Avoid indexing with <code>.where</code> as much as possible. In particulate <code>.where(..., drop=True)</code> will trigger a compute since it needs    to know where NaNs are present to drop them. Instead see if you can write your statement as a <code>.clip</code>, <code>.sel</code>, <code>.isel</code>, or    <code>.query</code> statement.</li> </ol>"},{"location":"esds/faq/#how-do-i-optimize-reading-multiple-files-using-xarray-and-dask","title":"How do I optimize reading multiple files using Xarray and Dask?","text":"<p>A good first place to start when reading in multiple files is Xarray's multi-file documentation.</p> <p>For example, if you are trying to read in multiple files where you are interested in concatenating over the time dimension, here is an example of the <code>xr.open_dataset</code> line would look like:</p> <pre><code>ds = xr.open_mfdataset(\n        files,\n        # Name of the dimension to concatenate along.\n        concat_dim=\"time\",\n\n        # Attempt to auto-magically combine the given datasets into one by using dimension coordinates.\n        combine=\"by_coords\",\n\n        # Specify chunks for dask - explained later\n        chunks={\"lev\": 1, \"time\": 500},\n\n        # Only data variables in which the dimension already appears are included.\n        data_vars=\"minimal\",\n\n        # Only coordinates in which the dimension already appears are included.\n        coords=\"minimal\",\n\n        # Skip comparing and pick variable from first dataset.\n        compat=\"override\",\n        parallel=True,\n    )\n</code></pre>"},{"location":"esds/faq/#where-can-i-find-xarray-tutorials","title":"Where can I find Xarray tutorials?","text":"<p>See videos and notebooks.</p>"},{"location":"esds/faq/#how-do-i-debug-my-code-when-using-dask","title":"How do I debug my code when using dask?","text":"<p>An option is to use <code>.compute(scheduler=\"single-threaded\")</code>. This will run your code as a serial for loop. When an error is raised you can use the <code>%debug</code> magic to drop in to the stack and debug from there. See this post for more debugging tips in a serial context.</p>"},{"location":"esds/faq/#killedworker-x-what-do-i-do","title":"<code>KilledWorker</code> X{. What do I do?","text":"<p>Keep an eye on the dask dashboard.</p> <ol> <li>If a lot of the bars in the Memory tab are orange, that means your workers are running out of memory. Reduce your chunk size.</li> </ol>"},{"location":"esds/faq/#help-my-analysis-is-slow","title":"Help, my analysis is slow!","text":"<ol> <li>Try subsetting for just the variable(s) you need for example, if you are reading in a dataset with ~25 variables, and you only need <code>temperature</code>, just read in temperature. You can specify which variables to read in by using the following syntax, following the example of the temperature variable.</li> </ol> <pre><code>ds = xr.open_dataset(file, data_vars=['temperature'])\n</code></pre> <ol> <li>Take a look at your chunk size, it might not be optimized. When reading a file in using Xarray with Dask, a \"general rule of thumb\" is to keep your chunk size down to around 100 mb.</li> </ol> <p>For example, let's say you trying to read in multiple files, each with ~600 time steps. This is case where each file is very large (several 10s of GB) and using Dask to help with data processing is essential.</p> <p>You can check the size of each chunk by subsetting a single DataArray (ex. <code>ds['temperature']</code>)</p> <p>If you have very large chunks, try modifying the number of chunks you specify within <code>xr.open_mfdataset(files, ..., chunks={'lev':1, \"time\": 500})</code> where lev and time are vertical and time dimensions respectively.</p> <p>Check to see how large each chunk is after modifying the chunk size, and modify as necessary.</p> <ol> <li>You do not have enough dask workers</li> </ol> <p>If you have a few large files, having the number of workers equal to to the number of input files read in using <code>xr.open_mfdataset</code> would be a good practice</p> <p>If you have a large number of smaller files, you may not run into this issue, and it is suggest you look at the other potential solutions.</p>"},{"location":"esds/faq/#i-have-to-do-lots-of-rechunking-but-the-rechunk-step-uses-too-much-memory-and-kills-my-workers","title":"I have to do lots of rechunking, but the rechunk step uses too much memory and kills my workers.","text":"<p>Try the rechunker package.</p>"},{"location":"esds/faq/#writing-to-files-in-parallel","title":"Writing to files in parallel","text":"<p>Distributed writes to netCDF are hard.</p> <ol> <li>Try writing to <code>zarr</code> using <code>Dataset.to_zarr</code>.</li> <li>If you need to write to netCDF and your final dataset can fit in memory then use <code>dataset.load().to_netcdf(...)</code>.</li> <li>If you really must write a big dataset to netCDF try using <code>save_mfdataset</code> (see here).</li> </ol>"},{"location":"esds/faq/#my-dask-workers-are-taking-a-long-time-to-start-how-can-i-monitor-them","title":"My Dask workers are taking a long time to start. How can I monitor them?","text":"<p>Dask worker requests are added to the job queues on Casper and Cheyenne with the <code>cluster.scale()</code> method. After this method is called, you can verify that they are waiting in the queue with this command:</p> <ul> <li><code>qstat -u &lt;my_username&gt;</code> on Cheyenne, and the same command will work on Casper after April 2021.</li> </ul> <p>If you see no pending worker jobs, then verify that you have called <code>cluster.scale()</code>.</p>"},{"location":"esds/faq/#github","title":"Github","text":""},{"location":"esds/faq/#setting-up-github-authentication","title":"Setting up Github Authentication","text":"<p>Beginning August 13, 2021, Github will no longer accept account passwords when authenticating git operations. There are essentially two options, which Github provides proper documentation for getting setup:</p> <ol> <li>Setup two-factor authentication</li> <li>Connect to Github via SSH</li> </ol>"},{"location":"esds/faq/#cesm-data","title":"CESM Data","text":""},{"location":"esds/faq/#dealing-with-cesm-monthly-output-is-there-something-wrong-with-time","title":"Dealing with CESM monthly output - is there something wrong with time","text":"<p>A well known issue of CESM data is that timestamps for fields saved as averages are placed at the end of the averaging period. For instance, in the following example, the <code>January/1920</code> average has a timestamp of <code>February/1920</code>:</p> <pre><code>In [25]: filename = '/glade/collections/cdg/data/cesmLE/CESM-CAM5-BGC-LE/atm/proc/tseries/monthly/TS/b.e11.B20TRC5CNBDRD.f09_g16.011.cam.h0.TS.192001-200512.nc'\n\nIn [33]: ds = xr.open_dataset(filename)\n\nIn [34]: ds.time\nOut[34]:\n&lt;xarray.DataArray 'time' (time: 1032)&gt;\narray([cftime.DatetimeNoLeap(1920, 2, 1, 0, 0, 0, 0),\n       cftime.DatetimeNoLeap(1920, 3, 1, 0, 0, 0, 0),\n       cftime.DatetimeNoLeap(1920, 4, 1, 0, 0, 0, 0), ...,\n       cftime.DatetimeNoLeap(2005, 11, 1, 0, 0, 0, 0),\n       cftime.DatetimeNoLeap(2005, 12, 1, 0, 0, 0, 0),\n       cftime.DatetimeNoLeap(2006, 1, 1, 0, 0, 0, 0)], dtype=object)\nCoordinates:\n  * time     (time) object 1920-02-01 00:00:00 ... 2006-01-01 00:00:00\nAttributes:\n    long_name:  time\n    bounds:     time_bnds\n</code></pre> <p>A temporary workaround is to fix the issue ourselves by computing new time axis by averaging the time bounds:</p> <pre><code>In [29]: import xarray as xr\n\nIn [30]: import cf_xarray # use cf-xarray so that we can use CF attributes\n\nIn [31]: filename = '/glade/collections/cdg/data/cesmLE/CESM-CAM5-BGC-LE/atm/proc/tseries/monthly/TS/b.e11.B20TRC5CNBDRD.f09_g16.011.cam.h0.TS.192001-200512.nc'\n\nIn [32]: ds = xr.open_dataset(filename)\n\nIn [34]: attrs, encoding = ds.time.attrs.copy(), ds.time.encoding.copy()\n\nIn [36]: time_bounds = ds.cf.get_bounds('time')\n\nIn [37]: time_bounds_dim_name = ds.cf.get_bounds_dim_name('time')\n\nIn [38]: ds = ds.assign_coords(time=time_bounds.mean(time_bounds_dim_name))\n\nIn [39]: ds.time.attrs, ds.time.encoding = attrs, encoding\n\nIn [40]: ds.time\nOut[40]:\n&lt;xarray.DataArray 'time' (time: 1032)&gt;\narray([cftime.DatetimeNoLeap(1920, 1, 16, 12, 0, 0, 0),\n       cftime.DatetimeNoLeap(1920, 2, 15, 0, 0, 0, 0),\n       cftime.DatetimeNoLeap(1920, 3, 16, 12, 0, 0, 0), ...,\n       cftime.DatetimeNoLeap(2005, 10, 16, 12, 0, 0, 0),\n       cftime.DatetimeNoLeap(2005, 11, 16, 0, 0, 0, 0),\n       cftime.DatetimeNoLeap(2005, 12, 16, 12, 0, 0, 0)], dtype=object)\nCoordinates:\n  * time     (time) object 1920-01-16 12:00:00 ... 2005-12-16 12:00:00\nAttributes:\n    long_name:  time\n    bounds:     time_bnds\n</code></pre> <pre><code>cf-xarray can be installed via pip or conda. cf-xarray docs are available [here](https://cf-xarray.readthedocs.io/en/latest/).\n</code></pre>"},{"location":"getting-started/","title":"Getting started with NCAR HPC Resources","text":"<p>About this page</p> <p>This document will guide you through the basics of using NCAR's supercomputers, storage systems, and services.</p> <p>Once you are authorized to use NCAR compute and storage resources, and you have an account and the necessary software, you can follow the procedures described below to log in.</p> <p>These pages provide information on compiling your code, submitting jobs, and performing other common tasks on all NCAR resources unless otherwise noted:</p> <ul> <li>Compiling Code on Derecho or Casper</li> <li>Understanding and Customizing your User and Software Environment</li> <li>Starting and Managing Jobs with PBS</li> <li>Managing Your Resource Allocation</li> </ul> <p>Don\u2019t run <code>sudo</code> on NCAR systems!</p> <p>If you need help with tasks that you think require <code>sudo</code> privileges, or if you aren\u2019t sure, please contact HPC User Support before trying to run <code>sudo</code> yourself. The command fails when unauthorized users run it and sends a security alert to system administrators.</p>"},{"location":"getting-started/#logging-in","title":"Logging In","text":"<p>To log in, start your terminal or Secure Shell client and run an <code>ssh</code> command as shown here:</p> DerechoCasperCheyenne <pre><code>ssh -X username@derecho.hpc.ucar.edu\n</code></pre> <pre><code>ssh -X username@casper.hpc.ucar.edu\n</code></pre> <pre><code>ssh -X username@cheyenne.ucar.edu\n</code></pre> <p>After running the <code>ssh</code> command, you will be asked to authenticate to finish logging in.</p> <p>The <code>-X</code> is optional and requests simple <code>X11</code> graphics forwarding to your client.  You can omit <code>username</code> in the command above if your Casper username is the same as your  username on your local computer.</p> <p>Tip</p> <p>Some users (particularly on Macs) need to use -Y instead of -X when calling ssh to enable X11 forwarding.</p>"},{"location":"getting-started/#new-user-resources","title":"New User Resources","text":"<ul> <li>New User Orientation</li> <li>New User Training for HPC Systems</li> <li>Getting Started on Derecho</li> <li>Getting Started on Casper</li> </ul>"},{"location":"getting-started/acknowledging-ncar-and-cisl/","title":"Acknowledging NCAR and CISL","text":"<p>A requirement of all allocations and use of NCAR HPC resources managed by CISL, including the CMIP Analysis Platform and Research Data Archive, is to acknowledge NCAR and CISL support for your research. Our ability to identify supported scientific results helps ensure continued support from NSF and other sources for future HPC systems.</p> <p>Upon completion of your project, NCAR requires an accomplishment report containing:</p> <ol> <li>The research results obtained using NCAR resources,</li> <li>a list of scientific publications that resulted from this research,</li> <li>the names and affiliations of graduate students who used NCAR resources, and</li> <li>the title, author, and relevant citation data for any theses or dissertations produced using NCAR computational resources.</li> </ol> <p>The accomplishment report should be sent as a PDF to alloc@ucar.edu.</p> <p>Note</p> <p>A PI\u2019s or project lead\u2019s subsequent allocations may be delayed pending compliance with these reporting requirements.</p> <p>More details follow below for Derecho, Cheyenne, Yellowstone, and CMIP Analysis Platform projects, and for the use of Research Data Archive data sets.</p>"},{"location":"getting-started/acknowledging-ncar-and-cisl/#for-derecho-projects","title":"For Derecho projects","text":"<p>If you are publishing a work that uses the Derecho resource, follow the guidance for Cheyenne below, but use the appropriate calendar year, \"Derecho: HPE Cray EX System\" for the system name, and the Derecho DOI (https://doi.org/10.5065/qx9a-pg09).</p>"},{"location":"getting-started/acknowledging-ncar-and-cisl/#for-cheyenne-projects","title":"For Cheyenne projects","text":""},{"location":"getting-started/acknowledging-ncar-and-cisl/#citation-formats","title":"Citation formats","text":"<p>We prefer that you cite your use of the Cheyenne system (and/or the Casper data analysis and visualization cluster) with the following citation formats, based on the source of your allocation and modified as needed to conform with citation style guidelines. Be sure to use the digital object identifier (DOI) as shown; it is unique to Cheyenne.</p> <ol> <li> <p>Computational and Information Systems Laboratory. 2019. Cheyenne: HPE/SGI ICE XA System (Climate Simulation Laboratory). Boulder, CO: National Center for Atmospheric Research. doi:10.5065/D6RX99HX.</p> </li> <li> <p>Computational and Information Systems Laboratory. 2019. Cheyenne: HPE/SGI ICE XA System (University Community Computing). Boulder, CO: National Center for Atmospheric Research. doi:10.5065/D6RX99HX.</p> </li> <li> <p>Computational and Information Systems Laboratory. 2019. Cheyenne: HPE/SGI ICE XA System (NCAR Community Computing). Boulder, CO: National Center for Atmospheric Research. doi:10.5065/D6RX99HX.</p> </li> <li> <p>Computational and Information Systems Laboratory. 2019. Cheyenne: HPE/SGI ICE XA System (Wyoming-NCAR Alliance). Boulder, CO: National Center for Atmospheric Research. doi:10.5065/D6RX99HX.</p> </li> </ol>"},{"location":"getting-started/acknowledging-ncar-and-cisl/#acknowledgments","title":"Acknowledgments","text":"<p>Acknowledgments are more difficult to track electronically due to the many possible variations, but you may choose to acknowledge support from Cheyenne if the citation method described above is inappropriate. You may modify the following examples as appropriate; however, please include at least the DOI in the acknowledgment text, as you would include funding agency award numbers.</p>"},{"location":"getting-started/acknowledging-ncar-and-cisl/#for-universitychap-and-ncar-allocations","title":"For University/CHAP and NCAR allocations","text":"<p>We would like to acknowledge high-performance computing support from Cheyenne (doi:10.5065/D6RX99HX) provided by NCAR's Computational and Information Systems Laboratory, sponsored by the National Science Foundation.</p>"},{"location":"getting-started/acknowledging-ncar-and-cisl/#for-climate-simulation-lab-csl-allocations","title":"For Climate Simulation Lab (CSL) allocations","text":"<p>Computing resources (doi:10.5065/D6RX99HX) were provided by the Climate Simulation Laboratory at NCAR's Computational and Information Systems Laboratory, sponsored by the National Science Foundation and other agencies.</p>"},{"location":"getting-started/acknowledging-ncar-and-cisl/#for-wyoming-ncar-alliance-allocations","title":"For Wyoming-NCAR Alliance allocations","text":"<p>We would like to acknowledge the use of computational resources (doi:10.5065/D6RX99HX) at the NCAR-Wyoming Supercomputing Center provided by the National Science Foundation and the State of Wyoming, and supported by NCAR's Computational and Information Systems Laboratory.</p>"},{"location":"getting-started/acknowledging-ncar-and-cisl/#for-yellowstone-projects","title":"For Yellowstone projects","text":"<p>If you are publishing a work that used the Yellowstone resource, follow the guidance for Cheyenne, but use 2016 for the citation year, \"Yellowstone: IBM iDataPlex System\" for the system name, and the Yellowstone ARK (http://n2t.net/ark:/85065/d7wd3xhc) for the DOI.</p>"},{"location":"getting-started/acknowledging-ncar-and-cisl/#for-cmip-analysis-platform-projects","title":"For CMIP Analysis Platform projects","text":"<p>See this CMIP Analysis Platform documentation for how to acknowledge NCAR/CISL support and for information on other requirements related to the use of CMIP data.</p>"},{"location":"getting-started/acknowledging-ncar-and-cisl/#for-use-of-rda-data-sets","title":"For use of RDA data sets","text":"<p>The data were provided by the Research Data Archive (RDA) of the Computational and Information Systems Laboratory at the National Center for Atmospheric Research. NCAR is supported by grants from the National Science Foundation.</p> <p>We ask that you cite the dataset(s) used. Citations templates are provided on the relevant RDA dataset home pages. For an example, see the \"How to Cite This Dataset\" section on https://rda.ucar.edu/datasets/ds094.0/citation.</p>"},{"location":"getting-started/best-practices-for-supercomputer-users/","title":"Best practices for supercomputer users","text":"<p>The practices described below will help you make the most of your computing and storage allocations.</p> <p></p>"},{"location":"getting-started/best-practices-for-supercomputer-users/#using-shared-resources","title":"Using shared resources","text":"<p>Be considerate of others in the user community when you work with these shared computing and storage resources. Here are a few key issues to keep in mind.</p>"},{"location":"getting-started/best-practices-for-supercomputer-users/#use-login-nodes-only-for-their-intended-purposes","title":"Use login nodes only for their intended purposes","text":"<p>You can run short, non-memory-intensive processes on the login nodes. These include tasks such as text editing or running small serial scripts or programs. Memory-intensive processes that slow login node performance for all users are killed automatically and the responsible parties are notified by email. See Appropriate use of login nodes0 for more information.</p>"},{"location":"getting-started/best-practices-for-supercomputer-users/#use-the-derecho-and-casper-nodes-that-best-meet-your-needs","title":"Use the Derecho and Casper nodes that best meet your needs","text":"<p>The Derecho system and Casper nodes are configured for distinct purposes. Derecho is best used for running climate and weather models and simulations while the Casper cluster of nodes is for other specialized work. Most Casper nodes are used for analyzing and visualizing data while others feature large-memory, dense GPU configurations that support explorations in machine learning and deep learning.</p> <p>This documentation explains how to get jobs running on the most appropriate system for your work and on the individual types of nodes that will best meet your needs:</p> <ul> <li>Derecho Overview</li> <li>Casper Overview</li> <li>Submitting jobs with PBS<ul> <li>Sample Derecho PBS job scripts</li> <li>Sample Casper PBS job scripts</li> </ul> </li> </ul> <p>For expert assistance or guidance in using these resources, contact the NCAR Research Computing help desk.</p>"},{"location":"getting-started/best-practices-for-supercomputer-users/#dont-monopolize-compute-resources","title":"Don't monopolize compute resources","text":"<p>Consider what impact you might have on the work of others and schedule jobs accordingly. For example, avoid writing job submission scripts that rapidly fill the scheduler with potentially concurrent compute resource requests. Contact the Consulting Services Group for guidance if your workload requires you to submit numerous jobs in a short time frame. CISL monitors the use of these resources and will kill jobs when necessary to ensure fair access for all users.</p>"},{"location":"getting-started/best-practices-for-supercomputer-users/#limit-your-use-of-shared-licenses","title":"Limit your use of shared licenses","text":"<p>Users share a limited number of licenses for running IDL, MATLAB, Mathematica, and some other applications. Be familiar with and follow the established license-use guidelines to ensure fair access for all users. CISL reserves the right to kill jobs/tasks of users who monopolize these licenses.</p>"},{"location":"getting-started/best-practices-for-supercomputer-users/#managing-allocations","title":"Managing allocations","text":""},{"location":"getting-started/best-practices-for-supercomputer-users/#monitor-usage-charges","title":"Monitor usage charges","text":"<p>Check your usage charges frequently to help ensure that you are using CISL resources as efficiently as possible. Also make sure that others who are authorized to charge against your allocation understand how to use them efficiently. Understand how your choice of queues affects charges against your computing allocation and be aware of other allocation-related policies. See Managing your allocation.</p> <p>If you are authorized to charge your work against multiple projects, check your usage charges and storage holdings for each project on a regular basis. This will help ensure that you are charging jobs correctly and help you avoid overspending your allocations.</p>"},{"location":"getting-started/best-practices-for-supercomputer-users/#optimize-on-a-single-processor","title":"Optimize on a single processor","text":"<p>Minimize your use of computing resources and conserve your allocation by optimizing your code on a single processor before running larger jobs in production. Use optimizing libraries if your code lends itself to that.</p>"},{"location":"getting-started/best-practices-for-supercomputer-users/#remove-unneeded-data","title":"Remove unneeded data","text":"<p>Periodically examine your GLADE and NCAR Campaign Storage holdings and remove unwanted, unneeded files. This reduces charges against your storage allocation and makes these systems more efficient for everyone.</p>"},{"location":"getting-started/best-practices-for-supercomputer-users/#contact-cisl-consultants","title":"Contact CISL consultants","text":"<p>Before you run a set of jobs that will consume a large portion of your allocation \u2013 a long experiment, for example \u2013 contact the NCAR Research Computing help desk to request a review of your job configuration. One of the consultants may be able to suggest an economical workflow that will help you conserve computing resources. This is especially important if you are unfamiliar with job configuration or with how to manage your allocation efficiently.</p>"},{"location":"getting-started/best-practices-for-supercomputer-users/#writing-job-scripts","title":"Writing job scripts","text":""},{"location":"getting-started/best-practices-for-supercomputer-users/#avoid-hardcoding-in-your-job-scripts","title":"Avoid hardcoding in your job scripts","text":"<p>Use relative paths and environment variables instead of hardcoding directory names in your job scripts. Hardcoding in scripts and elsewhere can make debugging your code more difficult and also complicate situations in which others need to copy your directories to build and run your code as themselves.</p> <p>Here\u2019s one simple example of what not to do in your script: <pre><code>cd /glade/derecho/scratch/joe/code/running_directory\n</code></pre></p> <p>Instead, replace your hardcoded username with <code>$USER</code>: <pre><code>cd /glade/derecho/scratch/$USER/code/running_directory\n</code></pre></p> <p>Better yet, assume that you will launch the job from your working directory so you don\u2019t need to include the path in your script at all.</p>"},{"location":"getting-started/best-practices-for-supercomputer-users/#use-comments-in-job-scripts","title":"Use comments in job scripts","text":"<p>When setting a variable in your job scripts or startup files, include the date and a brief description of the variable's purpose. This practice may help prevent propagation of variables that are possibly inappropriate in carrying jobs and environments forward. One example is noting the use of a variable that is not set or appropriate in most other scripts.</p> <pre><code># yyyy-mm-dd Context: Cheyenne MPT peak_memusage job.\n# Variable MPI_SHEPHERD is set in this job in order to\n# enable peak_memusage. Do not propagate it to other MPT\n# jobs as it may cause significant slowdown or timeout.\n\nexport MPI_SHEPHERD=\"true\"\n</code></pre>"},{"location":"getting-started/best-practices-for-supercomputer-users/#prepare-for-debugging-and-troubleshooting","title":"Prepare for debugging and troubleshooting","text":"<p>Arrange the script, source code, and data used in your job in a few directories to make it easy for others to copy and debug if necessary. Also: Include a README file that details the environment needed to configure, build and run, and that identifies the required modules and environment variables. Ask a colleague or CISL Consulting Services Group consultant to copy and run the code themselves.</p>"},{"location":"getting-started/best-practices-for-supercomputer-users/#managing-files","title":"Managing files","text":""},{"location":"getting-started/best-practices-for-supercomputer-users/#set-permissions-when-you-create-files","title":"Set permissions when you create files","text":"<p>Set permissions when you create a file. While you can change file ownership and permissions after the fact, establishing them when you create the file will simplify your life and save you time and effort later.</p>"},{"location":"getting-started/best-practices-for-supercomputer-users/#configure-jobs-to-avoid-massive-directories","title":"Configure jobs to avoid massive directories","text":"<p>Ensemble runs, data assimilation runs, and other jobs generate tens or hundreds of thousands of output files, log files, and others over time. Such large numbers of files can be difficult to manage and remove from GLADE file spaces when they are no longer needed. Configuring jobs to place no more than 2,000 to 3,000 files in a single directory will make them easier to manage.</p> <p>See Removing large numbers of files for how to remove massive accumulations of files.</p>"},{"location":"getting-started/best-practices-for-supercomputer-users/#use-scratch-space-for-temporary-files","title":"Use scratch space for temporary files","text":"<p>The GLADE scratch file space is a temporary space for data that will be analyzed and removed within a short amount of time. It is also the recommended space for temporary files that would otherwise reside in small /tmp or /var/tmp directories that many users share. See Storing temporary files with TMPDIR for more information.</p>"},{"location":"getting-started/best-practices-for-supercomputer-users/#use-the-most-appropriate-storage-system","title":"Use the most appropriate storage system","text":"<p>Review and understand the intended uses of the GLADE file spaces and the NCAR Campaign Storage file system. For example, use your <code>/glade/work</code> space to work with data sets over time periods greater than what is permitted in the scratch space. Individual NCAR labs and project leads for universities that have Campaign Storage allocations establish their own workflows and storage policies.</p>"},{"location":"getting-started/best-practices-for-supercomputer-users/#store-large-files","title":"Store large files","text":"<p>Storing large files, such as tar files, is more efficient than storing numerous small files. In the case of GLADE disk storage, this is because the system allocates a minimum amount of space for each file, no matter how small. That amount varies depending on which of several file spaces holds the file. See this GLADE documentation for details.</p>"},{"location":"getting-started/best-practices-for-supercomputer-users/#avoid-sharing-home-spaces","title":"Avoid sharing home spaces","text":"<p>If you have an account for using the supercomputers, analysis, and visualization systems that CISL manages, you have your own <code>/glade/u/home</code> directory. Other users have their own home directories, too, so there is no need to share by giving others write permission. Sharing often leads to unnecessary confusion over file ownership as your work progresses.</p> <p>If you and your colleagues need to write files to a common space, consider using a work space or project space.</p>"},{"location":"getting-started/best-practices-for-supercomputer-users/#organize-for-efficiency","title":"Organize for efficiency","text":"<p>Organize your files and keep them that way. Arrange them in same-purpose trees, for example. Say you have 20 TB of Mount Pinatubo volcanic aerosols data. Keep the files in a subdirectory such as <code>/glade/u/home/$USER/pinatubo</code> rather than scattered among unrelated files or in multiple directories. Specialized trees are easier to share with other users and to transfer to other users or projects as necessary.</p>"},{"location":"getting-started/best-practices-for-supercomputer-users/#back-up-critical-files","title":"Back up critical files","text":"<p>With the exception of users' <code>/glade/u/home</code> spaces, the GLADE and NCAR Campaign Storage file systems are not backed up. You are responsible for replicating any data that you feel should be stored at an additional location.</p>"},{"location":"getting-started/best-practices-for-supercomputer-users/#dont-abandon-files","title":"Don't abandon files","text":"<p>Don't leave orphaned files behind. Before your involvement in a project ends, transfer your files or arrange for someone else to take ownership of them.</p>"},{"location":"getting-started/best-practices-for-supercomputer-users/#transferring-data","title":"Transferring data","text":""},{"location":"getting-started/best-practices-for-supercomputer-users/#use-globus-to-transfer-files","title":"Use Globus to transfer files","text":"<p>CISL recommends using Globus to transfer large files or data sets between the GLADE centralized file service, the NCAR Campaign Storage file system, and remote destinations such as university facilities. In addition to web and command line interfaces, Globus offers a feature called Globus Connect Personal that enables users to move files easily to and from laptop or desktop computers and other systems.</p> <p>Secure Copy Protocol (SCP) works well for transferring a few relatively small files between most systems.</p>"},{"location":"getting-started/data-retention-policies/","title":"General data retention policies and procedures","text":"<p>The following general guidelines apply to project and user data. For a complete listing of all shared file systems, including purge details for scratch space, see GLADE file spaces.</p>"},{"location":"getting-started/data-retention-policies/#project-data","title":"Project data","text":"<p>When a sponsored project approaches expiry, there are several steps in the process that affect the accessibility of associated data:</p> <ul> <li> <p>30 days before expiration, the project PIs will receive an email   reminding them of the pending expiration. The project team should   assess remaining files and disposition appropriately in preparation   for group deactivation.</p> </li> <li> <p>90 days after project expiration, the UNIX group associated with the   project is removed. At this point users with accounts remaining on the   system will likely no longer have access permissions to the projects'   data, as the primary group no longer exists. It is   therefore imperative that any remaining project data be relocated   and ownership permissions assessed prior to this group deactivation.</p> </li> <li> <p>Finally, files are removed as scheduled on the timeline described   above for the relevant file system.</p> </li> </ul>"},{"location":"getting-started/data-retention-policies/#restoring-access-to-project-data","title":"Restoring access to project data","text":"<p>CISL has limited ability to modify access to project data after the 90-day post-expiry window. Such modifications require the approval of the original project owner. CISL has no ability to restore data after the purge or removal policies stated above have taken effect.</p>"},{"location":"getting-started/data-retention-policies/#user-accounts","title":"User accounts","text":"<p>User accounts are deactivated when they are no longer associated with an active project. When a user account is deactivated, several steps in the process affect the accessibility of the users' data:</p> <ul> <li> <p>30 days after a user account is deactivated, a final home directory   backup is performed and the home directory is removed.</p> </li> <li> <p>The user\u2019s work directory is removed. No backup is performed.</p> </li> <li> <p>Finally, additional scratch files are removed as scheduled on the   timeline described above for the relevant file system.</p> </li> </ul>"},{"location":"getting-started/data-retention-policies/#restoring-access-to-collaborators-data","title":"Restoring access to collaborators' data","text":"<p>A typical request for data access comes not from the departing user, but from remaining collaborators. Colleagues occasionally request access to a departed users' files, sometimes many months after the account is terminated, often when they realize the original owner set permissions that limit their access.</p> <p>While CISL has a limited ability to help in these situations, there are also legal limits to what we can do. For example, CISL cannot share files beyond the clear intent of the original owner as inferred from the UNIX file permissions. If a collaborator would like access to a file that was previously group- or world-readable, we may be able to help. If the original file was restricted to user-only read, however, we cannot override those intentions. The only exceptions to this policy are in compliance with broader UCAR IT records or investigation policies as described in UCAR's 1-7 Information Security Policy.</p>"},{"location":"getting-started/managing-your-allocation/","title":"Managing your allocation","text":"<p>Actively managing and monitoring your allocation of computing and storage resources will help ensure that you use these resources as efficiently as possible. Allocations are made to the project lead, who is designated in the allocation request. The project lead often is the principal investigator (PI) for the associated funding awards.</p> <p>The sections below describe how to perform basic administrative tasks and use the Systems Accounting Manager (SAM) to monitor charges against your allocation. All authorized users on a project can manage their own preferences and review their own usage reports in SAM.</p> <p>To get the most from your allocation, be sure to fully utilize the compute resources you request and use available memory efficiently. Also be aware that event failed jobs use some of your allocation, so be proactive in identifying reasons for failures (and contact the NCAR Research Computing help desk if you need assistance).</p>"},{"location":"getting-started/managing-your-allocation/#guidelines-and-allocation-management","title":"Guidelines and allocation management","text":"<p>The project lead may authorize a project administrator (project admin) to perform some tasks on behalf of the project, such as adding or removing users. To designate a project admin, submit a request through the NCAR Research Computing help desk.</p> <p>Here are some guidelines for managing user access to an allocation:</p> <ul> <li> <p>Establish user accounts only for appropriate personnel and make users   aware of   relevant responsibilities and   best practices.</p> </li> <li> <p>Request removal of users' accounts when those individuals are no   longer associated with your project.</p> </li> </ul>"},{"location":"getting-started/managing-your-allocation/#adding-and-removing-user-accounts","title":"Adding and removing user accounts","text":"<p>Some users who are authorized to share access to a project's allocation are identified when the allocation is requested. PIs, project leads, and project admins can also add other users to their projects any time after receiving an allocation award.</p> <p>To add new user accounts for a project, send a request through the help desk link above or call 303-497-2400. Include the user's name, email address, phone number, and a full shipping address for delivery of the new user's authentication token if a physical token is required. You do not need to include a shipping address if the user already has a CISL token.</p> <p>A PI, project lead, or project admin can deauthorize or remove a user from a project by contacting the help desk.</p>"},{"location":"getting-started/managing-your-allocation/#specifying-project-to-be-charged","title":"Specifying project to be charged","text":"<p>A user account must be associated with at least one allocated project and may be associated with more than one. You will have an alphanumeric project code for each such project. These project codes are used to charge your use of computing and storage resources against the appropriate allocations. Take care to specify the correct project code when you submit jobs.</p> <p>Even if you have only one project code, you must specify the project code to be charged when you submit a job. How to do this is described in the documentation for each HPC system.</p>"},{"location":"getting-started/managing-your-allocation/#charges-for-computing","title":"Charges for computing","text":"<p>Projects must have allocations of both CPU core-hours and GPU hours in order to use both types of nodes. Depending on which computing resources are being used, charges are assessed in adjusted core-hours or GPU hours or both in the case of hybrid jobs. These metrics are defined as the number of processors requested multiplied by the duration of the job in hours and, for Derecho and Cheyenne jobs, modified by job priority.</p> Priority level Charging factor Economy 0.7x Regular 1.0x Premium 1.5x <p>If you are concerned about your usage rate, contact the NCAR Research Computing help desk for guidance on running jobs more efficiently and conserving your allocation. Sometimes jobs can be configured to make better use of the compute resources, and you may be able to save allocation by using a less-expensive priority level. Seek help if you notice anything amiss with your allocation.</p>"},{"location":"getting-started/managing-your-allocation/#charging-formula","title":"Charging formula","text":"<p>All jobs on Derecho and Casper are charged according to the following formula:</p> <p>wall-clock hours \u00d7 requested resource (ncpus or ngpus) \u00d7 queue factor</p> <p>Consider the impact of your choice of nodes before you submit a job on these systems.</p> <ul> <li> <p>When using exclusive nodes in the \"main\" submission queue on   Derecho, you will be charged for all CPUs or GPUs on those nodes   regardless of how many you request. For example, if you request two   nodes with <code>ncpus = 18</code> on each node, you will still be charged for   256 CPUs when running in main because those nodes are for your   exclusive use when your job is running.</p> </li> <li> <p>Conversely, when using shared nodes in the \"develop\" queue on   Derecho or the \"casper\" queue on Casper, you will be charged only for   the CPUs or GPUs that you requested.</p> </li> </ul> <p>Note</p> <p>When using GPU nodes, you are not charged for the CPU wall-time used on those nodes. However, it is possible for a job to incur both CPU and GPU costs if you use a heterogeneous mix of node types, with each node being charged according to the dominant resource type (GPUs take precedence).</p>"},{"location":"getting-started/managing-your-allocation/#tracking-usage","title":"Tracking usage","text":"<p>Individuals can track their HPC system usage in the Systems Accounting Manager (see below). SAM reports show usage data and charges against your allocations. Charges for computing are calculated and updated daily; storage use reports are updated weekly.</p>"},{"location":"getting-started/managing-your-allocation/#using-the-systems-accounting-manager","title":"Using the Systems Accounting Manager","text":""},{"location":"getting-started/managing-your-allocation/#user-preferences","title":"User preferences","text":"<p>Log in at sam.ucar.edu and select User, then Preferences.</p> <p>From there, you can:</p> <ul> <li> <p>Change your primary group if you belong to more than one UNIX   group for using NCAR computing resources.</p> </li> <li> <p>Specify your default login shell for the systems to which you have   access.</p> </li> <li> <p>See what your home directory is on each system.</p> </li> </ul>"},{"location":"getting-started/managing-your-allocation/#sam-reports","title":"SAM reports","text":"<p>Log in to sam.ucar.edu and you will see the following choices on the Reports menu:</p> <ul> <li> <p>My Account Statements</p> </li> <li> <p>My 30/90 Day Use</p> </li> <li> <p>Project Search</p> </li> </ul> <p>If you are authorized to charge usage to multiple projects, you will see them listed when you select either of the first two report types. Select one of the projects listed to get information.</p> <p>NCAR divisional users often have access to numerous projects, while individual university users most often have just one or a few.</p> <p>In either case, use Project Search to:</p> <ul> <li> <p>go directly to a report on a specified project, or</p> </li> <li> <p>search by username to see all projects with which you are associated.</p> </li> </ul>"},{"location":"getting-started/managing-your-allocation/#my-account-statements","title":"My Account Statements","text":"<p>Your account statement includes an overall report on the status of your project\u2019s computing and storage allocation and the usage associated with it. If you are authorized to charge usage to more than one project, you will have an account statement for each project.</p> <p></p> <p>The overall usage report on project activity shows your allocations\u2019 start and end dates, amounts allocated, and remaining balances.</p> <p>The Activity link at the end of each line reveals more details: the project\u2019s allocation history, a monthly summary of job charges, and other activity, such as refunds. You can select a month and then view or download the individual job records.</p> <p>Another table includes additional information regarding your project\u2019s status in relation to any 30- and 90-day usage thresholds that apply and to any related projects. This is most common for NCAR users on divisional projects. Your own statement may show lines for multiple projects or subprojects, as is often the case for NCAR divisional allocations.</p>"},{"location":"getting-started/managing-your-allocation/#my-3090-day-use","title":"My 30/90 Day Use","text":"<p>This selection lets you focus on your usage in relation to any 30- and 90-day usage thresholds that apply. Again, this is most common for NCAR divisional projects.</p>"},{"location":"getting-started/managing-your-allocation/#project-search","title":"Project Search","text":"<p>You can search by individual project code and get an account statement as described above.</p> <p>If you search by your username, you will see a list of any projects you are associated with and you can select any of them to get an account statement.</p> <p></p>"},{"location":"getting-started/system-usage-policies/","title":"System use policies","text":""},{"location":"getting-started/system-usage-policies/#appropriate-use-of-login-nodes","title":"Appropriate use of login nodes","text":"<p>Users may run short, non-memory-intensive processes interactively on the Derecho\u00a0system's login nodes. These include tasks such as text editing or running small serial scripts or programs.</p> <p>However, the login nodes\u00a0may not\u00a0be used to run processes that consume excessive resources. This is to ensure an appropriate balance between user convenience and login node performance.</p> <p>This applies to individual processes that consume excessive amounts of CPU time, more than a few GB of memory, or excessive I/O resources. It also applies collectively to multiple concurrent tasks that an individual user runs.</p> <p>Processes that use excessive resources on the login nodes are terminated automatically. Affected users are informed by email that their sessions were terminated. They are also advised to run such processes in batch or interactive jobs on the Casper cluster.</p>"},{"location":"getting-started/system-usage-policies/#fair-share-policy","title":"Fair share policy","text":"<p>CISL manages scheduling priorities to ensure fair access to the system by all of these stakeholder groups: the university community, the NCAR community, the Community Earth System Model (CESM) community, the\u00a0Antarctic Mesoscale Prediction System (AMPS),\u00a0and the Wyoming community.</p> <p>The\u00a0fair-share policy\u00a0takes the community-wide usage balance into account along with several additional factors. These include the submitting users' currently running jobs and recently completed jobs. The scheduling system uses a dynamic-priority formula to weigh these factors, calculate each job's priority, and make scheduling decisions.</p> <p></p>"},{"location":"getting-started/system-usage-policies/#job-scheduling-priorities","title":"Job scheduling priorities","text":"<p>The PBS Pro workload management system scheduling policy for running jobs in the Derecho environment requires balancing several factors. Jobs generally are sorted based on the following:</p> <ol> <li> <p>Job priority (user selectable)</p> </li> <li> <p>Fair share factor</p> </li> <li> <p>Eligible time in queue</p> </li> <li> <p>Job size</p> </li> </ol> <p>Job sorting is adjusted frequently in response to varying demands and workloads. PBS examines the jobs in sorted order in each scheduling cycle and starts those that it can. Jobs that cannot be started immediately are either scheduled to run at a future time or bypassed for the current cycle. Under typical system usage, multiple scheduling cycles are initiated every minute.</p> <p>The scheduler may not start a job for a number of reasons, including:</p> <ul> <li> <p>The necessary resources are not yet available.</p> </li> <li> <p>The system has been reserved for a scheduled outage.</p> </li> <li> <p>The job has been placed on hold.</p> </li> <li> <p>You have reached your concurrent core-usage limit when using the   develop queue.</p> </li> </ul> <p>A high-priority job might be delayed by one of the limits on the list, while a lower-priority job from a different user or a job requesting fewer resources might not be blocked.</p> <p>If your job is waiting in the queue, you can run the <code>qstat</code> command as shown to obtain information that can indicate why it has not started running. (Use this command sparingly.) <pre><code>qstat -s jobID\n</code></pre></p> <p>Note</p> <p>To prevent jobs from languishing in the queues indefinitely, PBS reserves resources for the top-priority jobs and doesn't allow lower-priority jobs to start if they would delay the start time of a higher-priority job.</p>"},{"location":"getting-started/system-usage-policies/#pbs-sorting-factors","title":"PBS sorting factors","text":""},{"location":"getting-started/system-usage-policies/#stakeholder-shares-and-fair-share-factor","title":"Stakeholder shares and fair-share factor","text":"<p>CISL manages scheduling priorities to ensure fair access to the system by these stakeholder groups: the university community, the NCAR community, the CESM community,\u00a0and the Wyoming community.</p> <p>Each stakeholder group is allocated a certain percentage of the available processors. A job cannot start if that action would cause the group to exceed its share, unless another group is using less than its share and has no jobs waiting. In such a case, the high-use group can \"borrow\" processors from the lower-use stakeholder group for a short time.</p> <p>When jobs are sorted, jobs from groups that are using less of their share are picked before jobs from groups using more of their shares. Shares are evaluated based on usage over the past week with usage the prior week being decayed by half.</p>"},{"location":"getting-started/system-usage-policies/#job-priority","title":"Job priority","text":"<p>Users can set job priority to one of three values. Jobs with higher priority are charged against the user's allocation at higher rates than others.</p> Job priority Priority order Priority factor Description premium 1 1.5 Jobs are charged at 150% of the regular rate. regular 2 1 All production jobs default to this priority. economy 3 0.7 Production batch jobs are charged at 70% of regular rate. preempt 4 0 Automatically selected when job is submitted to preempt queue."},{"location":"getting-started/system-usage-policies/#job-size","title":"Job size","text":"<p>Jobs asking for more nodes are favored over jobs asking for fewer. The reasoning is that while it is easier for small jobs to fill gaps in the schedule, larger jobs need help collecting enough CPUs or GPUs to start.</p>"},{"location":"getting-started/system-usage-policies/#gpu-usage","title":"GPU usage","text":"<p>In order to submit jobs that will use GPUs, you must be associated with a project that has an allocation of GPU hours. If you submit a job with a project code that does not have an allocation of GPU hours, your job will be rejected.</p>"},{"location":"getting-started/system-usage-policies/#backfilling","title":"Backfilling","text":"<p>When a job cannot start immediately, PBS sets aside resources for it before examining other jobs to see if any of them can run as backfill. That is, PBS looks at running jobs to determine when they will finish based on wall-time requested. From those finish times, PBS decides when enough resources (such as CPUs, memory, and job limits) will become available to run the top job. PBS then reserves the resources that the job requests at that identified time.</p> <p>When PBS looks at other jobs to see if they can start immediately, it also checks whether starting any of them would collide with one of these resource reservations. Only if there are no collisions will PBS start the lower-priority jobs.</p>"},{"location":"getting-started/system-usage-policies/#preemption","title":"Preemption","text":"<p>Derecho has a preemption routing queue that can be used to submit jobs that will run when the required resources are not in use for higher-priority work in the main or develop execution queues. In order to take advantage of preemption, submit your job to the preempt routing queue and it job will run when the necessary resources are available.</p> <p>When resources for any preempt jobs are needed by higher-priority work, the scheduler sends a <code>SIGTERM</code> signal that can be detected by your job. After the <code>SIGTERM</code> signal is sent to the job, there is a five-minute window in which the job has a chance to checkpoint or save any work that was accomplished. After the five-minute window, the job will be killed by the scheduler and deleted.</p>"},{"location":"getting-started/user-responsibilities/","title":"User responsibilities","text":"<p>When you are granted access to use NCAR resources, you accept the responsibilities listed below.</p> <ul> <li> <p>You will use these computer and information systems in an ethical and   legal manner.</p> </li> <li> <p>You agree not to duplicate or use copyrighted or proprietary software   without proper authorization.</p> </li> <li> <p>You may not use NCAR computer and information systems in any manner   for any business, professional, or other activity that is unrelated to   the purpose of your allocation or terms of employment except as   otherwise stated in Access to and Use of Information Systems and   Technology   Infrastructure   (staff login required).</p> </li> <li> <p>You are required to acknowledge the use of NCAR resources, including   the CMIP Analysis Platform and Research Data Archive, in any resulting   publications, in a manner consistent with the examples provided here.</p> </li> <li> <p>You are responsible for protecting your personal identification   number, authentication token, and/or passwords.</p> </li> <li> <p>You may not share your account privileges with anyone or knowingly   permit any unauthorized access to a computer, computer privileges,   systems, networks, or programs. The accounts of those involved will be   disabled if sharing is detected.</p> </li> <li> <p>While NCAR storage systems are highly reliable, you are responsible   for backing up critical data to protect it against loss or corruption.   You also are responsible for understanding the usage and data   retention policies for the file system and data archive resources   used.</p> </li> <li> <p>You agree to report potential security breaches as soon as possible by   calling the Research Computing Help Desk at 303-497-2400.</p> </li> <li> <p>You are responsible for ensuring that NCAR has your current contact   information, including your phone number, email address, and mailing   address. If your name, phone number, email address, or other   information changes, notify CISL   through support.ucar.edu. If CISL   personnel can\u2019t reach you when they need to \u2013 about a problem caused   by a job you are running, for example \u2013 the job may be killed and you   will be locked out of the system.</p> </li> <li> <p>Project leads (including instructors associated with classroom   allocations) are responsible for ensuring that users on their projects   are aware of these responsibilities and for ensuring that   authentication tokens are returned when users complete their work or   students finish their classes. Project leads are responsible for any   token replacement fees.</p> </li> </ul> <p>Please contact the NCAR Research Computing help desk if you have questions.</p>"},{"location":"getting-started/vpn-access/","title":"VPN access","text":"<p>NCAR and UCAR employees who are working remotely, and some other individuals who work closely with NCAR, may need to use the virtual private network (VPN) to connect with internal resources.</p> <p>If you need VPN access, see your lab's system administrator about downloading the necessary client software.</p> <p>When you have the client software and the required authentication token or app, you can connect to the VPN as described below.</p>"},{"location":"getting-started/vpn-access/#connecting-with-globalprotect","title":"Connecting with GlobalProtect","text":"<p>Start\u00a0by clicking the GlobalProtect icon on your desktop or taskbar.</p> <p>Enter your username. (Remove <code>CIT\\</code> if it appears in the password field. Enter only your username.)</p> <p>Follow the documented procedures for using your authentication token or app (for example, CIT password and Duo push). Use <code>gp.ucar.edu</code> as the portal name if it doesn't appear already.</p> <p></p> <p>A box like the one below will confirm that you are connected to the VPN.</p> <p></p> <p>Click Disconnect when you are finished.</p>"},{"location":"getting-started/accounts/","title":"Authentication and security","text":"<p>Individuals who are granted access to the computing and storage resources that CISL manages use their assigned user names and one of the authentication methods that are described below to log in to those systems.</p> <p>Passwords, apps, tokens, and PINs must be protected and may not be shared with anyone. If sharing is detected, CISL will disable the accounts of those involved.\u00a0The same applies to passwords that give users access to internal UCAR systems.</p> <p>UCAR and NCAR computers, computing systems, and associated communications systems are to be used for official business only.\u00a0By signing the required authentication acknowledgement form, you agree not to misuse these resources, and you accept responsibility for activity associated with your username and token. You also agree not to duplicate or use copyrighted or proprietary software without proper authorization.</p>"},{"location":"getting-started/accounts/#duo-two-factor-authentication","title":"Duo two-factor authentication","text":"<p>Logging in with the Duo two-factor authentication (2FA) service requires the user to enter a CIT password in conjunction with the Duo Mobile app or a landline phone. See Authenticating with Duo for details.</p>"},{"location":"getting-started/accounts/#security-overview","title":"Security overview","text":"<p>All users must comply with UCAR computer security policies and procedures. See Access to and Use of Information Systems and Technology Infrastructure (staff login required).</p> <p>We strive to maximize the availability and value of our computer and network systems by protecting them from unauthorized access. Good security practices help prevent data loss or corruption, malicious activity, and loss of computer time.</p> <p>As a user, you have an important role in ensuring the security of these resources. In addition to protecting the passwords, PINS, and tokens that give you access to our systems, we ask that you do the following:</p>"},{"location":"getting-started/accounts/#loss-theft-or-compromise","title":"Loss, theft, or compromise","text":"<p>Loss, theft, or compromise of a YubiKey must be reported within 48 hours to the Research Computing Help Desk at x2400 (303-497-2400). Quick reporting will help the organization minimize security risk.</p>"},{"location":"getting-started/accounts/#protecting-your-duo-app-or-yubikey-token","title":"Protecting your Duo app or YubiKey token","text":"<p>You must protect your Duo or YubiKey solution by agreeing to the following:</p> <ul> <li> <p>Your YubiKey token or Duo application will remain in your custody and   is for your use only; it may not be shared.</p> </li> <li> <p>You will immediately (within 48 hours) report loss of custody of your   hardware authentication token to the Research Computing Help Desk at   x2400 (303-497-2400). Loss of custody may be due to loss or theft.</p> </li> <li> <p>Your PIN number or CIT password may not be shared or made available in   unencrypted electronic form.</p> </li> <li> <p>Compromise (disclosure of PIN number or CIT password) must be reported   to the Research Computing Help Desk at x2400 (303-497-2400) and/or to   the UCAR Security Operations Center at x4300 (307-996-4300).</p> </li> </ul>"},{"location":"getting-started/accounts/#protect-your-pin","title":"Protect your PIN","text":"<p>Do not leave your PIN where others may view it, and do not affix it to your workstation or your token. Do not use the same PIN that you use for debit cards or credit cards.</p> <p>Try to memorize your PIN instead of writing it down. You may write it down, but do not store it with the token. If you do write it down, keep it where others cannot access it, such as in a locked desk drawer or file cabinet that only you can access.</p>"},{"location":"getting-started/accounts/#use-encryption-for-logging-in-and-transferring-files","title":"Use encryption for logging in and transferring files","text":"<p>Our systems require this, but it also is good practice to use encryption for other computers and systems.</p>"},{"location":"getting-started/accounts/#patch-your-systems-and-use-anti-virus-software","title":"Patch your systems and use anti-virus software","text":"<p>This applies to any computer from which you log in to UCAR and NCAR systems. If you are using your own personal computer or another non-UCAR or non-NCAR computer, be sure that it is kept up to date with the latest software patches and anti-virus protection.</p> <p>If you are planning to visit UCAR and bringing your own computer, discuss wireless and guest network access with your UCAR contact before you arrive. Procedures regarding guest network access also apply to personally owned computers that UCAR and NCAR staff bring in.</p>"},{"location":"getting-started/accounts/#be-careful","title":"Be careful","text":"<p>Be aware of email scams and so-called \"social engineering\" methods that hackers and fraudsters use to gain access to passwords and other information. Never give anyone your password. UCAR and NCAR system administrators will not ask you for your password via phone or email.</p>"},{"location":"getting-started/accounts/#other-cautions","title":"Other cautions","text":"<ul> <li> <p>Don't run strange binaries or executables.</p> </li> <li> <p>Don't log in to sites that you receive in email or other messages,   especially if the message seems urgent and you are not familiar with   the site.</p> </li> <li> <p>Some malware is spread via USB flash drives, so make sure any flash   drives that you use are from trusted sources.</p> </li> </ul>"},{"location":"getting-started/accounts/cit-passwords/","title":"CIT passwords","text":""},{"location":"getting-started/accounts/cit-passwords/#cit-password-portal","title":"CIT Password Portal","text":"<p>If you enrolled in the password management portal and have forgotten or need to reset your password, you can reset it here: CIT Password Portal.</p>"},{"location":"getting-started/accounts/cit-passwords/#non-staff","title":"Non-staff","text":"<p>Users who need to get a CIT password must call 303-497-2400 for assistance.</p> <p>If you have already submitted a password request but have not yet returned the required token authorization form, please do so to complete the process.</p>"},{"location":"getting-started/accounts/cit-passwords/#ucarncar-staff","title":"UCAR/NCAR staff","text":"<p>Either log into the CIT Password Portal or contact your divisional or laboratory sysadmin for assistance.</p>"},{"location":"getting-started/accounts/systems-accounting-manager/","title":"Systems Accounting Manager","text":"<p>The\u00a0Systems Accounting Manager (SAM) enables users to manage their system preferences and get reports on charges for using the computational and storage resources that CISL manages.</p>"},{"location":"getting-started/accounts/systems-accounting-manager/#user-preferences","title":"User preferences","text":"<p>Log in at sam.ucar.edu and select User, then Preferences.</p> <p></p> <p>From there, you can:</p> <ul> <li> <p>Change your primary group if you belong to more than one UNIX   group for using NCAR supercomputing resources.</p> </li> <li> <p>Specify your default login shell for the systems to which you have   access.</p> </li> <li> <p>See what your home directory is on each system.</p> </li> </ul>"},{"location":"getting-started/accounts/systems-accounting-manager/#sam-reports","title":"SAM reports","text":"<p>Log in to sam.ucar.edu and you will see the following choices on the Reports menu:</p> <ul> <li>My Account Statements</li> <li>My 30/90 Day Use</li> <li>Project Search</li> </ul> <p>If you are authorized to charge usage to multiple projects, you will see them listed when you select either of the first two report types. Select one of the projects listed to get information.</p> <p>NCAR divisional users often have access to numerous projects, while individual university users most often have just one or a few.</p> <p>In either case, use Project Search to:</p> <ul> <li>go directly to a report on a specified project, or</li> <li>search by username to see all projects with which you are associated.</li> </ul>"},{"location":"getting-started/accounts/systems-accounting-manager/#my-account-statements","title":"My Account Statements","text":"<p>Your Account Statement includes an overall report on the status of your project\u2019s computing and storage allocation and the usage associated with it. If you are authorized to charge usage to more than one project, you will have an Account Statement for each project.</p> <p></p> <p>The overall usage report on project activity shows your allocations\u2019 start and end dates, amounts allocated, and remaining balances.</p> <p>The \"Activity\" link at the end of each line reveals more details: the project\u2019s allocation history, a monthly summary of job charges, and other activity, such as refunds. You can select a month and then view or download the individual job records.</p> <p>Another table includes additional information regarding your project\u2019s status in relation to any 30- and 90-day usage thresholds that apply and to any related projects. This is most common for NCAR users on divisional projects. Your own statement may show lines for multiple projects or subprojects, as is often the case for NCAR divisional allocations.</p>"},{"location":"getting-started/accounts/systems-accounting-manager/#my-3090-day-use","title":"My 30/90 Day Use","text":"<p>This selection lets you focus on your usage in relation to any 30- and 90-day usage thresholds that apply. Again, this is most common for NCAR divisional projects.</p>"},{"location":"getting-started/accounts/systems-accounting-manager/#project-search","title":"Project Search","text":"<p>You can search by individual project code and get an Account Statement as described above.</p> <p>If you search by your username, you will see a list of any projects you are associated with and you can select any of them to get an Account Statement.</p> <p></p>"},{"location":"getting-started/accounts/duo/","title":"Authenticating with Duo","text":"<p>Danger</p> <p>NCAR Duo Authentication is export-controlled. Taking the app on your phone to Cuba, Iran, Syria, North Korea, or Sudan is strictly prohibited.</p>"},{"location":"getting-started/accounts/duo/#overview","title":"Overview","text":"<p>Logging in with Duo two-factor authentication (2FA) requires you to enter a CIT password and then use a Duo-configured device to confirm your identity.</p> <p>Best practice recommendation: Use a screen lock on your mobile device to increase security.</p> <p>Three ways of logging in with Duo:</p> <ol> <li> <p>Push notification (preferred)     The app sends a request (a \"push\" notification) to your phone or     tablet, asking you to approve or deny the login request.</p> </li> <li> <p>Rolling passcode     When you can't receive a push notification, enter both your CIT     password and a numerical passcode from the Duo app, separated by a     comma. Example: password,passcode</p> </li> <li> <p>Phone callback     Enter your CIT password and the word \"phone,\" separated by a comma,     then follow instructions you receive in a phone call. Example:     password,phone</p> </li> </ol> <p>The examples below use the push notification method of authentication. See How to Use Append Mode for more information on other methods.</p>"},{"location":"getting-started/accounts/duo/#getting-started-with-duo","title":"Getting started with Duo","text":"<p>To get started, contact the NCAR Research Computing help desk to request enrollment (and to get a CIT password if you don't already have one).</p> <p>CISL will send you a link for setting up a Duo account.</p> <p>During setup, Duo asks some questions about the device you want to use. Smartphone and tablet users are asked to download this free Duo Mobile app.</p> <p>When your setup is complete, follow the instructions below to log in to the system, such as Cheyenne, the NCAR virtual private network, or others that accept Duo 2FA.</p>"},{"location":"getting-started/accounts/duo/#logging-in-with-duo","title":"Logging in with Duo","text":""},{"location":"getting-started/accounts/duo/#hpc-and-ssh-logins","title":"HPC and SSH logins","text":"<p>To log in to a system like Derecho:</p> <ul> <li>Enter your ssh command.</li> <li>Enter your CIT password where a token response is requested.</li> </ul> <p></p> <p>The Duo App will send a \"push\" notification to your phone or tablet, asking you to approve or deny the login request.</p> <p>When you approve the request, you will be logged in.</p>"},{"location":"getting-started/accounts/duo/#other-application-logins","title":"Other application logins","text":"<p>Duo authentication with other systems is somewhat different. Logging on to the NCAR virtual private network (VPN) is one example.</p> <p>You will:</p> <ul> <li>Enter your username.</li> <li>Enter your CIT password.</li> <li>You may get an automatic Duo Push, or select Send Me a Push from     the Duo screen.</li> </ul> <p>The Duo App will send a push notification to your phone or tablet, asking you to approve or deny the login request.</p> <p>When you approve the request, you will be logged in.</p> <p></p>"},{"location":"getting-started/accounts/duo/#duo-device-portal","title":"Duo Device Portal","text":"<p>The Duo Device Portal is where you can change device settings, add new devices (a new smartphone, tablet or landline), or update your preferred contact methods.</p> <p>You can also choose to have Duo send you a push automatically after you enter your CIT password. Look for \"When I log in\" after you sign in to the portal.</p> <p></p>"},{"location":"getting-started/accounts/duo/#changing-smartphone","title":"Changing smartphone","text":"<p>When you replace your smartphone and need to use it to authenticate, use one of the following methods to get your new phone up and running with Duo Mobile:</p> <p>Recommended: Duo Instant Restore, a feature for recovering Duo-protected accounts.</p> <p>Alternative:</p> <ul> <li>Go to the Duo Device Portal.</li> <li>Choose Call Me. Even if your phone number hasn't changed, Duo     needs to call your new phone to complete the setup process.</li> </ul>"},{"location":"getting-started/accounts/duo/#user-guides-and-related-links","title":"User guides and related links","text":"<p>For additional information, see the following links or contact the NCAR Research Computing help desk for assistance:</p> <ul> <li> <p>Common issues</p> </li> <li> <p>Duo Guide to Two-Factor Authentication</p> </li> <li> <p>Duo Travel Guide</p> </li> <li> <p>Duo Quick Sheet</p> </li> </ul>"},{"location":"getting-started/accounts/duo/enrolling/","title":"Enrolling your phone or tablet for Duo 2FA","text":"<p>Duo Mobile is an app that runs on your smartphone and helps you authenticate quickly and easily with Duo two-factor authentication (2FA). You will still be able to log in using a phone call or text message without the app, but for the best experience, we recommend using Duo Mobile with a smartphone.</p> <p>You can also enroll a landline telephone or iOS/Android tablets.</p> <p>After you request enrollment and receive an email from Duo Security, follow these steps to enroll your device.</p>"},{"location":"getting-started/accounts/duo/enrolling/#step-1-click-start-setup","title":"Step 1: Click Start setup","text":"<p>Click the personalized enrollment link in your email from Duo Security, then click Start setup.</p> <p></p>"},{"location":"getting-started/accounts/duo/enrolling/#step-2-enter-your-phone-number","title":"Step 2: Enter your phone number","text":"<p>Select your country from the drop-down list and enter your phone number. Use the number of the smartphone or cell phone that you'll have with you when you're logging in to a Duo-protected service. You can enter your desk phone number if you don\u2019t have a cell phone.</p> <p>Double-check the phone number to make sure you entered it correctly, check the box to confirm that the number is correct, and click Continue. (If you are enrolling a tablet, you aren't prompted to enter a phone number.)</p> <p></p>"},{"location":"getting-started/accounts/duo/enrolling/#step-3-choose-your-platform","title":"Step 3: Choose your platform","text":"<p>Choose the type of phone you have and click Continue.</p> <p></p>"},{"location":"getting-started/accounts/duo/enrolling/#step-4-install-the-duo-mobile-app","title":"Step 4: Install the Duo Mobile app","text":"<p>Follow the platform-specific instructions on the screen to install Duo Mobile. After installing the app, return to the enrollment window and click I have Duo Mobile installed.</p> <p></p>"},{"location":"getting-started/accounts/duo/enrolling/#step-5-activate-duo-mobile","title":"Step 5: Activate Duo Mobile","text":"<p>Activating the app links it to your account so you can use it for authentication.</p> <p>To activate Duo Mobile on an iPhone, Android, or Windows Phone, scan the barcode with the app's built-in barcode scanner. Follow the platform-specific instructions for your device.</p> <p></p> <p>The \"Continue\" button is clickable after you scan the barcode.</p> <p>If you can\u2019t scan the barcode, click Email me an activation link instead and follow the instructions.</p> <p></p>"},{"location":"getting-started/accounts/duo/enrolling/#step-6-configure-device-options-optional","title":"Step 6: Configure device options (optional)","text":"<p>By default Duo will ask you to choose how you want to authenticate each time you log in \u2013 with a push, phone call, or passcode, for example. The default is recommended, but you can change the setting so you automatically receive a push or a call instead of being asked every time. To do that, make your selection from the dropdown menu and click Saved.</p> <p></p>"},{"location":"getting-started/accounts/duo/enrolling/#step-7-finish","title":"Step 7: Finish","text":"<p>Choose an optional authentication method from the \u201cWhen I log in\u201d dropdown menu, then click Finish Enrollment.</p> <p>Duo automatically sends an authentication request via a push notification to the Duo Mobile app on your smartphone or a phone call to your device (depending on your selection).</p> <p></p>"},{"location":"getting-started/accounts/duo/enrolling/#congratulations","title":"Congratulations!","text":"<p>Your device is ready to approve Duo push authentication requests. Click Send me a Push to give it a try. All you need to do is tap Approve on the Duo login request you get on your phone.</p> <p></p>"},{"location":"nhug/","title":"Welcome to the NCAR HPC User Group (NHUG)","text":"<p>The NCAR HPC Users\u2019 Group, NHUG, welcomes participation from all NCAR HPC users!</p> <p>The NCAR HPC Users\u2019 Group, NHUG, is a dedicated community that aims to promote the productive use of high-performance computing (HPC) facilities at NCAR and increase collaboration among all of the NCAR HPC community.</p> <p>NHUG holds monthly meetings featuring different HPC-related topics. The goal of these meetings is to foster a community around HPC-related issues and increase the collaboration among all of the NCAR HPC user community.  We encourage you to join our monthly meetings.</p> <p>NHUG Communication Channels</p> <p>Join NHUG email list to receive calendar invites for upcoming monthly meetings and other important announcements. We also encourage you to join NCAR HPC User Group NCAR HPC User Group Slack channel channel which we will use for focussed interactions during and outside the monthly meetings. Join NHUG Email List Join NHUG Slack Channel</p> <p>Contribute to Future NHUG Topics</p> <p>If you have suggestions for future meeting topics or ideas that can enhance our community, we'd love to hear from you. Please submit your ideas using the form below to help shape our upcoming sessions.     Submit Your Ideas</p>"},{"location":"nhug/upcoming-events/","title":"NHUG Upcoming Events","text":""},{"location":"pbs/","title":"Starting and managing jobs with PBS","text":"<p>About this page</p> <p>This documentation provides information for how to use PBS Pro to submit and manage interactive jobs and batch jobs on NCAR systems.</p> <p>The basic PBS commands are the same on each cluster, but refer to these system-specific pages for details that are unique to each of them, including hardware specifications, software, and job-submission queues and procedures:</p> <ul> <li>Derecho</li> <li>Casper</li> </ul> <p>Computerized batch processing is a method of running software programs called jobs in batches automatically. While users are required to submit the jobs, no other interaction by the user is required to process the batch. Batches may automatically be run at scheduled times as well as being run contingent on the availability of computer resources. For additional background, see Batch Computng Overview. NCAR's HPC resources use the Portable Batch System as implemented in Altair's PBS Pro across shared resources.</p>"},{"location":"pbs/#job-scripts","title":"Job scripts","text":"<p>Job scripts form the basis of batch jobs. A job script is simply a text file with instructions of the work to execute.  Job scripts are usually written in <code>bash</code> or <code>tcsh</code> and thus mimic commands a user would execute interactively through a shell; but instead are executed on specific resources allocated by the scheduler when available.  Scripts can also be written in other languages - commonly Python.  See our job scripts page for a detailed discussion of job scripts and examples.</p>"},{"location":"pbs/#submitting-jobs","title":"Submitting jobs","text":"<p>In the examples that follow, <code>script_name</code>, <code>job.pbs</code> etc... represent a job script files submitted for batch execution.</p> <p>PBS Pro is used to schedule both interactive jobs and batch compute jobs. Detailed examples of how to start both types of jobs are included in the documentation (see links above) for each individual system.</p> <p>Commands for starting interactive jobs are specific to individual systems. The basic command for starting a batch job, however, is the same.</p> <p>To submit a batch job, use the <code>qsub</code> command followed by the name of your PBS batch script file.</p> <pre><code>qsub script_name\n</code></pre>"},{"location":"pbs/#propagating-environment-settings","title":"Propagating environment settings","text":"<p>Some users find it useful to set environment variables in their login environment that can be temporarily used for multiple batch jobs without modifying the job script. This practice can be particularly useful during iterative development and debugging work.</p> <p>PBS has two approaches to propagation:</p> <ol> <li>Specific variables can be forwarded to the job upon request.</li> <li>The entire environment can be forwarded to the job.</li> </ol> <p>In general, the first approach is preferred because the second may have unintended consequences.</p> <p>These settings are controlled by qsub arguments that can be used at the command line or as directives within job scripts. Here are examples of both approaches:</p> <pre><code># Selectively forward runtime variables to the job (lower-case v)\nqsub -v DEBUG=true,CASE_NAME job.pbs\n</code></pre> <p>When you use the selective option (lower-case <code>v</code>), you can either specify only the variable name to propagate the current value (as in <code>CASE_NAME</code> in the example), or you can explicitly set it to a given value at submission time (as in <code>DEBUG</code>).</p> <p><pre><code># Forward the entire environment to the job (upper-case V)\nqsub -V job.pbs\n</code></pre> Do not use full propagation when peer-scheduling jobs. Doing so will cause libraries and binaries to be inherited via variables like <code>PATH</code> and <code>LD_LIBRARY_PATH</code>. These inherited settings will cause applications to break, and may render the job completely unusable.</p>"},{"location":"pbs/#managing-jobs","title":"Managing jobs","text":"<p>Here are some of the most useful commands for managing and monitoring jobs that have been launched with PBS.</p> <p>Most of these commands will only modify or query data from jobs that are active on the same system. That is, run each command on Derecho if you want to interact with a job on Derecho.</p> <p>Run any command followed by <code>-h</code> to get help, as in <code>qhist -h</code>.</p>"},{"location":"pbs/#qdel","title":"<code>qdel</code>","text":"<p>Run <code>qdel</code> with the job ID to kill a pending or running job.</p> <p><pre><code>qdel jobID\n</code></pre> Kill all of your own pending or running jobs. (Be sure to use backticks as shown.)</p> <pre><code>qdel `qselect -u $USER`\n</code></pre>"},{"location":"pbs/#qhist","title":"<code>qhist</code>","text":"<p>Run <code>qhist</code> for information on finished jobs.</p> <pre><code>qhist -u $USER\n</code></pre> <p>Your output will include jobs that finished on the current day unless you specify the number (<code>N</code>) of days to include.</p> <pre><code>qhist -u $USER -d N\n</code></pre> <p>Your output will be similar to this, with <code>Mem(GB)</code> and <code>CPU(%)</code> indicating approximate total memory usage per job and average CPU usage per core per job: <pre><code>Job ID  User     Queue  Nodes NCPUs Finish  RMem(GB)  Mem(GB) CPU(%) Elap(h)\n2426690 stormyk  regular    1     1 05-1527        -     0.3   75.0    0.09\n2426693 stormyk  regular    1     1 05-1527        -     0.1   90.0    0.09\n2426541 stormyk  regular    1     1 05-1523        -     0.1   83.0    0.03\n2426542 stormyk  regular    1     1 05-1524        -     0.1   70.0    0.04\n2426683 stormyk  regular    1     1 05-1523        -     0.1    0.0    0.02\n2426444 stormyk  regular    1     1 05-1522        -     0.1   19.0    0.02\n2426435 stormyk  regular    1     1 05-1522        -     0.1   13.0    0.02\n</code></pre> You can obtain additional job details using <code>qhist -w</code> for wide output, or customize the output - see <code>qhist --format=help</code> for a list of options.</p> <p>The following variation will generate a list of jobs that finished with non-zero exit codes to help you identify jobs that failed.</p> <pre><code>qhist -u $USER -r x0\n</code></pre>"},{"location":"pbs/#qstat","title":"<code>qstat</code>","text":"<p>Run this to see the status of all of your own unfinished jobs.</p> <pre><code>qstat -u $USER\n</code></pre> <p>Your output will be similar to what is shown just below. Most column headings are self-explanatory \u2013 <code>NDS</code> for nodes, <code>TSK</code> for tasks, and so on.</p> <p>In the status <code>(S)</code> column, most jobs are either queued <code>(Q)</code> or running <code>(R)</code>. Sometimes jobs are held <code>(H)</code>, which might mean they are dependent on the completion of another job. If you have a job that is held and is not dependent on another job, CISL recommends killing and resubmitting the job.</p> <pre><code>                                                       Req'd  Req'd   Elap\nJob ID         Username Queue   Jobname SessID NDS TSK Memory Time  S Time\n------         -------- -----   ------- ------ --- --- ------ ----- - ----\n657237.chadmin apatelsm economy ens603   46100 60  216   --   02:30 R 01:24\n657238.chadmin apatelsm regular ens605     --   1   36   --   00:05 H   --\n657466.chadmin apatelsm economy ens701    5189 60  216   --   02:30 R 00:46\n657467.chadmin apatelsm regular ens703     --   1   36   --   00:10 H   --\n</code></pre> <p>Following are examples of <code>qstat</code> with some other commonly used options and arguments.</p> <p>Get a long-form summary of the status of an unfinished job.</p> <pre><code>qstat -f jobID\n</code></pre> <p>Warning</p> <p>Use the above command only sparingly; it places a high load on PBS.</p> <p>Get a single-line summary of the status of an unfinished or recently completed job (within 72 hours).</p> <pre><code>qstat -x jobID\n</code></pre> <p>Get information about unfinished jobs in a specified execution queue.</p> <p><pre><code>qstat queue_name\n</code></pre> See job activity by queue (e.g., pending, running) in terms of numbers of jobs.</p> <pre><code>qstat -Q\n</code></pre> <p>Display information for all of your pending, running, and finished jobs.</p> <pre><code>qstat -x -u $USER\n</code></pre> <p>Query jobs running on one system by specifying the system as shown here. (Only these options are supported when running qstat in this cross-server mode: <code>-x</code>, <code>-u</code>,<code>-w</code>, <code>-n</code>, <code>-s</code>)</p> <pre><code>qstat -w -u $USER @derecho\n</code></pre> <p>Tip</p> <p>Query jobs running on one system by specifying <code>@derecho</code>, <code>@cheyenne</code>, or <code>@casper</code> from either system as shown here. <pre><code>qstat -w -u $USER @derecho\nqstat -w -u $USER @casper\n</code></pre> Only these options are supported when running <code>qstat</code> in this cross-server mode: <code>-x</code>, <code>-u</code>, <code>-w</code>, <code>-n</code>, <code>-s</code></p>"},{"location":"pbs/#job-dependencies","title":"Job Dependencies","text":"<p>It is possible to schedule jobs to run based on the status of other jobs. For example, you might schedule a preprocessing job to run; start a computation job when the preprocessing job is complete; then start a post-processing job when the computation is done.</p> <p>One way to schedule such a series or chain of jobs is to use <code>qsub -W [job-dependency-expression]</code> to specify the job dependencies you need.  This can also be accomplished by submitting subsequent jobs from inside another job.</p>"},{"location":"pbs/#pbs-job-dependencies-w-depend","title":"PBS job dependencies (<code>-W depend</code>)","text":"<p>Let's say you have you have three scripts to submit and run consecutively:</p> <ol> <li><code>pre.pbs</code>: a preprocessing job</li> <li><code>main.pbs</code>: a computation job</li> <li><code>post.pbs</code>: a post-processing job</li> </ol> <p>The main job can be run only when the preprocessing job finishes, and the post-processing job can be run only when the computation job finishes.</p> <p>Submit the first job, placing it on hold. (If it starts before the dependent jobs are submitted, the dependent jobs might never start.): <pre><code>JOBID1=$(qsub -h pre.pbs)\n</code></pre> Make starting of the second job dependent on successful completion of the first: <pre><code>JOBID2=$(qsub -W depend=afterok:$JOBID1 main.pbs)\n</code></pre> Make starting of the post-processing job dependent on successful completion of the second job: <pre><code>JOBID3=$(qsub -W depend=afterok:$JOBID2 post.pbs)\n</code></pre> Release the first job to initiate the sequence: <pre><code>qrls $JOBID1\n</code></pre> (Strictly speaking, <code>JOBID3</code> is not used in these examples and can be omitted if desired, but the sequence listed above is easily extensible to another dependent job.)</p> <p>The complete sequence is shown below for additional clarity:</p> bashtcsh <pre><code>JOBID1=$(qsub -h pre.pbs)\nJOBID2=$(qsub -W depend=afterok:$JOBID1 main.pbs)\nJOBID3=$(qsub -W depend=afterok:$JOBID2 post.pbs)\nqrls $JOBID1\n</code></pre> <pre><code>set JOBID1=`qsub -h pre.pbs`\nset JOBID2=`qsub -W depend=afterok:$JOBID1 main.pbs`\nset JOBID3=`qsub -W depend=afterok:$JOBID2 post.pbs`\nqrls $JOBID1\n</code></pre> <p>In the example above, the clause <code>afterok</code> is used to indicate the subsequent jobs should only begin after the preceding job completes successfully. This is likely the most common use case, however other clauses are available, and jobs may depend on multiple predecessors. Some of the more common dependency clauses are listed below, where <code>&lt;arg_list&gt;</code> is usually a single PBS job id as shown above, but can be a colon-separated list of IDs if appropriate.  See <code>man qsub</code> for a full listing and additional details.</p> Clause Effect <code>after:&lt;arg_list&gt;</code> This job may be scheduled for execution at any point after all jobs in <code>&lt;arg_list&gt;</code> have started execution. <code>afterok:&lt;arg_list&gt;</code> This job may be scheduled for execution only after all jobs in <code>&lt;arg_list&gt;</code> have terminated with no errors. <code>afternotok:&lt;arg_list&gt;</code> This job may be scheduled for execution only after all jobs in <code>&lt;arg_list&gt;</code> have terminated with errors. <code>afterany:&lt;arg_list&gt;</code> This job may be scheduled for execution after all jobs in <code>&lt;arg_list&gt;</code> have finished execution, with any exit status (with or without errors.) This job will not run if a job in the <code>&lt;arg_list&gt;</code> was deleted without ever having been run. <code>before:&lt;arg_list&gt;</code> Jobs in <code>&lt;arg_list&gt;</code> may begin execution once this job has begun execution. <code>beforeok:&lt;arg_list&gt;</code> Jobs in <code>&lt;arg_list&gt;</code> may begin execution once this job terminates without errors. <code>beforenotok:&lt;arg_list&gt;</code> If this job terminates execution with errors, jobs in <code>&lt;arg_list&gt;</code> may begin. <code>beforeany:&lt;arg_list&gt;</code> Jobs in <code>&lt;arg_list&gt;</code> may begin execution once this job terminates execution, with or without errors."},{"location":"pbs/#nested-jobs","title":"Nested jobs","text":""},{"location":"pbs/#peer-scheduling-scheduling-between-systems","title":"Peer Scheduling scheduling between systems","text":"<p>Peer Scheduling scheduling between HPC Systems</p> <p>Cheyenne, Derecho,  and Casper use the PBS scheduler, and each system has its own dedicated PBS server to manage job submission and execution. These \"peer\" servers can share data between each other, and jobs can be submitted from one system to another. It is also possible to create dependencies between jobs on each server, enabling simulation-analysis workflows that target the appropriate system for each task.</p>"},{"location":"pbs/#submitting-a-job-to-a-peer-system","title":"Submitting a job to a peer system","text":"<p>To submit a job to a queue on a peer server, you need to append the name of the server to the queue directive in your job script. The names of the PBS servers are:</p> System PBS server name Cheyenne <code>chadmin1.ib0.cheyenne.ucar.edu</code> Casper <code>casper-pbs</code> Derecho <code>desched1</code>"},{"location":"pbs/#examples","title":"Examples","text":"Casper to DerechoCasper to CheyenneDerecho to Casper <p>You want to submit to the Derecho \"main\" queue from a Casper login node or compute node. Append the Derecho server name as follows in your job script when specifying the queue: <pre><code>#PBS -q main@desched1\n</code></pre></p> <p>You want to submit to the Cheyenne \"regular\" queue from a Casper login node or compute node. Append the Cheyenne server name as follows in your job script when specifying the queue: <pre><code>#PBS -q regular@chadmin1.ib0.cheyenne.ucar.edu\n</code></pre></p> <p>You want to submit a job to Casper from Derecho. Append the Casper server name as follows in your job script when specifying the destination: <pre><code>#PBS -q casper@casper-pbs\n</code></pre></p> <p>The server-specific queue names will be understood by both PBS servers, so if you will want to submit the same script at times from either Cheyenne or Casper, always append the server name to your queue.</p> <p>The <code>qinteractive</code> and <code>execcasper</code> scripts, which start interactive jobs on Cheyenne and Casper respectively, will adjust the queue name for you to include the server, so you do not need to append the server name manually for interactive jobs.</p>"},{"location":"pbs/#querying-jobs","title":"Querying jobs","text":"<p>You can use <code>qstat</code> to query jobs from peer servers by including the server name in your field of interest. You can also use the system names noted above when running qstat.</p> <p>Note that the separator character differs for jobs (<code>.</code>) and queues (<code>@</code>). <pre><code>qstat 123456.chadmin1.ib0.cheyenne.ucar.edu\nqstat regular@cheyenne\nqstat 654321.casper\n\nqstat @casper-pbs\nqstat @desched1\n</code></pre></p>"},{"location":"pbs/#creating-dependencies-between-peer-scheduled-jobs","title":"Creating dependencies between peer-scheduled jobs","text":"<p>Creating job dependencies between submissions on peer servers is straightforward; there is nothing unique about this workflow in PBS. As with all jobs, pay close attention to specifying the destination server in your queue designations. The job IDs returned by PBS include the server name, so you do not need to append a server to the job ID you specify in your dependency argument.</p> <p>Here is an example of a workflow that runs a simulation on Cheyenne and, if successful, then runs a post-processing job on Casper. Thanks to peer scheduling, these jobs can be submitted from either Cheyenne or Casper login nodes.</p> bashtcsh <pre><code>JID=$(qsub -q economy@chadmin1.ib0.cheyenne.ucar.edu run_model.pbs)\nqsub -q casper@casper-pbs -W depend=afterok:$JID run_postprocess.pbs\n</code></pre> <pre><code>set JID=`qsub -q economy@chadmin1.ib0.cheyenne.ucar.edu run_model.pbs`\nqsub -q casper@casper-pbs -W depend=afterok:$JID run_postprocess.pbs\n</code></pre>"},{"location":"pbs/charging/","title":"Job-submission queues and charges","text":""},{"location":"pbs/charging/#derecho-queue-details","title":"Derecho queue details","text":"<p>The <code>main</code> queue, which has a 12-hour wall-clock limit, meets most users' needs for running batch jobs on Derecho.  On Derecho users request a specific job priority via the <code>#PBS -l job_priority=&lt;regular|premium|economy&gt;</code> resource directive, instead of through distinct queues as was the case previously on Cheyenne.</p> Queue name PBS<code>job_priority</code> Wall clock limit (hours) Charging factor Derecho queue description <code>main</code> <code>premium</code> 12 1.5 Jobs are charged at 150% of the regular rate. <code>regular</code> 1 Most production batch jobs run in this queue; also accepts interactive jobs. <code>economy</code> 0.7 Production batch jobs are charged at 70% of the regular rate. <code>preempt</code> 24 0.2 Jobs will only run on resources otherwise idle.  Jobs may be preempted with a short grace period to make room for higher priority jobs. <code>develop</code> 6 1 Interactive and serial batch use for debugging and other tasks on shared 256-GB nodes. Jobs in this queue should specify memory required via <code>-l select=...:mem=5GB</code> up to 235GB. An individual job can use up to 256 cores or 8 GPUs across two nodes. A user can run multiple jobs in the share queue concurrently if the total number of cores or GPUs used is no more than 256 or 8 respectively. <p>Some additional queues on the system are for dedicated purposes and accessible only to authorized users.</p>"},{"location":"pbs/charging/#calculating-charges","title":"Calculating charges","text":""},{"location":"pbs/charging/#exclusive-nodes","title":"Exclusive nodes","text":"<p>Charges for use of Derecho are calculated in terms of core-hours. Jobs run in Derecho queues other than \"share\" are charged for exclusive use of the nodes by this formula:</p> <p>wall-clock hours \u00d7 nodes used \u00d7 cores per node \u00d7 charging factor</p> <p>Derecho node charging</p> <p>Your batch script indicates how many Derecho nodes your job will use.</p> Fully Subscribed CPU NodeFully Subscribed CPU Node (Hybrid)Under-subscribed CPU Node  <p>In this example, you have selected 2 nodes, each of which has 128 cores, all of which will be used as MPI \"ranks.\" <pre><code>#PBS -l select=2:ncpus=128:mpiprocs=128\n</code></pre> Your job will be charged for the use of 256 cores.</p> <p>In this example, you have selected 4 nodes, each of which has 128 cores. Each node will have 32 MPI ranks and 4 OpenMP threads. <pre><code>#PBS -l select=4:ncpus=128:mpiprocs=32:ompthreads=4\n</code></pre> Your job will be charged for the use of 512 cores.</p> <p>In this example, you have selected 2 nodes, each of which has 128 cores. To double the  available memory per rank, you elect to \"under-subscribe\" the node, that is, only use 64 cores as MPI ranks. <pre><code>#PBS -l select=2:ncpus=128:mpiprocs=64:mem=235GB\n</code></pre> Your job will be charged for all 256 cores, even though you are only using 128 (assuming your code has no hybrid parallel capability - e.g OpenMP or other threading).</p> <p>Exclusive nodes are charged by resource allocation, not utilization</p> <p>Most Derecho batch queues are for exclusive use, so jobs submitted to Derecho queues are charged for all 128 CPU cores on each node that is used regardless of how many CPUs are used. Requesting a Derecho CPU node for 1 hour will result in a 128 core hour charge, even if left idle by the user.</p>"},{"location":"pbs/charging/#shared-nodes-casper","title":"Shared nodes (Casper)","text":"<p>Charges for jobs that you run on a shared node, including Casper nodes, are calculated by this formula:</p> <p>core-seconds/3600 (core-hours)</p>"},{"location":"pbs/charging/#checking-and-managing-charges","title":"Checking and managing charges","text":"<p>Users can check computing and storage charges through the CISL Systems Accounting Manager. (Go to SAM documentation or to SAM app.)</p> <p>If you have concerns about using your allocation most efficiently, contact the NCAR Research Computing help desk for guidance. Sometimes jobs can be configured to make better use of the processors, and you may be able to save by using a less expensive queue.</p> <p>CISL can refund core-hours if system failures cause jobs to fail and the failed jobs are reported promptly. Use this core-hours refund request form (login required) if you think a refund is warranted. Technical limitations prevent us from verifying refund eligibility for jobs that are more than seven days old.</p>"},{"location":"pbs/preemption/","title":"Job preemption with PBS","text":"<p>Derecho Only!</p> <p>This page describes functionality only present on Derecho.</p> <p>Job Preemption on Derecho</p> <p>Some jobs are suitable for running in incrementally on resources that would otherwise be idle.</p> <p>Derecho supports running such jobs in the <code>preempt</code> queue, which allows them to be preempted with minimal impact when a higher-priority job requires the use of those resources. Suitable workflows include those with short or fairly unpredictable run times; for example, data processing, file movement, or running analysis tools that have an efficient checkpoint/restart capability.</p>"},{"location":"pbs/preemption/#using-the-preempt-queue","title":"Using the preempt queue","text":"<p>The <code>preempt</code> queue is similar to the <code>main</code> Derecho queue in that it serves to route jobs to the system's CPU or GPU nodes. It is different, however, in that:</p> <ul> <li> <p>Jobs will start only when they can make use of resources that would   otherwise be idle.</p> </li> <li> <p>They will be terminated \u2013 with a 10-minute grace period \u2013 if the   resources being used are required by a higher priority job in a   different queue.</p> </li> </ul> <p>The start time of a job in the preempt queue will be unpredictable because of the idle resource prerequisite. Once it starts, it is guaranteed at least 10 minutes of run time but potentially much more. The duration depends on jobs other users submit to PBS after your job begins.</p> <p>To submit a job to the preempt queue, simply specify preempt as the queue name in your PBS script header as follows:</p> <p>Requesting the <code>preempt</code> queue</p> <pre><code>#!/bin/bash\n#PBS -A &lt;project_code&gt;\n#PBS -N preemptable_job\n#PBS -q preempt\n#PBS -r n\n#PBS -l walltime=04:00:00\n#PBS -l select=2:ncpus=128:mpiprocs=32:ompthreads=4\n\n...\n</code></pre> <p>The <code>walltime</code> specification is the job duration upper limit - as discussed above, the actual execution could be shorter if the job is preempted.</p> <p>Use the specifier <code>#PBS -r</code> to indicate if the job should be rerun if it is preempted; valid options are <code>y</code> or <code>n</code> for yes or no. All other aspects of the PBS script are unchanged by the use of preemption.</p> <p>Abrupt termination may be entirely acceptable for some workflows. This could be the case for batch removal of a large number of files, for example, or if the application writes frequent checkpoint files and can restart successfully after being interrupted. In other cases, it may be beneficial for the application to take a specific action within the 10-minute grace window. Such an approach is possible with minor changes to the application as described below.</p>"},{"location":"pbs/preemption/#practical-implications-for-preempt-throughput","title":"Practical implications for preempt throughput","text":"<p>Idle resources are a prerequisite for jobs in the preempt queue to start. The smaller the resource request, the more likely there will be an idle portion of the machine on which to schedule the job. Conversely, large jobs in the queue are likely to wait for long periods of time, if they execute at all. The ideal use case is small-to-medium sized jobs that are robust to interruption and that can make meaningful progress in short periods of time.</p>"},{"location":"pbs/preemption/#charging-and-allocations","title":"Charging and allocations","text":"<p>Jobs run in the preempt queue are charged at a queue factor of only 0.2, vs. 1.0 for regular jobs. Jobs that do not run to completion because of preemption are not charged against your allocation.</p>"},{"location":"pbs/preemption/#terminating-proactively-with-signal-handling","title":"Terminating proactively with signal handling","text":"<p>When a job running in the preempt queue is targeted for preemption, PBS notifies the running process through a UNIX signal. PBS then waits 10 minutes before killing the process. A properly configured application can receive the notification signal, act upon it (typically through an existing checkpoint functionality), and then terminate gracefully rather than be terminated abruptly at the end of the grace period. The steps required to configure an application in this manner are:</p> <ol> <li> <p>Provide a signal handler function to receive the termination     request.</p> </li> <li> <p>Register the signal handler function with the operating system.</p> </li> <li> <p>Invoke or add checkpoints to be triggered by the signal handler.</p> </li> </ol> <p>Steps 1 and 2 are fairly common across applications and even programming languages. Step 3 is application-specific, and usually involves writing the application state to disk so that it can be restarted later. For some applications, however, an even simpler approach may be possible. For example, if the target application is a data-processing pipeline, it may suffice to receive the termination notification, complete the current processing step, and simply exit without beginning additional steps in the pipeline.</p>"},{"location":"pbs/preemption/#signal-handling-and-registration","title":"Signal handling and registration","text":""},{"location":"pbs/preemption/#cc-and-fortran","title":"C/C++ and Fortran","text":"<p>For traditional compiled languages such as C/C++ and Fortran, signal handling is most readily accomplished through some minimal C functions, even inside a predominantly Fortran application. This is because the operating system application interface is C based. The following shows the minimal required steps.</p> <p>Sample C program to catch signals sent from the operating system</p> <pre><code>#include &lt;stdio.h&gt;\n#include &lt;unistd.h&gt;\n#include &lt;time.h&gt;\n#include &lt;signal.h&gt;\nstatic int checkpoint_requested = 0;\n\nvoid my_sig_handler (int signum)\n{\n  time_t now;\n  time(&amp;now);\n\n  switch (signum)\n    {\n    case SIGINT:\n    case SIGTERM:\n    case SIGUSR1:\n      checkpoint_requested = 1;\n      printf(\"...caught signal %d at %s\", signum, ctime(&amp;now));\n      break;\n\n    default:\n      printf(\"...caught other unknown signal: %d at %s\", signum, ctime(&amp;now));\n      printf(\"   see \\\"man 7 signal\\\" for a list of known signals\\n\");\n      break;\n    }\n}\n\nint main (int argc, char **argv)\n{\n  /* register our user-defined signal handlers */\n  signal(SIGINT,  my_sig_handler);\n  signal(SIGTERM, my_sig_handler);\n  signal(SIGUSR1, my_sig_handler);\n\n  return 0;\n}\n</code></pre> <p>First, we declare a C function <code>my_sig_handler</code>, which takes the signal identifier as input. In this example we construct a switch statement that allows for processing different types of signals in the same code block. It is evident from the listing that if the function is called with a <code>SIGINT</code>, <code>SIGTERM</code>, or <code>SIGUSR1</code> signal then we set the flag <code>checkpoint_requested</code> and print an informative statement. For completeness, if called with any other signal, we print a diagnostic message as well but take no other action.</p> <p>Second, we call the system routine <code>signal()</code>to register our function for the specific signals we want the application to process. In this case, we are asking the operating system to call our function my_sig_handler() any time a <code>SIGINT</code>, <code>SIGTERM</code>, or <code>SIGUSR1</code> is encountered.</p> <p>The third step is application specific and not listed, but the general idea is elsewhere in the application (for example, the main time step loop) we would check the value of the <code>checkpoint_requested</code> flag and take appropriate action to save state and exit gracefully.</p> Expand this box for a complete MPI/C++ example <pre><code>#include &lt;iostream&gt;\n#include &lt;iomanip&gt;\n#include &lt;stdio.h&gt;\n#include &lt;unistd.h&gt;\n#include &lt;signal.h&gt;\n#include &lt;mpi.h&gt;\n\n\n\nnamespace {\n  int numranks, rank;\n  char hn[256];\n  int checkpoint_req = 0;\n\n  void done_checkpoint () { checkpoint_req = 0; }\n\n}\n\n\nint checkpoint_requested (MPI_Comm comm)\n{\n  int local_checkpoint_req = checkpoint_req;\n  MPI_Allreduce(&amp;local_checkpoint_req, &amp;checkpoint_req,\n                1, MPI_INT, MPI_MAX, comm);\n  return checkpoint_req;\n}\n\n\n\nvoid my_sig_handler (int signum)\n{\n  time_t now;\n  time(&amp;now);\n\n  if (0 == rank) printf(\"...inside handler function\\n\");\n\n  switch (signum)\n    {\n    case SIGSTOP:\n    case SIGINT:\n    case SIGTERM:\n    case SIGUSR1:\n      checkpoint_req = 1;\n      if (0 == rank) printf(\"...caught signal %d at %s\", signum, ctime(&amp;now));\n      break;\n\n    default:\n      if (0 == rank)\n        {\n          printf(\"...caught other unknown signal: %d at %s\", signum, ctime(&amp;now));\n          printf(\"   see \\\"man 7 signal\\\" for a list of known signals\\n\");\n        }\n      break;\n    }\n\n  // re-register default signal handler for action\n  //if (0 == rank) printf(\" --&gt; Restoring default handler for signal %d\\n\", signum);\n  //signal(signum, SIG_DFL);\n\n  return;\n}\n\n\n\n\nvoid register_sig_handler ()\n{\n  if (0 == rank) printf(\"Registering user-specified signal handlers for PID %d\\n\", getpid());\n\n  signal(SIGSTOP, my_sig_handler);\n  signal(SIGINT,  my_sig_handler);\n  signal(SIGTERM, my_sig_handler);\n  signal(SIGUSR1, my_sig_handler);\n  signal(SIGUSR2, my_sig_handler);\n}\n\n\n\nvoid do_checkpoint (MPI_Comm comm)\n{\n  for (int i=1; i&lt;=10; i++)\n    {\n      if (0 == rank)\n        {\n          time_t now;\n          time(&amp;now);\n          printf(\"\\t%2d : Inside checkpoint function : %s\",i, ctime(&amp;now));\n          fflush(stdout);\n          sleep(5);\n        }\n      MPI_Barrier(comm);\n    }\n\n  done_checkpoint();\n\n  if (0 == rank)\n    {\n      time_t now;\n      time(&amp;now);\n      printf(\" --&gt; Gracefully exiting after checkpoint : %s\", ctime(&amp;now));\n      fflush(stdout);\n    }\n\n  MPI_Finalize();\n  exit(EXIT_SUCCESS);\n\n  return;\n}\n\n\n\nint main (int argc, char **argv)\n{\n  gethostname(hn, sizeof(hn) / sizeof(char));\n\n  MPI_Init(&amp;argc, &amp;argv);\n\n  MPI_Comm_size (MPI_COMM_WORLD, &amp;numranks);\n  MPI_Comm_rank (MPI_COMM_WORLD, &amp;rank);\n\n  std::cout &lt;&lt; \"Hello from \" &lt;&lt; rank &lt;&lt; \" / \" &lt;&lt; std::string (hn)\n        &lt;&lt; \", running \" &lt;&lt; argv[0] &lt;&lt; \" on \" &lt;&lt; numranks &lt;&lt; \" ranks\" &lt;&lt; std::endl;\n\n  // register our user-defined signal handlers, on every rank\n  register_sig_handler();\n\n  for (int i=1; i&lt;=5000 ;i++)\n    {\n      if (0 == rank)\n        {\n          time_t now;\n          time(&amp;now);\n          printf(\"%2d : Main function loop : %s\",i,ctime(&amp;now));\n          fflush(stdout);\n          sleep(5);\n        }\n      MPI_Barrier(MPI_COMM_WORLD);\n\n      // this function needs to perform a reduction to see if any rank received\n      // a signal, hence it is blocking.\n      if (checkpoint_requested(MPI_COMM_WORLD))\n        do_checkpoint(MPI_COMM_WORLD);\n    }\n\n  MPI_Finalize();\n  return 0;\n}\n</code></pre> <p>You can see this example on Github here.</p> <p>To integrate such an approach into a Fortran application, it is simplest to create a C function taking no arguments that encapsulates the signal registration process and calling it from within your Fortran main application. Please contact CISL help for further assistance.</p> <p>While the most common use case is compiled languages as shown above, it is also possible to catch and act upon signals when your main application is a shell script or in Python, as shown below.</p>"},{"location":"pbs/preemption/#shell-scripts","title":"Shell scripts","text":"<p>In shell scripting, the process is generally similar, with some very slight changes in terminology. Notably, in shell scripts <code>traps</code> can be used to intercept signals and call a user-specified function, in much the same way a signal handler can be installed in a compiled language. A complete example follows.</p> <p>Sample bash script to catch signals sent from the operating system</p> <pre><code>#!/usr/bin/env bash\n\ncheckpoint_requested=false\n\nfunction my_signal_handler()\n{\n    case ${sig} in\n        SIGINT)\n        SIGTERM)\n        SIGUSR1)\n            printf \"...caught %s at %s\\n\" ${sig} \"$(date)\"\n            checkpoint_requested=true\n            ;;\n        *)\n            printf \"...caught unknown signal: %s\\n\" ${sig}\n            exit 1\n            ;;\n    esac\n}\n\nfunction register_signal_handler ()\n{\n    printf \"Registering user-specified signal handlers for PID %d\\n\" $$\n\n    trap \"sig=SIGINT;  my_signal_handler\" SIGINT\n    trap \"sig=SIGTERM; my_signal_handler\" SIGTERM\n    trap \"sig=USR1;    my_signal_handler\" SIGUSR1\n}\n\nfunction do_checkpoint ()\n{\n    for i in $(seq 1 10); do\n        printf \"\\t%2d : Inside checkpoint function\\n\" $i\n        sleep 5s\n    done\n\n    checkpoint_requested=false;\n}\n\n# main function follows\nregister_signal_handler\n\nfor i in $(seq 1 50); do\n    printf \"%2d : Main function\\n\" $i\n    sleep 5s\n\n    if ${checkpoint_requested}; then\n        do_checkpoint\n    fi\ndone\n</code></pre> <p>Running the previous code will enter a \"Main Function\" loop that executes a number of steps. Sending Control+C to the running program effectively sends a <code>SIGINT</code> and invokes the desired signal handling function through the bash trap mechanism.</p>"},{"location":"pbs/preemption/#python-applications","title":"Python applications","text":"<p>Finally, Python provides the <code>signal</code> module, which can be used in a user application to catch signals from the operating system as shown here:</p> <p>Sample python script to catch signals sent from the operating system</p> <pre><code>#!/usr/bin/env python3\n\nimport signal\nimport sys\nimport time\nimport os\n\nfrom datetime import datetime\n\ncheckpoint_requested = False\n\ndef my_signal_handler(sig, frame):\n    global checkpoint_requested\n\n    if signal.SIGINT == sig or signal.SIGTERM == sig or signal.SIGUSR1 == sig:\n        print(\"...caught signal {} at \".format(sig) + datetime.now().strftime(\"%H:%M:%S %B %d, %Y\"))\n        checkpoint_requested = True\n    else:\n        print(\"...caught unknown signal: {}\".format(sig))\n        sys.exit(1)\n    return\n\ndef register_signal_handler ():\n    print(\"Registering user-specified signal handlers for PID {}\".format(os.getpid()))\n    signal.signal(signal.SIGINT,  my_signal_handler)\n    signal.signal(signal.SIGTERM, my_signal_handler)\n    signal.signal(signal.SIGUSR1, my_signal_handler)\n    return\n\ndef do_checkpoint ():\n    global checkpoint_requested\n\n    for i in range(1, 11):\n        print(\"\\t{:2d} : Inside checkpoint function\".format(i))\n        sys.stdout.flush()\n        time.sleep(5)\n\n    checkpoint_requested = False;\n    return\n\nif __name__ == \"__main__\":\n    register_signal_handler();\n\n    for i in range(1, 51):\n        print(\"{:2d} : Main function\".format(i))\n        sys.stdout.flush()\n        time.sleep(5)\n\n        if checkpoint_requested:\n            do_checkpoint()\n</code></pre>"},{"location":"pbs/preemption/#additional-resources","title":"Additional resources","text":"<p>All the sample scripts above are available through the NCAR hpc-demos GitHub repository in the <code>PBS/preemp</code>t subdirectory.</p> <ul> <li> <p>More detail on signal handling in C</p> </li> <li> <p>More detail on using traps in shell scripts</p> </li> <li> <p>More detail on signal handling in Python</p> </li> </ul>"},{"location":"pbs/storing-temporary-files/","title":"Storing temporary files","text":""},{"location":"pbs/storing-temporary-files/#storing-temporary-files-with-tmpdir","title":"Storing temporary files with <code>TMPDIR</code>","text":"<p><code>/tmp</code>, <code>/var/tmp</code>, or similar shared directories to hold temporary files can increase the risk of your own programs and other users' programs failing when no more space is available. Compilers and utilities often create such temporary files without your knowledge of their size or number.</p> <p>Specifying your own directory for temporary files can help you avoid the problem.</p>"},{"location":"pbs/storing-temporary-files/#interactive-use","title":"Interactive use","text":"<p>In interactive use on the login nodes, the default TMPDIR is <code>/glade/derecho/scratch/$USER</code>. You can change that by running the commands shown below on your command line when you log in or by setting the <code>TMPDIR</code> variable in your start file.</p>"},{"location":"pbs/storing-temporary-files/#batch-use","title":"Batch use","text":"<p>For batch use, CISL recommends setting <code>TMPDIR</code> within each batch script for all batch jobs. Include these commands as the first two executable lines of your batch script after the <code>#PBS</code> directives.</p> bash/zshtcsh <pre><code>export TMPDIR=$SCRATCH/temp &amp;&amp; mkdir -p $TMPDIR\n</code></pre> <pre><code>setenv TMPDIR $SCRATCH/temp &amp;&amp; mkdir -p $TMPDIR\n</code></pre>"},{"location":"pbs/storing-temporary-files/#using-local_scratch-on-casper-nodes","title":"Using <code>/local_scratch/</code> on Casper nodes","text":""},{"location":"pbs/checking-memory-use/","title":"Checking memory use","text":"<p>All compute nodes have a limited amount of physical memory \u2013 RAM \u2013 available to your application. If your program exceeds the available memory on a Derecho node, it will either be killed by system monitors or crash and dump core, often with a \"bus error\" message in your logs. On the Casper cluster, nodes can swap data to NVMe storage, which typically prevents failures because of running out of memory in all but the most intensive workflows. Computing on swapped data, however, is much slower than processing data that is resident in memory, so jobs may can still fail from exhausting the requested wall-clock.</p> <p>Applications that offload computation to GPUs must also consider the available memory on each GPU, which is known as the VRAM. Asking for a large amount of memory in every job might seem like a good idea, but it will typically result in slow dispatch times and long queue waits.</p> <p>Tip</p> <p>Always try to match your memory request to your needs. This allows the scheduler to best fit all jobs onto the available resources. A good match will also prevent you from needlessly waiting for more memory than you actually require.</p> <p>As detailed below, you can query the PBS scheduler for bulk memory statistics from any completed job. You can also observe memory usage via either instrumenting your application or live monitoring.</p>"},{"location":"pbs/checking-memory-use/#available-memory-by-system","title":"Available memory by system","text":"System Usable memory per compute node Derecho 240 GB (2,488 CPU nodes)         490 GB (82 GPU nodes) Cheyenne 45 GB (3,168 nodes)         109 GB (864 nodes) Casper 365 GB (22 nodes)         738 GB (4 nodes)         1115 GB (6 nodes) <p>If your job approaches the usable memory per node threshold shown in the table, you may experience unexpected issues or job failures. Leave a margin of 2 or 3 percent.</p>"},{"location":"pbs/checking-memory-use/#querying-memory-usage-from-pbs","title":"Querying memory usage from PBS","text":"<p>The <code>qhist</code> tool can be used to display information about completed PBS jobs. By default, it will print the node memory the job consumed (the high-water mark usage), as in this example: <pre><code>$ qhist -u $USER -d 3\nJob ID    User       Queue    Nodes NCPUs NGPUs Finish    Mem(GB)   CPU(%)  Elap(h)\n3632559   benkirk    htc          1     1     0 27-1802      21.8     99.0     8.01\n3632475   benkirk    htc          1     1     0 27-0949      30.1     86.0     0.26\n...\n</code></pre></p> <p>The output shows all jobs run in the previous three days, and it lists the amount of memory consumed.</p> <p>Running qhist with the <code>-w</code> argument specifies \"wide\" output and provides additional details, including the memory requested, as in this example: <pre><code>$ qhist -u $USER -d 3 -w\nJob ID    User            Queue      Nodes NCPUs NGPUs Finish Time  Req Mem Used Mem(GB)  Avg CPU (%)  Elapsed (h) Job Name\n3632559   benkirk         htc            1     1     0 07-27T18:02     40.0         21.8         99.0         8.01 STDIN\n3632475   benkirk         htc            1     1     0 07-27T09:49    256.0         30.1         86.0         0.26 regrid_bmr_casper\n...\n</code></pre></p> <p>The output in the first job shows that it requested 40 GB of memory but used only 21.8 GB. The second job requested 256 GB but used only 30.1GB.</p> <p>If you see a job where the used memory matches the requested memory, it likely either failed with an out-of-memory error or began swapping data to disk, negatively affecting performance. Using <code>qhist</code>, you can easily check for large under- and overestimates and adjust future jobs accordingly.</p> <p>PBS logs do not include information about GPU memory usage.</p>"},{"location":"pbs/checking-memory-use/#instrumenting-your-application","title":"Instrumenting your application","text":"<p>While the scheduler can provide the maximum memory amount used by a job, it cannot provide details on specific commands in your script, time-varying memory logging, or insight into GPU VRAM usage. For these details, you will want to turn to application instrumentation. Using instrumentation will incur some memory usage overhead, so it can be a good idea to request more memory than your job would typically need when profiling.</p>"},{"location":"pbs/checking-memory-use/#using-peak_memusage-to-report-and-log-memory-usage","title":"Using <code>peak_memusage</code> to report and log memory usage","text":"<p>The <code>peak_memusage</code> tool is a command-line utility that outputs the maximum node memory \u2013 and GPU memory, if relevant) \u2013 used by each process and/or thread of an application. It has several modes of operation which vary in detail reported and complexity of use.</p>"},{"location":"pbs/checking-memory-use/#peak_memusage-for-determining-maximum-memory-usage","title":"<code>peak_memusage</code> for determining maximum memory usage","text":"<p>The <code>peak_memusage</code> command can be used to run your application and report its \"high water,\" or peak memory footprint. To use it, you must load the appropriate module and simply prefix your application launch command as follows: <pre><code>$ module load peak-memusage\n$ peak_memusage &lt;my_application&gt; &lt;--my-apps-arguments&gt;\n...\ncasper-login1 used memory in task 0: 51MiB (+2MiB overhead). ExitStatus: 0. Signal: 0\n</code></pre></p> <p>The regular application output has been abbreviated. The final line of output reveals this application required a total of 51MiB of system memory, including 2MiB \"overhead.\" The overhead may vary considerably by application and depends largely on what libraries the application requires. This approach also works inside MPI applications, with specific examples provided here. Run <code>man peak_memusage</code> for additional details and examples.</p>"},{"location":"pbs/checking-memory-use/#determining-memory-use-history","title":"Determining memory use history","text":"<p>CISL provides a <code>log_memusage</code> tool you can use to produce a time history of system memory usage as well as GPU memory if allocated to the job. This tools provides a shared library that can be either linked to directly or, more likely, used with an existing application through the <code>LD_PRELOAD</code> mechanism. For example, if you have a previously compiled application and would like to determine its memory usage over time, set the environment variable LD_PRELOAD when executing the application as follows: <pre><code>$ LD_PRELOAD=${NCAR_ROOT_PEAK_MEMUSAGE}/install/lib/liblog_memusage.so &lt;my_application&gt;\n...\n# (memusage) --&gt; gust02 / PID 44899, peak used memory: 18 MiB\n</code></pre></p> <p>The <code>log_memusage</code> tool works by creating a monitoring thread that polls the operating system for the application's memory usage at specified intervals. Several environment variables control the tool's behavior. Run <code>man log_memusage</code> for additional details and examples. This approach also works under MPI if the <code>LD_PRELOAD</code> environment variable is propagated properly into the MPI execution, as in this example: <pre><code>$ mpiexec -n 6 -env LD_PRELOAD=${NCAR_ROOT_PEAK_MEMUSAGE}/lib/liblog_memusage.so &lt;my_application&gt;\n...\n# (memusage) --&gt; gu0017 / PID 121027 / MPI Rank 0, peak used memory: 166 MiB (CPU), 563 MiB (GPU)\n# (memusage) --&gt; gu0017 / PID 121029 / MPI Rank 2, peak used memory: 93 MiB (CPU), 563 MiB (GPU)\n# (memusage) --&gt; gu0017 / PID 121031 / MPI Rank 4, peak used memory: 93 MiB (CPU), 563 MiB (GPU)\n# (memusage) --&gt; gu0017 / PID 121032 / MPI Rank 5, peak used memory: 92 MiB (CPU), 563 MiB (GPU)\n# (memusage) --&gt; gu0017 / PID 121030 / MPI Rank 3, peak used memory: 93 MiB (CPU), 563 MiB (GPU)\n# (memusage) --&gt; gu0017 / PID 121028 / MPI Rank 1, peak used memory: 165 MiB (CPU), 563 MiB (GPU)\n</code></pre></p> <p>The details for how a given MPI implementation passes environment variables can vary. Some use <code>-env</code> while others use <code>-x</code>. Run <code>man mpiexec</code> for specifics of your implementation.</p>"},{"location":"pbs/checking-memory-use/#using-cray-mpich-to-report-and-log-derecho-memory","title":"Using <code>cray-mpich</code> to report and log Derecho memory","text":"<p>The preferred Cray MPICH MPI implementation on Derecho provides capabilities, similar to those above, for recording an application's high-water memory usage. They are controlled by these environment variables:</p> <ul> <li><code>MPICH_MEMORY_REPORT</code></li> <li><code>MPICH_MEMORY_REPORT_FILE</code></li> </ul> <p>Here's an example: <pre><code>export MPICH_MEMORY_REPORT=1\nmpiexec &lt;typical arguments&gt; ./myapp\n...\n# MPICH_MEMORY: Max memory allocated by malloc:  5540920 bytes by rank 0\n# MPICH_MEMORY: Min memory allocated by malloc:  5540224 bytes by rank 1\n# MPICH_MEMORY: Max memory allocated by mmap:    110720 bytes by rank 0\n# MPICH_MEMORY: Min memory allocated by mmap:    110720 bytes by rank 0\n# MPICH_MEMORY: Max memory allocated by shmget:  17834896 bytes by rank 0\n# MPICH_MEMORY: Min memory allocated by shmget:  0 bytes by rank 1\n</code></pre></p> <p>The <code>mpiexec</code> command also provides a high-level summary of resources consumed, including memory use, when the <code>--verbose</code> flag is employed: <pre><code>mpiexec --verbose &lt;typical arguments&gt; ./myapp\n...\nApplication dc6a4084 resources: utime=7s stime=7s maxrss=222296KB inblock=0 oublock=0 minflt=289502 majflt=0 nvcsw=16550 nivcsw=276\n</code></pre></p> <p>The <code>maxrss</code> (maximum resident set size) is the maximum CPU memory required by a single MPI rank in the application. Run <code>man intro_mpi</code> and <code>man mpiexec</code> on Derecho for more information on these Cray-specific features.</p>"},{"location":"pbs/checking-memory-use/#other-parallel-memory-profiling-tools","title":"Other parallel memory profiling tools","text":"<p>We recommend the tools described above for understanding your job memory use and crafting your PBS resource specifications. However, sometimes you need even more detailed information on application memory use patterns (e.g., during code development cycles). Many tools exist for detailed memory profiling; on Derecho and Casper we recommend Linaro Forge.</p>"},{"location":"pbs/checking-memory-use/#interactive-monitoring","title":"Interactive monitoring","text":"<p>The tools described above record the memory consumed during a program's execution, but monitoring in real time can be useful, too. It allows you to take action if you need to while the job is running and you have access to the compute nodes. Take care when executing additional commands on your job nodes, as any additional load you put on the execution host will slow your program down, and any additional memory you consume subtracts from what is available. This consideration is especially important on the Casper cluster, where your host is likely shared with other users.</p> <p>The first step is to identify which nodes are in use by running the <code>qstat -f &lt;JOBID&gt;</code> command as shown here: <pre><code>Job Id: 5677216.casper-pbs\n...\n    resources_used.mem = 78004288kb\n    resources_used.ncpus = 1\n    resources_used.vmem = 1262344kb\n    resources_used.walltime = 00:19:27\n    job_state = R\n    queue = htc\n    server = casper-pbs\n...\n    exec_host = crhtc53/17*4+crhtc62/11*4\n...\n    project = _pbs_project_default\n    Submit_Host = chadmin4.hpc.ucar.edu\n</code></pre></p> <p>The command will create a lot of output, which is abbreviated above. Note specifically that PBS is reporting the amount of memory used so far.</p> <p>The execution hosts (compute nodes) are listed in the exec_host field. The job in the example above ran on two hosts, <code>crhtc53</code> and <code>crhtc62</code>. While the job is in a running state, it is possible to <code>ssh</code> directly to any of the named hosts and query basic diagnostic programs such as <code>top</code> (for node memory) and <code>nvidia-smi</code> (for GPU memory), or use more advanced tools such as <code>strace</code> and <code>gdb</code>. Typically, you will want to begin with the first execution host listed, as many applications perform file I/O on the first host so memory usage tends to be highest there. Be aware that your ssh session will be terminated abruptly when the PBS job ends and you no longer have batch processes assigned to the host(s).</p>"},{"location":"pbs/checking-memory-use/#understanding-and-resolving-various-memory-issues","title":"Understanding and resolving various memory issues","text":"<p>Issue 1. Your Derecho or Casper job shows signs of a memory issue (e.g., the job log ends with \"Killed\" or \"Bus Error\") despite <code>qhist</code> reporting a higher requested than used memory amount.</p> <p>The output from <code>qhist</code> indicates the total memory used and requested across all nodes in the job. This heuristic, while useful, can obfuscate memory usage patterns. Many scientific applications perform read/write operations on the \"head\" (primary) node of the job, and because data must be collected to this node to do I/O, memory usage can exceed what is available on that node while others use far less. Additionally, the PBS scheduler only records data at set intervals, so aliasing issues come into play for rapid increases in memory footprint. More sophisticated instrumentation like the temporal logging mode of <code>peak_memusage</code> can provide insight into these memory usage patterns.</p> <p>Issue 2. Your data analysis job on Casper begins to run slowly after a large array has been allocated, but no memory error is reported.</p> <p>It is likely that you are swapping data from RAM to disk. If possible, perform garbage collection in your language of choice to free memory. Otherwise, you will likely need to start a new job with a higher memory limit.</p> <p>Issue 3. Your batch job crashes shortly after execution and the output provides no explanation.</p> <p>If you suspect memory problems, it makes sense to examine recent changes to your code or runtime environment. Ask yourself questions like these:</p> <ul> <li> <p>When did you last run the job successfully? Was the model's resolution   increased since then?</p> </li> <li> <p>Did you port your code from a computer having more memory?</p> </li> <li> <p>Did you add new arrays or change array dimensions?</p> </li> <li> <p>Have you modified the batch job script?</p> </li> </ul> <p>If the answer to those questions is \u201cno,\u201d your job might be failing because you have exceeded your disk quota rather than because of a memory issue. To check, follow these steps:</p> <ul> <li> <p>Run the <code>gladequota</code> command.</p> </li> <li> <p>Check the \"<code>% Full</code>\" column in the command's output.</p> </li> <li> <p>Clean up any GLADE spaces that are at or near 100% full.</p> </li> <li> <p>Look for core files or other large files that recent jobs may have   created.</p> </li> <li> <p>Exceeding disk quota at runtime can result in symptoms that are   similar to those resulting from memory problems. If running gladequota   doesn't indicate a problem, consider using the ARM Forge DDT advanced   memory debugger to isolate the problem.</p> </li> </ul> <p>If you have tried all of the above and are still troubled that your job is exceeding usable memory, contact the NCAR Research Computing help desk with the relevant job number(s).</p>"},{"location":"pbs/common-causes-of-job-failures/","title":"Common causes of job failures","text":"<p>These are some of the most common causes of job failures on NCAR systems and some tips for how to avoid them. Keep an eye out for other tips, and change notices that you need to be aware of, in the\u00a0CISL Daily Bulletin.</p> <ol> <li> <p>Running close to the node or GPU memory limit.</p> <p>If you experience failures early in your job, especially at times when the program is reading data from disk or allocating data arrays, you may be running out of memory on compute nodes or GPUs. You may simply need to request more memory per node. Other potential solutions are to either spread your job across more nodes/GPUs (with fewer processes per resource) or shrink your problem size. See Checking memory use to determine how much memory your program or script requires to run.</p> </li> <li> <p>Home or other directory filling up.</p> <p>If you are seeing erratic job failures or experiencing failures for jobs that have run successfully many times in the past, you may have used up your disk quota. Run the <code>gladequota</code> command and clean up any storage spaces that are at or near 100% full.</p> </li> <li> <p>Specifying version-specific modules in your dotfiles.</p> <p>The problem with specifying version-specific modules in your dotfiles (.bashrc and .tcshrc, for example) is that someday the version you specify will be removed, and so the reproducibility of your job is reduced. You'll probably look at your batch job, not see anything wrong with it, and be puzzled, not realizing the problem is rooted in your dotfiles. In fact, it is best not to specify any modules in your dotfiles. Instead, include your environment modifications \u2013 any <code>module load</code> commands, for example \u2013 in your job script.</p> </li> <li> <p>Failure to clean directories when remaking executables and binaries.</p> <p>This failure can be puzzling because it looks like everything built correctly. When you run your application, it fails in a way that does not point to the root cause of the problem: that the build is using one or more binaries from previous, incompatible builds.</p> </li> <li> <p>Using old binaries with new module versions.</p> <p>Some software libraries \u2013 for example, many components of the Cray Programming Environment and NVIDIA HPC Toolkit \u2013 do not guarantee forward-compatibility with newer versions as they are released. Attempting to run binaries compiled with older versions of these libraries can result in strange runtime failures at launch. If you switch to a new version of a compiler, MPI, or GPU toolkit, it is best to rebuild your application.</p> </li> <li> <p>Filling up temporary file space.</p> <p>Using shared <code>/tmp</code>, <code>/var/tmp</code> or similar shared directories to hold temporary files can increase the risk of your own programs and other users' programs failing when no more space is available. Set <code>TMPDIR</code> to point to your GLADE scratch space (or the fast NVMe local scratch on Casper) in every script for all batch jobs. See Storing temporary files with <code>TMPDIR</code>.</p> </li> </ol>"},{"location":"pbs/job-scripts/","title":"Portable Batch System (PBS) Job Scripts","text":"<p>Job scripts form the basis of batch jobs. A job script is simply a text file with instructions of the work to execute.  Job scripts are usually written in <code>bash</code> or <code>tcsh</code> and thus mimic commands a user would execute interactively through a shell; but instead are executed on specific resources allocated by the scheduler when available.  Scripts can also be written in other languages - commonly Python.</p>"},{"location":"pbs/job-scripts/#anatomy-of-a-job-script","title":"Anatomy of a Job Script","text":"<p>Sample basic PBS scripts are listed below:</p> <p>PBS job scripts</p> bashtcshPython <pre><code>#!/bin/bash\n#PBS -N hello_pbs\n#PBS -A &lt;project_code&gt;\n#PBS -j oe\n#PBS -k eod\n#PBS -q main\n#PBS -l walltime=00:05:00\n#PBS -l select=2:ncpus=128:mpiprocs=128\n\n### Set temp to scratch\nexport TMPDIR=${SCRATCH}/${USER}/temp &amp;&amp; mkdir -p $TMPDIR\n\n### specify desired module environment\nmodule purge\nmodule load ncarenv/23.09 gcc/12.2.0 cray-mpich/8.1.25\nmodule list\n\n### Compile &amp; Run MPI Program\nmpicxx -o hello_world_derecho /glade/u/home/benkirk/hello_world_mpi.C -fopenmp\nmpiexec -n 256 -ppn 128 ./hello_world_derecho\n</code></pre> <p>The first line denotes the interpreter to be used for the script: <pre><code>#!/bin/bash\n</code></pre></p> <pre><code>#!/bin/tcsh\n#PBS -N hello_pbs\n#PBS -A &lt;project_code&gt;\n#PBS -j oe\n#PBS -k eod\n#PBS -q main\n#PBS -l walltime=00:05:00\n#PBS -l select=2:ncpus=128:mpiprocs=128\n\nsource /etc/csh.cshrc\n\n### Set temp to scratch\nsetenv TMPDIR ${SCRATCH}/${USER}/temp &amp;&amp; mkdir -p ${TMPDIR}\n\n### specify desired module environment\nmodule purge\nmodule load ncarenv/23.09 gcc/12.2.0 cray-mpich/8.1.25\nmodule list\n\n### Compile &amp; Run MPI Program\nmpicxx -o hello_world_derecho /glade/u/home/benkirk/hello_world_mpi.C -fopenmp\nmpiexec -n 256 -ppn 128 ./hello_world_derecho\n</code></pre> <p>The first line denotes the interpreter to be used for the script: <pre><code>#!/bin/tcsh\n</code></pre></p> <pre><code>#!/glade/u/apps/opt/conda/envs/npl/bin/python\n#PBS -N hello_pbs\n#PBS -A &lt;project_code&gt;\n#PBS -j oe\n#PBS -k eod\n#PBS -q main\n#PBS -l walltime=00:05:00\n#PBS -l select=1:ncpus=128:mpiprocs=128\n\nimport sys\nprint(\"Hello, world!!\\n\\n\")\n\nprint(\"Python version:\")\nprint(sys.version)\nprint(\"Version info:\")\nprint(sys.version_info)\n</code></pre> <p>The  first line denotes the interpreter to be used for the script: <pre><code>#!/glade/u/apps/opt/conda/envs/npl/bin/python\n</code></pre> indicates this is a <code>python</code> script (and, specifically, the NCAR NPL instance).</p> <p>Focusing on the <code>bash</code> example for discussion, the remainder of the script contains two main sections:</p> <ol> <li> <p>The lines beginning with <code>#PBS</code> are directives that will be interpreted by PBS when this script is submitted with <code>qsub</code>.     Each of these lines contains an instruction that will be used by <code>qsub</code> to control job resources, execution, etc...</p> </li> <li> <p>The remaining script contents are simply <code>bash</code> commands that will be run inside the batch environment on the selected resources and define the work to be done in this job.</p> </li> </ol>"},{"location":"pbs/job-scripts/#pbs-directives","title":"PBS directives","text":"<p>The example above contains several directives which are interpreted by the <code>qsub</code> submission program:</p> <ul> <li><code>-N hello_pbs</code> provides a job name.  This name will be displayed by the scheduler for diagnostic and file output.  If omitted, and a script is used to submit the job, the job's name is the  name  of  the  script.</li> <li><code>-A &lt;project_code&gt;</code> indicates which NCAR Project Accounting code resource allocation will be applicable to this job. (You will want to replace <code>&lt;project_code&gt;</code> with your project's specific code.)</li> <li><code>-j oe</code> requests we combine any standard text output (<code>o</code>) and error (<code>e</code>) into one output file.   (By default, PBS will write program output and error to different log files.  This behavior is contrary to what many users expect from terminal interaction, where output and error are generally interspersed. This optional flag changes that behavior.)</li> <li><code>-q main</code> specifies the desired PBS queue for this job.</li> <li><code>-l walltime=00:05:00</code> requests 5 minutes as the maximum job execution (walltime) time.  Specified in <code>HH:MM:SS</code> format.</li> <li><code>-l select=2:ncpus=128:mpiprocs=128</code> is a computational resource chunk request, detailing the quantity and configuration of compute nodes required for this job. This example requests a selection of 2 nodes, where each node must have 128 CPU cores, each of which we will use as an MPI rank in our application.</li> </ul>"},{"location":"pbs/job-scripts/#script-contents","title":"Script contents","text":"<p>The remaining script contains shell commands that define the job execution workflow.  The commands here are arbitrary, however we strongly recommend the general structure presented above.  This includes:</p> <ol> <li> <p>Explicitly setting the <code>TMPDIR</code> variable.</p> <p>As described here, many programs write temporary data to <code>TMPDIR</code>, which is usually small and shared among st users.  Specifying your own directory for temporary files can help you avoid the risk of your own programs and other users' programs failing when no more space is available.</p> </li> <li> <p>Loading and reporting the specific module environment required for this job.</p> <p>While strictly not necessary (in general, the system default modules will be loaded anyway), we recommend this as best practice as it facilitates debugging and reproducing later.  (While the system default modules will change over time, manually specifying module versions allows you to recreate the same execution environment in the future.)</p> </li> <li> <p>(Optional) Defining any environment variables specific to the chosen module environment.</p> <p>Occasionally users will want to define particular run time environment variables e.g. for a specific MPI or library chosen via the <code>module load</code> commands.</p> </li> <li> <p>Remaining job-specific steps.</p> <p>In the example above, we first compile and then execute <code>hello_world_mpi.C</code>, a simple MPI program.</p> </li> </ol>"},{"location":"pbs/job-scripts/#common-pbs-directives","title":"Common <code>#PBS</code> directives","text":""},{"location":"pbs/job-scripts/#resource-requests","title":"Resource requests","text":"<p>Resources (compute node configuration, job duration) are requested through a combination of resource selection flags, each preceded with <code>-l</code>.</p> <p>For example: <pre><code>#PBS -l walltime=00:05:00\n#PBS -l select=1:ncpus=64:mpiprocs=4:ngpus=4:mem=400GB\n#PBS -l gpu_type=a100\n#PBS -l job_priority=economy\n</code></pre> specifies job <code>walltime</code>, compute node selection, GPU type, and job priority.  See more details below.</p>"},{"location":"pbs/job-scripts/#select-statements","title":"<code>select</code> statements","text":"<p>Resources are specified through a <code>select</code> statement.  The general form of a homogeneous selection statement is <pre><code>select=&lt;# NODES&gt;:ncpus=&lt;# CPU Cores/node&gt;:mpiprocs=&lt;# MPI Ranks/node&gt;:ompthreads=&lt;# OpenMP Threads/rank&gt;:mem=&lt;RAM/node&gt;:ngpus=&lt;# GPUs/node&gt;\n</code></pre> where</p> <ul> <li> <p><code>&lt;# NODES&gt;</code> is the total number of compute nodes requested, followed by a colon-separated list of</p> </li> <li> <p><code>&lt;# CPU Cores/node&gt;</code> is the total number of CPUs requested on each node, which can be a mix of MPI Ranks and/or OpenMP threads,</p> </li> <li> <p><code>&lt;# MPI Ranks/node</code> is the number of MPI Ranks on each node,</p> </li> <li> <p><code>&lt;# OpenMP Threads/node&gt;</code> is the number of OpenMP ranks per MPI Rank on each node. (Optional, defaults to 1),</p> </li> <li> <p><code>&lt;RAM/node&gt;</code> is how much main memory (RAM) the job will be able to access on each node. (Optional, default is system dependent), and</p> </li> <li> <p><code>&lt;# GPUs/node&gt;</code> is the number of GPUs per node. (Optional, defaults to 0).</p> </li> </ul> <p>Taken together, this specifies a resource chunk. Homogeneous resource chunks are the most common case, however, heterogeneous selection statements can be constructed by multiple chunks separated by a + (examples below).</p>"},{"location":"pbs/job-scripts/#examples","title":"Examples","text":"<ul> <li> <p>4 128-core nodes, each running 128 MPI ranks (4 <code>x</code> 128 = 512 MPI ranks total).    <pre><code>select=4:ncpus=128:mpiprocs=128\n</code></pre></p> </li> <li> <p>4 128-core nodes, each running 32 MPI ranks with 4 OpenMP threads per ranl (4 <code>x</code> 32 = 128 MPI ranks total, each with 4 threads = 512 total CPU cores).    <pre><code>select=4:ncpus=128:mpiprocs=32:ompthreads=4\n</code></pre></p> </li> <li> <p>2 64-core nodes, each running 4 MPI ranks, 4 GPUS, and 384 GB memory (8 GPUs total, with 8 MPI ranks).    <pre><code>select=2:ncpus=64:mpiprocs=4:ngpus=4:mem=384GB\n</code></pre></p> </li> <li> <p>4 36-core nodes, each running 4 MPI ranks, 4 GPUS configured with NVIDIA's Multi-Process Service (MPS), and 768 GB memory (16 GPUs total, with 16 MPI ranks).    <pre><code>select=4:ncpus=36:mpiprocs=4:ngpus=4:mem=768GB:mps=1\n</code></pre>    MPS is simply enabled via <code>mps=1</code>, and is disabled by default (<code>mps=0</code>)</p> </li> <li> <p>A heterogeneous selection, 96 128-core nodes each with 128 MPI ranks, and 32 128-core nodes each with 16 MPI ranks and 8 OpenMP threads    <pre><code>select=96:ncpus=128:mpiprocs=128+32:ncpus=16:ompthreads=8\n</code></pre></p> </li> </ul> <p>The particular values for <code>ncpus</code>, <code>mem</code>, <code>ngpus</code> are node-type dependent, and most NCAR systems have more than one available node type. (See system specific documentation for recommended values.)</p> <p>Request all <code>ncpus</code> when running on exclusive nodes</p> <p>For large multi-node jobs on machines like Derecho nodes are usually assigned exclusively to a single PBS job at a time.  For most use cases, users will request the maximum number of CPUS available via <code>ncpus</code>, and consume all through a combination of <code>mpiprocs</code> and <code>ompthreads</code>.</p> <p>Occasionally users may want fewer than the maximum CPUs for computation, \"under-subscribing\" compute nodes.  This is usually done for memory intensive applications, where some cores are intentionally left idle in order to increase the memory available for the running cores.  In such circumstances users should still request access to all CPUs, but only use a subset.  For example <pre><code>select=4:ncpus=128:mpiprocs=64:ompthreads=1:mem=235GB\n</code></pre> requests access to all 128 CPUs on a dedicated node, but only assigns 64 for MPI use.</p> <p>By requesting access to all <code>ncpus=128</code> is recommended for this case because it allows optimally locating the actually used <code>mpiprocs=64</code> across the compute node via process binding.</p>"},{"location":"pbs/job-scripts/#walltime","title":"<code>walltime</code>","text":"<p>The <code>-l walltime=HH:MM:SS</code> resource directive specifies maximum job duration. Jobs still running when this wall time is exceeded will be terminated automatically by the scheduler. <pre><code>walltime=HH:MM:SS\n</code></pre></p>"},{"location":"pbs/job-scripts/#job_priority","title":"<code>job_priority</code>","text":"<p>Users may request a specific job priority with the <code>-l job_priority=...</code> resource directive. Valid options are: <pre><code>job_priority=&lt;regular|premium|economy&gt;\n</code></pre> Job priority impacts both scheduling and resource accounting, allowing users to run at a higher/lower priority in exchange for additional/reduced allocation consumption.  See here for additional information.</p>"},{"location":"pbs/job-scripts/#gpu_type","title":"<code>gpu_type</code>","text":"<p>For highly heterogeneous systems such as Casper, a resource chunk statement including GPUS may match more than one particular GPU type. The resource specification <code>-l gpu_type=...</code> requests a particular GPU type, removing such ambiguity.  Valid options are: <pre><code>gpu_type=&lt;gp100|v100|a100&gt;\n</code></pre></p>"},{"location":"pbs/job-scripts/#listing-of-frequently-used-pbs-directives","title":"Listing of frequently used <code>#PBS</code> directives","text":"Directive Impact <code>-A &lt;project_code&gt;</code> NCAR Project Accounting  string  associated  with  the  job. <code>-a &lt;date at time&gt;</code> Allows users to request a future  eligible time for job execution.(By default jobs are considered immediately eligible for execution.) Format: <code>[[[YY]MM]DD]hhmm[.SS]</code> <code>-h</code> Holds the job.&lt;Held jobs can be released with <code>qrls</code>. <code>-I</code> Specifies interactive execution.Interactive jobs place the user a login session on the first compute node.Interactive jobs terminate when the shell exits, or <code>walltime</code> is exceeded. <code>-J &lt;range&gt;</code> Specifies an array job.Use  the range argument to specify the indices of the sub jobs of the array.  range is specified in the form <code>X-Y[:Z]</code> where <code>X</code> is the first index, <code>Y</code> is the upper bound on the indices, and <code>Z</code>  is  the  stepping factor. Indices must be greater than or equal to zero.Use the optional <code>%max_subjobs</code> argument to set a limit on the number of subjobs that can be running  at  one time.more details on array jobs <code>-m &lt;mail events&gt;</code> Sends email on specific events (may be combined).<code>n</code>: No mail is sent<code>a</code>: Mail is sent when the job is aborted by the batch system<code>b</code>: Mail is sent when the job begins execution<code>e</code>:  Mail is sent when the job terminatesExample: <code>-m abe</code> <code>-M &lt;address(es)&gt;</code> List of users to whom mail about the job is sent.The user list argument has the form: <code>&lt;username&gt;[@&lt;hostname&gt;][,&lt;username&gt;[@&lt;hostname&gt;],...]</code> <p><code>qsub</code> arguments take precedence over <code>#PBS</code> directives</p> <p>Best practice is to fully specify your PBS queue, job name, and resources in your job script as shown above. This allows for better debugging and facilitates reproducing runs in the future.    When a job's PBS attributes are fully specified, you can usually submit the script with no additional arguments, for example <pre><code>qsub script.pbs\n</code></pre> (See Running Jobs for more details on interacting with the scheduler.)</p> <p>On occasion users may want to change some of the PBS parameters without modifying the job script.  A common example may be the account code, the job name (<code>-N</code>) or even the <code>walltime</code>.</p> <p>Any <code>#PBS</code> directives specified in the job script can be overridden at submission time by equivalent arguments to <code>qsub</code>. For example, <pre><code> qsub -A &lt;OTHER_ACCOUNT&gt; \\\n      -N testing \\\n      -l job_priority=premium \\\n      script.pbs\n</code></pre> will run <code>script.pbs</code> under the specified <code>&lt;OTHER_ACCOUNT&gt;</code> with the job name <code>testing</code> and requests <code>premium</code> priority, regardless of what other values may be specified in <code>script.pbs</code></p>"},{"location":"pbs/job-scripts/#execution-environment-variables","title":"Execution environment variables","text":"<p>Within the script contents of the job script, it is common for the specifics of the job to depend slightly on the PBS and specific <code>module</code> execution environment.  Both running under PBS and loading certain module files create some environment variables that might be useful when writing portable scripts; for example scripts that might be shared among users or executed within several different configurations.</p> <p>Use common environment variables to write portable PBS batch scripts</p> <p>Avoid hard-coding paths into your shell scripts if instead any of the environment variables below might be used.  This will facilitate moving scripts between systems, users, and application versions with minimal modifications, as output paths can be defined generically as opposed to hard-coded for each user.</p>"},{"location":"pbs/job-scripts/#pbs-execution-environment-variables","title":"PBS execution environment variables","text":"<p>PBS creates a number of environment variables that are accessible within a job's execution environment. Some of the more useful ones are:</p> Variable Value <code>PBS_ACCOUNT</code> The NCAR Project Accounting code used for this job. <code>PBS_JOBID</code> The PBS Job ID for this job.Example: <code>1473351.desched1</code> <code>PBS_JOBNAME</code> The name of this job. Matches the <code>-N</code> specified.Example: <code>hello_pbs</code> <code>PBS_O_WORKDIR</code> The working directory from where the job was submitted. <code>PBS_SELECT</code> The resource specification <code>-l select=</code> line for this job.This can be useful for setting runtime-specific configuration options that might depend on resource selection.(e.g. processor layout, CPU binding, etc...)Example: <code>2:ncpus=128:mpiprocs=2:ompthreads=2:mem=200GB:Qlist=cpu:ngpus=0</code> <code>PBS_NODEFILE</code> A file whose contents lists the nodes assigned to this job. Typically listed as one node name per line, for each MPI rank in the job.Each node will be listed for as many times as it has MPI ranks. Example: <code>/var/spool/pbs/aux/1473351.desched1</code>"},{"location":"pbs/job-scripts/#ncar-module-specific-execution-environment-variables","title":"NCAR <code>module</code>-specific execution environment variables","text":"Variable Value <p>Machine and Software Environment</p> <code>NCAR_HOST</code> Specifies the host class of machine, e.g. <code>derecho</code> or <code>casper</code> <code>NCAR_BUILD_ENV_COMPILER</code> A unique string identifying the host and compiler+version currently loaded.Example: <code>casper-oneapi-2023.2.1</code> <code>NCAR_BUILD_ENV_MPI</code> A unique string identifying the host, compiler+version, and mpi+version currently loaded. Example: <code>casper-oneapi-2023.2.1-openmpi-4.1.5</code> <code>NCAR_BUILD_ENV</code> A unique string identifying the current build environment, identical to <code>NCAR_BUILD_ENV_MPI</code> when an MPI module is loaded, or <code>NCAR_BUILD_ENV_COMPILER</code> if only a compiler is loaded. <code>LMOD_FAMILY_COMPILER</code><code>LMOD_FAMILY_COMPILER_VERSION</code> Specifies the type and version of compiler currently loaded, if any.Example:<code>intel</code>, <code>gcc</code>, <code>nvhpc</code> <code>LMOD_FAMILY_MPI</code><code>LMOD_FAMILY_MPI_VERSION</code> Specifies the type and version of MPI currently loaded, if any.Example:<code>openmpi</code>, <code>cray-mpich</code>, <code>intel-mpi</code> <p>User and File System Paths</p> <code>${USER}</code> The <code>username</code> of user executing the script. <code>${HOME}</code> The GLADE home file space for the user executing the script.Example: <code>/glade/u/home/${USER}</code> <code>${WORK}</code> The GLADE work file space for the user executing the script.Example: <code>/glade/work/${USER}</code> <code>${SCRATCH}</code> The GLADE scratch file space for the user executing the script.Example: <code>/glade/derecho/scratch/${USER}</code>"},{"location":"pbs/job-scripts/#pbs-job-arrays","title":"PBS Job Arrays","text":"<p>Occasionally users may want to execute a large number of similar jobs. Such workflows may arise when post-processing a large number of similar files, for example, often with a serial post-processing tool. One approach is simply to create a unique job script for each.  While simple, this approach has some drawbacks, namely scheduler overhead and job management complexity.</p> <p>PBS provides a convenient job array mechanism for such cases. When using job arrays, the queue script contents can be thought of as a template that is applied repeatedly, for different instances of the <code>PBS_ARRAY_INDEX</code>.  Consider the following example:</p> <p>PBS job array</p> <p>Suppose your working directory contains a number of files <code>data.year-2010</code>, <code>data.year-2011</code>, ..., <code>data.year-2020</code>.  You would like to run <code>./executable_name</code> on each. job_array.pbs<pre><code>#PBS -N job_array\n#PBS -A &lt;project_code&gt;\n### Each array sub-job will be assigned a single CPU with 4 GB of memory\n#PBS -l select=1:ncpus=1:mem=4GB\n#PBS -l walltime=00:10:00\n#PBS -q casper\n### Request 11 sub-jobs with array indices spanning 2010-2020 (input year)\n#PBS -J 2010-2020\n#PBS -j oe\n\nexport TMPDIR=${SCRATCH}/temp\nmkdir -p ${TMPDIR}\n\n### Run program on a specific input file:\n./executable_name data.year-${PBS_ARRAY_INDEX}\n</code></pre></p> <p>The directive <code>-J 2010-2020</code> instructs the scheduler to launch a number of sub-jobs, each with a distinct value of <code>PBS_ARRAY_INDEX</code> spanning from 2010 to 2020 (inclusive).</p> <p>The <code>-l select=1:ncpus=1:mem=4GB</code> resource chunk and <code>-l walltime=00:10:00</code> run time limit applies to each sub-job.</p> <p>PBS job arrays provide a convenient mechanism to launch a large number of small, similar jobs. The approach outlined above is particularly suitable for Casper, where nodes are typically shared and individual CPU cores are scheduled.  This allows a job array sub-job to be as small as a single core.</p> <p>When entire compute nodes are assigned to jobs (and therefore also array sub-jobs) we need a slightly different approach, as employed in the following use case.</p>"},{"location":"pbs/job-scripts/#using-job-arrays-to-launch-a-command-file","title":"Using job arrays to launch a \"command file\"","text":"<p>Multiple Program, Multiple Data (MPMD) jobs run multiple independent, typically serial executables simultaneously. Such jobs can easily be dispatched with PBS job arrays, even on machines like Derecho where compute nodes are exclusively and entirely assigned to a users' job. This process is outlined in the example below.</p> <p>Command File / MPMD jobs with PBS Job Arrays</p> <p>For this example, executable commands appear in a command file (<code>cmdfile</code>).</p> cmdfile<pre><code># this is a comment line for demonstration\n./cmd1.exe &lt; input1 # comments are allowed for steps too, but not required\n./cmd2.exe &lt; input2\n./cmd3.exe &lt; input3\n...\n./cmdN.exe &lt; inputN\n</code></pre> <p>The command file, executables, and input files should all reside in the directory from which the job is submitted. If they don't, you need to specify adequate relative or full paths in both your command file and job scripts.  The sub-jobs will produce output files that reside in the directory in which the job was submitted. The command file can then be executed with the <code>launch_cf</code> command.</p> <p>Examples: <pre><code># launches the commands listed in ./cmdfile:\nlaunch_cf -A PBS_ACCOUNT -l walltime=1:00:00\n\n# launches the OpenMP-threaded commands listed in ./omp_cmdfile:\n#  (requires ppn=128 = (32 steps/node) * (4 threads/step)\nlaunch_cf -A PBS_ACCOUNT -l walltime=1:00:00 --nthreads 4 --steps-per-node 32 ./omp_cmdfile\n</code></pre></p> <p>The jobs listed in the <code>cmdfile</code> will be launched from a <code>bash</code> shell in the users default environment.  The optional file <code>config_env.sh</code> will be \"sourced\" from the run directory in case environment modification is required, for example to load a particular <code>module</code> environment, to set file paths, etc...</p> <p>The command will assume reasonable defaults on Derecho and Casper for the number of job \"steps\" from the <code>cmdfile</code> to run per node, memory per node, PBS queues, etc... Each of these parameters can be controlled via <code>launch_cf</code> command line arguments, see <code>launch_cf --help</code>: launch_cf command line options<pre><code>launch_cf &lt;-h|--help&gt;\n     &lt;--queue PBS_QUEUE&gt;\n     &lt;--ppn|--processors-per-node #CPUS&gt;\n     &lt;--steps-per-node #Steps/node&gt;\n     &lt;--nthreads|--threads-per-step #Threads/step&gt;\n     &lt;--mem|--memory RAM/node&gt;\n     -A PBS_ACCOUNT -l walltime=01:00:00\n     ... other PBS args ...\n     &lt;command file&gt;\n\n------------------------------------------------------------------\nAll options in \"&lt;&gt;\" brackets are optional.\nAny unrecognized arguments are passed through directly to qsub.\nThe PBS options -A and -l walltime are required at minimum.\n</code></pre></p> <p>The two PBS required arguments are <code>-A &lt;project_code&gt;</code> and <code>-l walltime=...</code>. Any command line argument not interpreted directly by <code>launch_cf</code> are assumed PBS arguments and are passed along to <code>qsub</code>.</p> <p>Discussion</p> <p>This PBS array implementation is a departure from the command file technique used previously on Cheyenne, where MPI was used to launch the desired commands on each rank.  While slightly more complex, the array approach has several advantages.  Since the array steps are independent, the job can begin execution as soon as even a single node is available, and can scale to fill the available resources.</p> <p>Additionally, the array approach is well suited for when the run times of the specific commands varies. In the previous MPI approach, all nodes were held until the slowest step completed, with the consequence of idle resources for varied command run times. With the array approach each node completes independently, when the slowest of its unique steps has completed.  Thus the utilization of each node is controlled by the run times of its own steps, rather than all steps.</p> <p>The implementation details are unimportant for general users exercising this capability, however may be interesting for advanced users wishing to leverage PBS job arrays in different scenarios.  See the hpc-demos GitHub repository for source code.</p>"},{"location":"pbs/job-scripts/#sample-pbs-job-scripts","title":"Sample PBS job scripts","text":""},{"location":"pbs/job-scripts/#casper","title":"Casper","text":"<p>Batch script to run a high-throughput computing (HTC) job on Casper</p> <p>This example shows how to create a script for running a high-throughput computing (HTC) job. Such jobs typically use only a few CPU cores and likely do not require the use of an MPI library or GPU.</p> bashtcsh <pre><code>#!/bin/bash -l\n### Job Name\n#PBS -N htc_job\n### Charging account\n#PBS -A &lt;project_code&gt;\n### Request one chunk of resources with 1 CPU and 10 GB of memory\n#PBS -l select=1:ncpus=1:mem=10GB\n### Allow job to run up to 30 minutes\n#PBS -l walltime=30:00\n### Route the job to the casper queue\n#PBS -q casper\n### Join output and error streams into single file\n#PBS -j oe\n\nexport TMPDIR=${SCRATCH}/temp\nmkdir -p ${TMPDIR}\n\n### Load Conda/Python module and activate NPL environment\nmodule load conda\nconda activate npl\n\n### Run analysis script\npython myscript.py datafile.dat\n</code></pre> <pre><code>#!/bin/tcsh\n### Job Name\n#PBS -N htc_job\n### Charging account\n#PBS -A &lt;project_code&gt;\n### Request one chunk of resources with 1 CPU and 10 GB of memory\n#PBS -l select=1:ncpus=1:mem=10GB\n### Allow job to run up to 30 minutes\n#PBS -l walltime=30:00\n### Route the job to the casper queue\n#PBS -q casper\n### Join output and error streams into single file\n#PBS -j oe\n\nsetenv TMPDIR ${SCRATCH}/temp\nmkdir -p ${TMPDIR}\n\n### Load Conda/Python module and activate NPL environment\nmodule load conda\nconda activate npl\n\n### Run analysis script\npython myscript.py datafile.dat\n</code></pre> <p>Batch script to run an MPI GPU job on Casper</p> bashtcsh <pre><code>#!/bin/bash -l\n#PBS -N mpi_job\n#PBS -A &lt;project_code&gt;\n#PBS -l select=2:ncpus=4:mpiprocs=4:ngpus=4:mem=40GB\n#PBS -l gpu_type=v100\n#PBS -l walltime=01:00:00\n#PBS -q casper\n#PBS -j oe\n\nexport TMPDIR=${SCRATCH}/temp\nmkdir -p ${TMPDIR}\n\n### Provide CUDA runtime libraries\nmodule load cuda\n\n### Run program\nmpirun ./executable_name\n</code></pre> <pre><code>#!/bin/tcsh\n### Job Name\n#PBS -N mpi_gpu_job\n### Charging account\n#PBS -A &lt;project_code&gt;\n### Request two resource chunks, each with 4 CPUs, GPUs, MPI ranks, and 40 GB of memory\n#PBS -l select=2:ncpus=4:mpiprocs=4:ngpus=4:mem=40GB\n### Specify that the GPUs will be V100s\n#PBS -l gpu_type=v100\n### Allow job to run up to 1 hour\n#PBS -l walltime=01:00:00\n### Route the job to the casper queue\n#PBS -q casper\n### Join output and error streams into single file\n#PBS -j oe\n\nsetenv TMPDIR ${SCRATCH}/temp\nmkdir -p ${TMPDIR}\n\n### Provide CUDA runtime libraries\nmodule load cuda\n\n### Run program\nmpirun ./executable_name\n</code></pre> <p>Batch script to run a pure OpenMP job on Casper</p> bashtcsh <pre><code>#!/bin/bash -l\n#PBS -N OpenMP_job\n#PBS -A &lt;project_code&gt;\n#PBS -l select=1:ncpus=8:ompthreads=8\n#PBS -l walltime=00:10:00\n#PBS -q casper\n#PBS -j oe\n\nexport TMPDIR=${SCRATCH}/temp\nmkdir -p ${TMPDIR}\n\n### Run program\n./executable_name\n</code></pre> <pre><code>#!/bin/tcsh\n#PBS -N OpenMP_job\n#PBS -A &lt;project_code&gt;\n#PBS -l select=1:ncpus=8:ompthreads=8\n#PBS -l walltime=00:10:00\n#PBS -q casper\n#PBS -j oe\n\nsetenv TMPDIR ${SCRATCH}/temp\nmkdir -p ${TMPDIR}\n\n### Run program\n./executable_name\n</code></pre> <p>Batch script to run a hybrid MPI/OpenMP job on Casper</p> bashtcsh <pre><code>#!/bin/bash -l\n#PBS -N hybrid_job\n#PBS -A &lt;project_code&gt;\n#PBS -l select=2:ncpus=8:mpiprocs=2:ompthreads=4\n#PBS -l walltime=00:10:00\n#PBS -q casper\n#PBS -j oe\n\nexport TMPDIR=${SCRATCH}/temp\nmkdir -p ${TMPDIR}\n\n### Run program\nmpirun ./executable_name\n</code></pre> <pre><code>#!/bin/tcsh\n#PBS -N hybrid_job\n#PBS -A &lt;project_code&gt;\n#PBS -l select=2:ncpus=8:mpiprocs=2:ompthreads=4\n#PBS -l walltime=00:10:00\n#PBS -q casper\n#PBS -j oe\n\nsetenv TMPDIR ${SCRATCH}/temp\nmkdir -p ${TMPDIR}\n\n### Run program\nmpirun ./executable_name\n</code></pre> <p>Batch script to run a job array on Casper</p> <p>Job arrays are useful for submitting and managing collections of similar jobs \u2013 for example, running the same program repeatedly on different input files. PBS can process a job array more efficiently than it can process the same number of individual non-array jobs.</p> <p>This example uses environment variable <code>PBS_ARRAY_INDEX</code> as an argument in running the jobs. This variable is set by the scheduler in each of your array subjobs, and spans the range of values set in the <code>#PBS -J</code> array directive.</p> bashtcsh <pre><code>#!/bin/bash -l\n#PBS -N job_array\n#PBS -A &lt;project_code&gt;\n### Each array subjob will be assigned a single CPU with 4 GB of memory\n#PBS -l select=1:ncpus=1:mem=4GB\n#PBS -l walltime=00:10:00\n#PBS -q casper\n### Request 10 subjobs with array indices spanning 2010-2020 (input year)\n#PBS -J 2010-2020\n#PBS -j oe\n\nexport TMPDIR=${SCRATCH}/temp\nmkdir -p ${TMPDIR}\n\n### Run program\n./executable_name data.year-$PBS_ARRAY_INDEX\n</code></pre> <pre><code>#!/bin/tcsh\n#PBS -N job_array\n#PBS -A &lt;project_code&gt;\n### Each array subjob will be assigned a single CPU with 4 GB of memory\n#PBS -l select=1:ncpus=1:mem=4GB\n#PBS -l walltime=01:00:00\n#PBS -q casper\n### Request 10 subjobs with array indices spanning 2010-2020 (input year)\n#PBS -J 2010-2020\n#PBS -j oe\n\nsetenv TMPDIR ${SCRATCH}/temp\nmkdir -p ${TMPDIR}\n\n### Run program\n./executable_name data.year-$PBS_ARRAY_INDEX\n</code></pre> <p>If you need to include a job ID in a subsequent <code>qsub</code> command, be sure to use quotation marks to preserve the <code>[]</code> brackets, as in this example: <pre><code>qsub -W \"depend=afterok:317485[]\" postprocess.pbs\n</code></pre></p> <p>Using NVIDIA MPS in Casper GPU jobs</p> <p>Some workflows benefit from processing more than one CUDA kernel on a GPU concurrently, as a single kernel is not sufficient to keep the GPU fully utilized. NVIDIA\u2019s Multi-Process Service (MPS) enables this capability on modern NVIDIA GPUs like the V100s on Casper.</p> <p>Consider using MPS when you are requesting more MPI tasks than physical GPUs. Particularly for jobs with large problem sizes, using multiple MPI tasks with MPS active can sometimes offer a performance boost over using a single task per GPU.</p> <p>The PBS job scheduler provides MPS support via a chunk-level resource. When you request MPS, PBS will perform the following steps on each specified chunk:</p> <ol> <li> <p>Launch the MPS control daemon on each job node.</p> </li> <li> <p>Start the MPS server on each node.</p> </li> <li> <p>Run your GPU application.</p> </li> <li> <p>Terminate the MPS server and daemon.</p> </li> </ol> <p>To enable MPS on job hosts, add <code>mps=1</code> to your select statement chunks as follows: <pre><code>#PBS -l select=1:ncpus=8:mpiprocs=8:mem=60GB:ngpus=1:mps=1\n</code></pre> On each V100 GPU, you may use MPI to launch up to 48 CUDA contexts (GPU kernels launched by MPI tasks) when using MPS. MPS can be used with OpenACC and OpenMP offload codes as well, as the compiler generates CUDA code from your directives at compile time.</p> <p>Jobs may not request MPS activation on nodes with GP100 GPUs.</p> <p>In this example, we run a CUDA Fortran program that also uses MPI. The application was compiled using the NVIDIA HPC SDK compilers, the CUDA toolkit, and Open MPI. We request all GPUs on each node and use NVIDIA MPS to use multiple MPI tasks on CPU nodes for each GPU.</p> bash <pre><code>#!/bin/bash\n#PBS -A &lt;project_code&gt;\n#PBS -N gpu_mps_job\n#PBS -q casper@casper-pbs\n#PBS -l walltime=01:00:00\n#PBS -l select=2:ncpus=36:mpiprocs=36:ngpus=4:mem=300GB:mps=1\n#PBS -l gpu_type=v100\n\n# Use scratch for temporary files to avoid space limits in /tmp\nexport TMPDIR=${SCRATCH}/temp\nmkdir -p ${TMPDIR}\n\n# Load modules to match compile-time environment\nmodule purge\nmodule load ncarenv nvhpc/22.5 cuda/11.4 openmpi/4.1.4\n\n# Run application using Open MPI\nmpirun ./executable_name\n</code></pre>"},{"location":"pbs/job-scripts/#derecho","title":"Derecho","text":"<p>Running a hybrid CPU program with MPI and OpenMP on Derecho</p> <p>In this example, we run a hybrid application that uses both MPI tasks and OpenMP threads. The executable was compiled using default modules (Intel compilers and MPI). We use a 2 nodes with 32 MPI ranks on each node and 4 OpenMP threads per MPI rank.</p> <p>Whenever you run a program that compiled with OpenMP support, it is important to provide a value for ompthreads in the select statement; PBS will use that value to define the\u00a0<code>OMP_NUM_THREADS</code>\u00a0environment variable.</p> <pre><code>#!/bin/bash\n#PBS -A &lt;project_code&gt;\n#PBS -N hybrid_job\n#PBS -q main\n#PBS -l walltime=01:00:00\n#PBS -l select=2:ncpus=128:mpiprocs=32:ompthreads=4\n\n# Use scratch for temporary files to avoid space limits in /tmp\nexport TMPDIR=${SCRATCH}/temp\nmkdir -p ${TMPDIR}\n\n# Load modules to match compile-time environment\nmodule purge\nmodule load ncarenv/22.12 oneapi/2022.2.1 craype/2.7.19 cray-mpich/8.1.21\n\n# Run application using cray-mpich with binding\nmpiexec --cpu-bind depth -n 64 -ppn 32 -d 4 ./executable_name\n</code></pre> <p>Running an MPI-enabled GPU application on Derecho</p> <p>In this example, we run an MPI CUDA program. The application was compiled using the NVIDIA HPC SDK compilers, the CUDA toolkit, and <code>cray-mpich</code> MPI. We request all four GPUs on each of two nodes.</p> <p>Please ensure that you have the <code>cuda</code> module loaded as shown below when attempting to run GPU applications or nodes may lock up and become unresponsive.</p> <pre><code>#!/bin/bash\n#PBS -A &lt;project_code&gt;\n#PBS -N gpu_job\n#PBS -q main\n#PBS -l walltime=01:00:00\n#PBS -l select=2:ncpus=64:mpiprocs=4:ngpus=4\n\n# Use scratch for temporary files to avoid space limits in /tmp\nexport TMPDIR=${SCRATCH}/temp\nmkdir -p ${TMPDIR}\n\n# Load modules to match compile-time environment\nmodule purge\nmodule load nvhpc cuda cray-mpich\n\n# (Optional: Enable GPU managed memory if required.)\n#   From \u2018man mpi\u2019: This setting will allow MPI to properly\n#   handle unify memory addresses. This setting has performance\n#   penalties as MPICH will perform buffer query on each buffer\n#   that is handled by MPI)\n# If you see runtime errors like\n# (GTL DEBUG: 0) cuIpcGetMemHandle: invalid argument,\n#  CUDA_ERROR_INVALID_VALUE\n# make sure this variable is set\nexport MPICH_GPU_MANAGED_MEMORY_SUPPORT_ENABLED=1\n\n# Run application using the cray-mpich MPI\n#   The \u2018set_gpu_rank\u2019 command is a script that sets several GPU-\n#   related environment variables to allow MPI-enabled GPU\n#   applications to run. The set_gpu_rank script is detailed\n#   in the binding section below, and is also made available\n#   via the ncarenv module.\nmpiexec -n 8 -ppn 4 set_gpu_rank ./executable_name\n</code></pre> <p>Binding MPI ranks to CPU cores and GPU devices on Derecho</p> <p>For some GPU applications, you may need to explicitly control the mapping between MPI ranks and GPU devices (see man mpi). One approach is to manually control the <code>CUDA_VISIBLE_DEVICES</code> environment variable so a given MPI rank only \u201csees\u201d a subset of the GPU devices on a node.</p> <p>Consider the following shell script: set_gpu_rank<pre><code>#!/bin/bash\n\nexport MPICH_GPU_SUPPORT_ENABLED=1\nexport LOCAL_RANK=${PMI_LOCAL_RANK}\nexport GLOBAL_RANK=${PMI_RANK}\nexport CUDA_VISIBLE_DEVICES=$(expr ${LOCAL_RANK} % 4)\n\necho \"Global Rank ${GLOBAL_RANK} / Local Rank ${LOCAL_RANK} / CUDA_VISIBLE_DEVICES=${CUDA_VISIBLE_DEVICES} / $(hostname)\"\n\nexec $*\n</code></pre> It can be used underneath mpiexec to bind an MPI process to a particular GPU:</p> <pre><code>#PBS -l select=2:ncpus=64:mpiprocs=4:ngpus=4\n...\n# Run application using the cray-mpich MPI, binding the local\n# mpi rank [0-3] to corresponding GPU index [0-3]:\nmpiexec -n 8 -ppn 4 ./set_gpu_rank ./executable_name\n</code></pre> <p>The command above will launch a total of 8 MPI ranks across 2 nodes, using 4 MPI ranks per node, and each rank will have dedicated access to one of the 4 GPUs on the node. Again, see <code>man mpi</code> for other examples and scenarios.</p> <p>Binding MPI ranks to CPU cores can also be an important performance consideration for GPU-enabled codes, and can be done with the <code>--cpu-bind</code> option to <code>mpiexec</code>. For the above example using 2 nodes, 4 MPI ranks per node, and 1 GPU per MPI rank, binding each of the MPI ranks to one of the four separate NUMA domains within a node is likely to be optimal for performance. This could be done as follows: <pre><code>mpiexec -n 8 -ppn 4 --cpu-bind verbose,list:0:16:32:48 ./set_gpu_rank ./executable_name\n</code></pre></p> <p>Running a containerized application  under MPI on GPUs</p> <pre><code>#!/bin/bash\n#PBS -q main\n#PBS -j oe\n#PBS -o fasteddy_job.log\n#PBS -l walltime=02:00:00\n#PBS -l select=6:ncpus=64:mpiprocs=4:ngpus=4\n\nmodule load ncarenv/23.09\nmodule load apptainer gcc cuda || exit 1\nmodule list\n\nnnodes=$(cat ${PBS_NODEFILE} | sort | uniq | wc -l)\nnranks=$(cat ${PBS_NODEFILE} | sort | wc -l)\nnranks_per_node=$((${nranks} / ${nnodes}))\n\ncontainer_image=\"./rocky8-openhpc-fasteddy.sif\"\n\nsingularity \\\n    --quiet \\\n    exec \\\n    ${container_image} \\\n    ldd /opt/local/FastEddy-model/SRC/FEMAIN/FastEddy\n\nsingularity \\\n    --quiet \\\n    exec \\\n    --bind ${SCRATCH} \\\n    --bind ${WORK} \\\n    --pwd $(pwd) \\\n    --bind /run \\\n    --bind /opt/cray \\\n    --bind /usr/lib64:/host/lib64 \\\n    --env LD_LIBRARY_PATH=${CRAY_MPICH_DIR}/lib-abi-mpich:/opt/cray/pe/lib64:${LD_LIBRARY_PATH}:/host/lib64 \\\n    --env LD_PRELOAD=/opt/cray/pe/mpich/${CRAY_MPICH_VERSION}/gtl/lib/libmpi_gtl_cuda.so.0 \\\n    ${container_image} \\\n    ldd /opt/local/FastEddy-model/SRC/FEMAIN/FastEddy\n\n\n\necho \"# --&gt; BEGIN execution\"; tstart=$(date +%s)\n\nmpiexec \\\n    --np ${nranks} --ppn ${nranks_per_node} --no-transfer \\\n    set_gpu_rank \\\n    singularity \\\n    --quiet \\\n    exec \\\n    --bind ${SCRATCH} \\\n    --bind ${WORK} \\\n    --pwd $(pwd) \\\n    --bind /run \\\n    --bind /opt/cray \\\n    --bind /usr/lib64:/host/lib64 \\\n    --env LD_LIBRARY_PATH=${CRAY_MPICH_DIR}/lib-abi-mpich:/opt/cray/pe/lib64:${LD_LIBRARY_PATH}:/host/lib64 \\\n    --env LD_PRELOAD=/opt/cray/pe/mpich/${CRAY_MPICH_VERSION}/gtl/lib/libmpi_gtl_cuda.so.0 \\\n    --env MPICH_GPU_SUPPORT_ENABLED=1 \\\n    --env MPICH_GPU_MANAGED_MEMORY_SUPPORT_ENABLED=1 \\\n    --env MPICH_SMP_SINGLE_COPY_MODE=NONE \\\n    ${container_image} \\\n    /opt/local/FastEddy-model/SRC/FEMAIN/FastEddy \\\n    ./Example02_CBL.in\n\necho \"# --&gt; END execution\"\necho $(($(date +%s)-${tstart})) \" elapsed seconds; $(date)\"\n</code></pre> <p>See here for a more complete discussion of the nuances of containerized applications on Derecho.</p>"},{"location":"storage-systems/","title":"Storage Resources, Data Transfer, and Data Access Resources","text":"<p>CISL provides several broad classes of storage resources and data access mechanisms. Follow the links below for additional information.</p>"},{"location":"storage-systems/#storage-resources","title":"Storage Resources","text":"<ul> <li>GLADE for typical POSIX file system use</li> <li>Quasar is a cold, tape-based archive for storing curated data collections that have an indefinite lifetime.</li> <li>Stratus is an object storage disk system for long-term data storage.</li> </ul>"},{"location":"storage-systems/#data-access-and-data-transfer","title":"Data Access and Data Transfer","text":"<ul> <li>Dedicated data-access for facilitating data transfers.</li> <li>Tools such as Globus, and <code>scp</code>, <code>sftp</code>, <code>bbcp</code> for performing large data set transfers.</li> </ul>"},{"location":"storage-systems/cmip-analysis-platform/","title":"CMIP Analysis Platform","text":"<p>The CMIP Analysis Platform gives researchers convenient access to climate data from the Coupled Model Inter-comparison Project (CMIP) on CISL\u2019s GLADE disk storage resource.</p> <p>Note</p> <p>The tables below show which data sets are already available on GLADE. They are updated each Friday.  These data are be located under <pre><code>/glade/collections/cmip/\n</code></pre></p> <p>If what you need is not already on GLADE, please review the additional documentation in the Data Services section of the CISL web site and submit a request.</p>"},{"location":"storage-systems/cmip-analysis-platform/#cmip6-data-sets-on-glade","title":"CMIP6 data sets on GLADE","text":""},{"location":"storage-systems/cmip-analysis-platform/#cmip5-data-sets-on-glade","title":"CMIP5 data sets on GLADE","text":""},{"location":"storage-systems/data-access-nodes/","title":"Using <code>data-access</code> nodes","text":"<p>The <code>data-access</code> nodes are provided to enable authorized users to perform a number of file-management operations. These include:</p> <ul> <li> <p>managing file and directory permissions for NCAR Campaign Storage data   holdings.</p> </li> <li> <p>copying data from the Research Data Archive.</p> </li> </ul> <p>The <code>data-access</code> nodes are not compute nodes and any tasks users run on them are run \"at risk.\" They also are not intended for use as long-term storage.</p> <p>Short, non-memory-intensive processes are allowed, including such tasks as text editing or running small scripts for transferring files. If any task consumes excessive resources (determined at the discretion of the CISL consultants or system administrators), a system administrator will kill the process or processes and you will be notified.</p> <p>No usage charges are assessed for reading data.</p> <p>If you do not have an allocation but need to use the nodes to get data from CISL storage resources, submit a Data Analysis Allocation request.</p>"},{"location":"storage-systems/data-access-nodes/#logging-in","title":"Logging in","text":"<p>To use these resources, log in to data-access.ucar.edu and authenticate as described in our Getting Started documentation.</p> <p>Use <code>ssh</code> as follows: <pre><code>ssh username@data-access.ucar.edu\n</code></pre></p>"},{"location":"storage-systems/data-access-nodes/#using-glade-scratch","title":"Using GLADE scratch","text":"<p>When you log in, you are in <code>/glade/u/home/username</code> by default. Use your larger <code>scratch</code> or <code>work</code> file spaces to hold data that you read out of Campaign Storage or other storage resources. You can also read files from other GLADE file spaces to which you have been granted read permissions.</p> <p>See GLADE for more information about file spaces.</p>"},{"location":"storage-systems/data-access-nodes/#transferring-files","title":"Transferring files","text":"<p>To transfer data between data-access.ucar.edu and another system, use any of these tools:</p> <ul> <li> <p>Globus \u2013 the endpoint for <code>data-access</code> projects is <code>NCAR GLADE</code></p> </li> <li> <p>SCP/SFTP and similar SSH-based tools</p> </li> </ul> <p>See those links for documentation.</p>"},{"location":"storage-systems/data-transfer/","title":"Data transfers and sharing","text":"<p>The best way to transfer files between systems or to share data with colleagues depends on the context. For example, you might want to move or copy data that you own from the GLADE system to a university storage resource or from GLADE to NCAR Campaign Storage. At another time, you might want to share data that you have on GLADE with someone who does not have access to NCAR systems.</p> <p>There are several ways of accomplishing these transfer and sharing tasks. Review the descriptions of each below and follow the links provided for details.</p>"},{"location":"storage-systems/data-transfer/#transferring-files-between-systems","title":"Transferring files between systems","text":""},{"location":"storage-systems/data-transfer/#globus","title":"Globus","text":"<p>CISL recommends using Globus to transfer files between systems \u2013 for example, between non-NCAR facilities and the resources that CISL manages. The Globus platform also enables users to transfer files between their GLADE file spaces and the NCAR Campaign Storage file system.</p> <p>Globus has both web and command line interfaces, and its Globus Connect feature enables users to move files to and from laptop or desktop computers and other systems. Globus has a typical transfer rate that ranges from 100 to 200 MBps. More information.</p>"},{"location":"storage-systems/data-transfer/#scp-and-sftp","title":"SCP and SFTP","text":"<p>The installed Secure Copy Protocol (SCP) and Secure FTP (SFTP) utilities are best suited to transferring small numbers of small files to or from a local computer and the resources that CISL manages. More information.</p> <p>These other tools also can be used to make secure transfers between your local computer and CISL systems:</p> <ul> <li> <p>PSCP / PSFTP \u2013 PuTTY utilities   including a Windows version</p> </li> <li> <p>WinSCP \u2013 SCP and SFTP transfers with a   graphical user interface for Windows users</p> </li> </ul>"},{"location":"storage-systems/data-transfer/#bbcp","title":"BBCP","text":"<p>BBCP is a multi-streaming utility for transferring large files. It splits files into multiple streams that are transferred simultaneously, so it is faster than the single-streaming SCP and SFTP utilities. More information.</p>"},{"location":"storage-systems/data-transfer/#sharing-data-with-colleagues","title":"Sharing data with colleagues","text":""},{"location":"storage-systems/data-transfer/#globus_1","title":"Globus","text":"<p>In addition to the features described above, Globus enables users to share files with colleagues and others who do not have NCAR user accounts. More information.</p>"},{"location":"storage-systems/data-transfer/#sharing-via-google-drive-and-other-services","title":"Sharing via Google Drive and other services","text":"<p>Individuals who have GLADE access can copy files from GLADE and then share them with others via Google Drive or services such as DropBox and Amazon Web Services. More information.</p>"},{"location":"storage-systems/data-transfer/scp-and-sftp/","title":"SSH-based command line tools for file transfer","text":""},{"location":"storage-systems/data-transfer/scp-and-sftp/#scp-and-sftp","title":"SCP and SFTP","text":"<p>Secure Copy Protocol (SCP) and Secure FTP (SFTP) are two utilities for transferring files between remote systems and the NCAR systems that CISL manages.</p> <p>They are best suited for transferring small numbers of small files (for example, fewer than 1,000 files totaling less than 200 MB). For larger-scale transfers, we recommend using Globus</p> <p>You can make SCP and SFTP transfers between the GLADE storage system and a remote machine if the remote machine accepts incoming SSH sessions. If it doesn't, the transfer will hang or you will receive a message such as \"connection refused,\" depending on the system's firewall settings.</p>"},{"location":"storage-systems/data-transfer/scp-and-sftp/#from-an-ncar-system","title":"From an NCAR system","text":"<p>To make SCP and SFTP transfers from your GLADE file space to a remote system, log in to the data access nodes at <code>data-access.ucar.edu</code> and execute the commands shown below.</p> <p>Use SCP if you need to transfer a single file or if you want to transfer multiple files with a single command by using a wildcard or recursive option.</p>"},{"location":"storage-systems/data-transfer/scp-and-sftp/#scp-transfer","title":"SCP transfer","text":"<p>To transfer multiple files with similar names or extensions, follow this example, in which <code>supersystem.univ.edu</code> is a fictitious remote system. <pre><code>scp /glade/u/home/pparker/mydata/*.dat\npparker@supersystem.univ.edu:/home/pparker\n</code></pre></p>"},{"location":"storage-systems/data-transfer/scp-and-sftp/#sftp-transfer","title":"SFTP transfer","text":"<p>If you need to transfer many files from multiple directories to a remote machine, doing so in an SFTP session is likely to be more efficient for you than SCP.</p> <p>Log in to <code>data-access.ucar.edu</code>, then start your transfer session with the <code>sftp</code> command followed by your login information for the remote system. <pre><code>sftp pparker@supersystem.univ.edu\n</code></pre></p> <p>You will be asked to authenticate at this point.</p> <p>Then, within the session, you can change between directories as needed and execute <code>put</code> commands to copy files to the remote machine. Use <code>lcd</code> to change local directories, and use <code>cd</code> to change directories on the remote system, as shown in this example. <pre><code>sftp&gt; lcd /glade/u/home/pparker/mydata\nsftp&gt; put filename1\nsftp&gt; lcd /glade/scratch/pparker\nsftp&gt; cd /home/mydata\nsftp&gt; put filename2\nsftp&gt; quit\n</code></pre></p> <p>You can also transfer files from batch jobs running on an NCAR machine.</p>"},{"location":"storage-systems/data-transfer/scp-and-sftp/#to-an-ncar-system","title":"To an NCAR system","text":"<p>To transfer files from a remote system to your GLADE file space, log in to the remote system and reverse the procedures shown above.</p> <p>For example: <pre><code>scp /remotedir/*.dat pparker@data-access.ucar.edu:/glade/u/home/pparker/mydata\n</code></pre></p> <p>You will be asked to authenticate for each individual SCP command that you execute to transfer files to the NCAR system.</p>"},{"location":"storage-systems/data-transfer/scp-and-sftp/#pscp-and-psftp","title":"PSCP and PSFTP","text":"<p>Note</p> <p>PSCP and PSFTP are PuTTY's implementation of SCP and SFTP, and may be useful for users on a Windows computer.</p> <p>PuTTY Secure Copy (PSCP) and PuTTY SFTP (PSFTP) enable you to transfer files to another system after opening a command window on a Windows computer. Both applications are available as free downloads.</p> <p>Usage is very similar to SCP and SFTP as described above.  Expand the example box below for a full description of PSCP and PSFTP.</p> Using PSCP and PSFTP <p>Go to the download site and find the latest release version of the <code>pscp.exe</code> and <code>psftp.exe</code> files.</p> <p>Click on each and save them to your hard drive\u2014for example, in your <code>C:\\Users\\username\\Downloads</code> folder or in <code>C:\\Program Files</code>.</p> <p>To run either program, first open a command window:</p> <ul> <li> <p>Enter <code>cmd.exe</code> in the search field of your Start menu.</p> </li> <li> <p>Press Enter.</p> </li> </ul> <p>Then follow the applicable instructions below.</p> <p>PSCP transfer</p> <p>To copy a file or files using PSCP, open a command window and change to the directory in which you saved <code>pscp.exe</code>. <pre><code>C:\\Users\\jbsmith&gt;cd C:\\Program Files\n</code></pre></p> <p>Then type <code>pscp</code>, followed by the path that identifies the files to copy and the target directory, as in this example. <pre><code>pscp C:\\Users\\jbsmith\\directory\\*.txt jbsmith@cheyenne.ucar.edu:/glade/u/home/username\n</code></pre></p> <p>Press Enter, then follow your usual authentication procedures to execute the transfer. <pre><code>Token_Response:\nfile1.txt               | 0 kB |   0.5 kB/s | ETA: 00:00:00 | 100%\nfile1.txt               | 0 kB |   0.5 kB/s | ETA: 00:00:00 | 100%\nfile1.txt               | 0 kB |   0.5 kB/s | ETA: 00:00:00 | 100%\nC:\\Users\\jbsmith\\Downloads&gt;\n</code></pre></p> <p>When the transfer is complete, type exit, then press Enter to close the command window.</p> <p>PSFTP transfer</p> <p>Open your command window, then change to the directory in which you saved <code>psftp.exe</code>. <pre><code>C:\\Users\\jbsmith&gt;cd C:\\Program Files\n</code></pre></p> <p>To start a session, type <code>psftp</code> followed by your login for the target computer. <pre><code>psftp jbsmith@cheyenne.ucar.edu\n</code></pre></p> <p>Press Enter, then follow your usual authentication procedures to log in to the remote machine. <pre><code>Token_Response: Remote working directory is /glade/u/home/jbsmith psftp&gt;\n</code></pre></p> <p>Within the session that you just started, you can copy a file or files from your computer to the remote system by changing between directories as needed and executing multiple <code>put</code> commands.</p> <p>Use <code>lcd</code> to change local directories, and <code>cd</code> to change directories on the remote system, as in this example: <pre><code>psftp&gt; lcd ..\\documents\npsftp&gt; lcd documents\nNew local directory is C:\\Users\\jbsmith\\documents\npsftp&gt; put file1.txt\nlocal:file1.txt =&gt; remote:/glade/u/home/jbsmith/file1.txt\npsftp&gt; cd /glade/scratch/jbsmith\nRemote directory is now /glade/scratch/jbsmith\npsftp&gt; mput file*.txt\nlocal:file1.txt =&gt; remote:/glade/scratch/jbsmith/file1.txt\nlocal:file2.txt =&gt; remote:/glade/scratch/jbsmith/file2.txt\nlocal:file3.txt =&gt; remote:/glade/scratch/jbsmith/file3.txt\npsftp&gt;\n</code></pre></p> <p>To end the psftp session, type exit, then press Enter.</p> <p>To close the command window, type exit again, then press Enter.</p> <p>To copy multiple files, you can use a wildcard and an <code>mput</code> or <code>mget</code> command rather than <code>put</code> or <code>get</code>.</p>"},{"location":"storage-systems/data-transfer/scp-and-sftp/#winscp","title":"WinSCP","text":"<p>For Windows users, WinSCP offers a choice of GUI interfaces for managing files. It is easy to download and install from winscp.net.</p> <p>Starting the application will bring you to a login screen like the one shown here. (Highlights added.)</p> <p></p> <p>To start a session, input the host name and your username for that system.</p> <p>Leave the password field blank, and click Login.</p> <p>The first time you log in to a system, you may get a dialog box like this:</p> <p></p> <p>Click Yes to continue.</p> <p>Next, you will be asked for your Token_Response. Follow your regular authentication procedures.</p> <p></p> <p>If you\u2019re using the \u201cCommander\u201d interface (shown below), WinSCP will display the contents of your local system on the left side of your screen and the contents of your remote system home directory on the right. You can manage files using typical Windows commands and tools.</p> <p></p> <p>The alternative \u201cExplorer\u201d interface displays only the remote folder, and you can transfer files by dragging and dropping from Windows File Explorer. On the WinSCP menu, go to Options/Preferences/Environment/Interface to use it.</p>"},{"location":"storage-systems/data-transfer/scp-and-sftp/#bbcp","title":"BBCP","text":"<p>The BBCP utility for transferring large files is an alternative for users who are unable to use Globus to transfer data. BBCP splits the files into multiple streams that are transferred simultaneously, so it is faster than the single-streaming SCP and SFTP utilities.</p> <p>To make transfers with BBCP, it must be installed on all the systems where you want to use it. It is already installed on the NCAR systems that CISL manages, including the data-access nodes.</p>"},{"location":"storage-systems/data-transfer/scp-and-sftp/#transfer-examples","title":"Transfer examples","text":"<p>To transfer a file from GLADE to a remote system that uses <code>bbcp</code>, log in to <code>data-access.ucar.edu</code> and follow this example. Replace \"target\" with the intended pathname of the file you are transferring. <pre><code>bbcp -w 4m -s 16 filename username@supersystem.univ.edu:target\n</code></pre></p> <p>To transfer a file from a remote system to GLADE, log in to the remote system and follow this example. Replace \"target\" with the intended pathname of the file you are transferring \u2013 for example,<code>/glade/u/home/\\$USER/filename</code>. <pre><code>bbcp -w 4m -s 16 -V -D filename username@data-access.ucar.edu:target\n</code></pre></p>"},{"location":"storage-systems/data-transfer/scp-and-sftp/#detailed-documentation","title":"Detailed documentation","text":"<p>For complete details, see the official BBCP man page.</p>"},{"location":"storage-systems/data-transfer/globus/","title":"Globus file transfers","text":"<p> Updated 4/25/2023:</p> <p>CISL has enabled the *Globus for Google Drive connector service to facilitate file transfers to NCAR's Google Drive by NCAR and UCAR staff. University users and others who are interested in using the connector service are advised to consult with their own institutional IT experts.</p> <p>Globus is the most efficient way to transfer files \u2013 large files, in particular \u2013 between NCAR file systems such as Campaign Storage, GLADE, and non-NCAR resources and storage systems. NCAR and UCAR researchers can also use the Globus for Google Drive connector service to transfer files to the NCAR Google Drive collection on Globus for sharing and storage purposes as described below.</p> <p>Globus has both a web interface and a command line interface (CLI), which are described below. To use either interface, the first step is to create a free personal account and log in using a Globus ID or a Google account. (UCAR and NCAR staff: Do not use the NCAR RDA organizational login.)</p> <p></p> <p>Several mapped collections, which provide access to different file system locations on a public endpoint, are set up on the Globus system for transferring files to and from NCAR storage systems. These include:</p> <ul> <li> <p>NCAR GLADE</p> </li> <li> <p>NCAR Campaign Storage</p> </li> <li> <p>NCAR Data Sharing Service</p> </li> </ul> <p>Users can also create their own guest collections as described here in order to facilitate data sharing with colleagues and to accommodate unattended workflows.</p> <p>Globus also offers a feature called Globus Connect Personal for moving files to and from a laptop or desktop computer and other endpoints.</p>"},{"location":"storage-systems/data-transfer/globus/#cautions","title":"Cautions","text":"<p>Globus Usage Cautions</p> <ul> <li>The Globus interface for transferring data does not handle symbolic   links and does not create symbolic links on a destination endpoint.</li> <li>It is possible to corrupt data when performing a transfer if you   accidentally specify the same files as both source and destination. To   avoid inadvertently deleting data when using the web interface,   activate the <code>sync</code> option in the Transfer &amp; Sync Options menu.   Sync can be set to allow a file transfer only if the file does not   exist on the destination, if there\u2019s a difference in checksum or file   size, or if the source copy is newer than the destination copy.</li> <li>Transferred files assume the user's <code>umask</code> permissions on the   destination system regardless of permissions on the source system.</li> </ul>"},{"location":"storage-systems/data-transfer/globus/#transferring-files-with-the-web-interface","title":"Transferring files with the web interface","text":"<p>When transferring files between systems, keep in mind that your username might not be the same on each system.</p> <p>Follow these steps to transfer files. See the image below for reference.</p> <ol> <li> <p>Go to the main Globus page (globus.org)     and log in using your personal Globus ID.</p> </li> <li> <p>Go to File Manager.</p> </li> <li> <p>Use the Panels button to display two endpoint panels side by     side.</p> </li> <li> <p>Enter the name of your source endpoint in the Collection field     on one panel.</p> </li> <li> <p>Specify the path where your source files are located.</p> </li> <li> <p>Enter your username     and authenticate as     you do when logging in to NCAR systems. You will not need to     authenticate to access the collection for the next 30 days.</p> </li> <li> <p>Identify your target endpoint in the other panel.</p> </li> <li> <p>Specify a destination path.</p> </li> <li> <p>Select the files you want to copy.</p> </li> <li> <p>Click the Start button to initiate the transfer.</p> </li> </ol> <p>You can check the status of your transfers any time through the web interface and will be notified when they are complete.</p> <p></p>"},{"location":"storage-systems/data-transfer/globus/#using-mapped-collections-for-cli-transfers","title":"Using mapped collections for CLI transfers","text":"<p>The Globus CLI application, an installable Python package, can be used to make both manual and unattended file transfers.</p> <p>The application is:</p> <ul> <li> <p>installed on the NCAR/CISL data-access nodes.</p> </li> <li> <p>available within the NCAR Python Library conda environment.</p> </li> <li> <p>can be added to a personal conda environment using conda install   globus-cli.</p> </li> </ul> <p>To begin, log in as shown in this example for using the data-access nodes. (If your UCAR username and your username on your local computer are different, follow the alternative example.)</p> <pre><code>ssh data-access.ucar.edu\n# (alternative: ssh username@data-access.ucar.edu)\n</code></pre> <p>Run globus login and follow the on-screen instructions.</p> <pre><code>globus login\n</code></pre> <p>Output example (if you are not already logged in): <pre><code>Please authenticate with Globus here:\n\n------------------------------------\n\nURL to copy and paste into your browser.\n\n------------------------------------\n\nEnter the resulting Authorization Code here:\n</code></pre></p> <p>Copy the lengthy URL and paste it into your browser. It will ask you to choose an identity \u2013 as above, use either a Globus ID or Google account \u2013 and then it will take you to a consent form that looks like this:</p> <p></p> <p>Click Allow to give the CLI app the necessary access and you will receive an authorization code.</p> <p>After entering the code at the terminal prompt, you will be logged in to the Globus CLI and in your <code>/glade/u/home</code> directory. You can use this authentication credential for 30 days before you will need to reactivate a mapped collection. Workflows that need longer collection access \u2013 to facilitate unattended file transfers, for example \u2013 can instead use guest collections.</p> <p>Using the CLI, it is possible to query your current mapped collection activation and also force Globus to reactivate it (thus extending your activation lifetime): <pre><code>gc_glade=$(globus endpoint search \"ncar@globusid.org NCAR GLADE\" --jq \"DATA[0].id\" --format UNIX)\n\nglobus endpoint activate $gc_glade\nEndpoint is already activated. Activation expires at 2022-11-22 20:47:46+00:00\n\nglobus endpoint activate --force $gc_glade\nAutoactivation succeeded with message: Endpoint activated successfully using Globus Online credentials.\n</code></pre></p> <p>Your default shell on the data-access nodes is tcsh. To change your current shell, just enter bash or another preferred shell.</p>"},{"location":"storage-systems/data-transfer/globus/#executing-cli-transfers","title":"Executing CLI transfers","text":"<p>For details regarding how to make batch transfers and single-item transfers, manage endpoints, and more, see these resources:</p> <ul> <li> <p>CLI Examples</p> </li> <li> <p>CLI QuickStart Guide</p> </li> <li> <p>Globus CLI Reference</p> </li> </ul>"},{"location":"storage-systems/data-transfer/globus/#globus-for-google-drive","title":"Globus for Google Drive","text":"<p>NCAR and UCAR researchers can use the Globus for Google Drive connector service to transfer files to the NCAR Google Drive collection on Globus for sharing and storage purposes by following the instructions below.</p> <p>The Globus Google Drive connector is not designed to transfer Google apps products \u2013 such as Google Docs, Sheets, or Slides \u2013 between Google accounts or beyond Google Drive. While such files might appear to be visible in your Google Drive as .gsheet or .gdoc files, those are just pointers and downloading them will not download the data stored in the files.</p> <p>It also is not intended for backing up your scratch space or other GLADE files. It is intended for sharing smaller files such as plots. If you need to transfer many small files, compress or archive them into a smaller number of files.</p> <p>To make a transfer, follow these steps after logging in to your personal Globus account:</p> <ol> <li> <p>Go to File Manager and search for the collection named NCAR     Google Drive.</p> </li> <li> <p>Select the drive and authenticate as required. You will be directed     to the NCAR/UCAR authentication page to give Globus access to the     Google Drive by using Duo and your CIT credentials. (You are only     required to do this once.)</p> </li> <li> <p>Select your target collection (endpoint) in the other File Manager     panel.</p> </li> <li> <p>Execute the transfer.</p> </li> </ol> <p>Your Google Drive endpoint will originate in your /My Drive/ path by default. To access other Google Drive folders such as Shared with me or Starred, simply navigate up one level.</p> <p>Warning</p> <p>Avoid giving the same name to multiple files when using Globus integration. While Google Drive can support multiple files with the same name, they cannot be mapped into a POSIX file system.</p>"},{"location":"storage-systems/data-transfer/globus/#storage-limits-and-limitations","title":"Storage limits and limitations","text":"<p>Keep these storage and file size limitations in mind when using Globus to transfer data to the NCAR Google Drive: - Google imposes a maximum file size limit of 5 TB. - The daily upload limit is 750 GB per user. However, Google allows uploading of a single file larger than 750 GB. - Each user is limited to having a maximum of 5 million files and   folders within their Google Drive, which includes all Google products   such as Google Docs.</p>"},{"location":"storage-systems/data-transfer/globus/#example-use-case","title":"Example use case","text":"<p>The Globus Google Drive connector can streamline data sharing and collaboration with team members who don't have access to Globus.</p> <p>For example, a meteorologist who performs daily operational simulations on an NCAR system as part of a multi-organizational field campaign and generates plots for flight planning can share them with the operational team and collaborators via a shared drive.</p> <p>Instead of manually transferring the plots to a personal device and then uploading them to the shared drive, the meteorologist can leverage the connector to automate the workflow and move the plots directly from GLADE to Google Drive.</p>"},{"location":"storage-systems/data-transfer/globus/#globus-connect-personal","title":"Globus Connect Personal","text":"<p>To set up your laptop or desktop computer to use Globus Connect Personal:</p> <ul> <li> <p>Go to Globus Connect   Personal and follow   the instructions to download and install it on your local system.</p> </li> <li> <p>Add your local system as an endpoint by following the instructions on   the Globus Connect website.</p> </li> <li> <p>Start Globus Connect, and then sign in   to globus.org.</p> </li> </ul> <p>Your local system should now appear as an endpoint that can be used for transferring files.</p>"},{"location":"storage-systems/data-transfer/globus/#using-globus-to-sharing-data-and-making-unattended-transfers","title":"Using Globus to sharing data and making unattended transfers","text":"<p>Some users need unauthenticated access of data from NCAR storage systems to share that data or to accommodate their workflows.  Guest collections enable unauthenticated data access by pointing to a specified subset of data stored in a mapped collection like NCAR GLADE. They can be created by anyone with authenticated access to NCAR storage systems.</p> <p>Usage of Globus Guest Collection for data sharing is described here.</p>"},{"location":"storage-systems/data-transfer/globus/Sharing%2Bdata%2Band%2Bmaking%2Bunattended%2Btransfers/","title":"Sharing data and making unattended transfers with Globus","text":"<p>Some users need unauthenticated access of data from NCAR storage systems to share that data or to accommodate their workflows. Common use cases include:</p> <ul> <li>Easily share specified subsets of their data with external colleagues.</li> <li>Access their own data without authentication in order to make   unattended transfers.</li> </ul> <p>Guest collections enable unauthenticated data access by pointing to a specified subset of data stored in a mapped collection like NCAR GLADE. They can be created by anyone with authenticated access to NCAR storage systems.</p> <p>Once a guest collection is created, it can be used in place of a mapped collection as a transfer endpoint in the web interface, Globus CLI, or Globus Python API. Users can also bypass the Globus transfer interface entirely by creating a URL to an individual file contained within a guest collection. The URL will give specified individuals download access to the data from a browser or terminal utility like <code>wget</code> or <code>curl</code>.</p> <p>New Collections must be created through Globus web interface</p> <p>While the Globus CLI supports the use of guest collections as transfer endpoints (see this note for more on endpoint vs collection nomenclature), it does not provide commands to create new guest collections or file URLs. Those actions must be done in the web interface.</p> <p>Do not allow file deletion!</p> <p>Be sure to review and follow the recommendations below to avoid inadvertently sharing your data with unknown users or allowing unauthorized deletions of your data. Guest collections make data sharing easy, but they also bypass the traditional data safeguards (two-factor authentication) that users and administrators rely on to protect data.</p>"},{"location":"storage-systems/data-transfer/globus/Sharing%2Bdata%2Band%2Bmaking%2Bunattended%2Btransfers/#creating-a-guest-collection","title":"Creating a guest collection","text":"<p>To create a guest collection using the Globus web interface, log in and navigate to a mapped collections like NCAR GLADE. Then:</p> <ol> <li> <p>Use the File Manager to navigate to the directory you want to     designate as your guest collection.</p> </li> <li> <p>Select the directory and click the Share button in the toolbar.</p> </li> <li> <p>From the next screen, click Add a Guest Collection. This option     will take you to the following page (sometimes after a one-time     consent page), which allows you to name and customize your     collection's metadata.</p> </li> </ol> <p></p> <p>Once your collection is created, it will be assigned an ID you can use for both web and CLI transfers. You can modify collection properties by selecting Collections on the left toolbar, then following these steps:</p> <ol> <li> <p>Select Administered by you to see your collections, including     your newly created guest collection.</p> </li> <li> <p>Select your guest collection, then the Permissions tab.</p> </li> <li> <p>Finally, click Add Permissions - Share With. At this point, you     can add read permissions for other users, groups, all Globus users,     or any individual to access data contained within your collection.</p> </li> </ol> <p>You can also now add read-write permission for yourself to enable unattended workflows via this guest collection (using the Globus CLI or Python API).</p>"},{"location":"storage-systems/data-transfer/globus/Sharing%2Bdata%2Band%2Bmaking%2Bunattended%2Btransfers/#providing-url-access-to-files","title":"Providing URL access to files","text":"<p>Globus allows you to create a sharable URL (web link) to any file in your guest collection or a mapped collection. This means that you can share files from supported file systems without the other user needing to interact at all with Globus itself. The individual can simply use a browser and your URL to download the files.</p> <p>The web links will be subject to the permissions model of the collection being used. If you create a link using a mapped collection like NCAR GLADE, the recipients will need to be able to authenticate to the collection as well. If you obtain a link using a guest collection that permits the recipient read-access, they will be able to open the link regardless of their ability to access the rest of the original mapped collection.</p> <p>To create a URL:</p> <ol> <li> <p>Navigate to the desired file in the web interface's File Manager.</p> </li> <li> <p>Click on the file, then select Get Link from the toolbar.</p> </li> </ol> <p></p>"},{"location":"storage-systems/data-transfer/globus/Sharing%2Bdata%2Band%2Bmaking%2Bunattended%2Btransfers/#recommendations-for-creating-guest-collections","title":"Recommendations for creating guest collections","text":"<p>The features described above make data sharing easy, but they also bypass the traditional data safeguards (two-factor authentication) that users and administrators rely on to protect data. Using guest collections, it is quite possible to grant access to data unintentionally or even allow unknown users to delete your data. To avoid these situations, consider the following recommendations:</p> <ol> <li> <p>Properly scope your collections. It may be tempting to set a     collection to a top-level directory (e.g., <code>/glade/derecho/scratch/${USER}</code>),     but this will grant access to all of your data on scratch.     Instead, create collections with a narrow scope and move or copy     files around to selectively grant access to data.</p> </li> <li> <p>Avoid write permissions. Generally speaking, Globus is most     useful as a tool to access/read data. Write permissions within     Globus are only useful for data collection curation, so avoid     granting it on your data. It may be useful to grant write     permissions on a directory to allow collaborators to \"push\" data to     you, but remember that the space will still be subject to storage     quotas and your collaborators will not have visibility into those     limits!</p> </li> </ol> <p>It is also wise to avoid changes to guest collection metadata \u2013 especially the collection name \u2013 once it has been shared with others. The text name is useful in both the web and command-line interfaces for collection discoverability, so changing it and other metadata may break workflows or result in a loss of reproducibility (e.g., when providing data for publication requirements).</p>"},{"location":"storage-systems/data-transfer/globus/Sharing%2Bdata%2Band%2Bmaking%2Bunattended%2Btransfers/#how-to-use-your-guest-collections","title":"How to use your guest collections","text":"<p>Users of your guest collections will access data in one of the following two ways, depending on how you have configured your collections.</p>"},{"location":"storage-systems/data-transfer/globus/Sharing%2Bdata%2Band%2Bmaking%2Bunattended%2Btransfers/#accessible-to-globus-users-or-groups","title":"Accessible to Globus users or groups","text":"<p>If you have set up your collection to provide access to individual users or groups, or all Globus users, they will be able to find and open your collection using the Globus website, CLI, or Python API.</p> <p>If they do not already have a Globus account, they will need to create one to log into the Globus service itself. NCAR/UCAR staff should use their Google login credentials. Universities may have their own guidance about which type of account to use.</p> <p>Once a user is logged in, no further authentication will be required to see data that you have made accessible in your collection. Alternatively, you can create web URLs to specific files that will allow permitted users to access the files either in the Globus web interface or via direct download, depending on how you have configured the URL.</p>"},{"location":"storage-systems/data-transfer/globus/Sharing%2Bdata%2Band%2Bmaking%2Bunattended%2Btransfers/#accessible-to-anyone","title":"Accessible to anyone","text":"<p>If you have configured your collection to be accessible to public (anonymous) users, you can create direct-download URLs as described above, but those users will not need to have a Globus account to open them.</p> <p>This method is the easiest for end users but has some limitations. For example, instead of having a file browser view with metadata, users will simply download the file as if it were hosted on a web server.</p>"},{"location":"storage-systems/glade/","title":"GLADE file spaces","text":""},{"location":"storage-systems/glade/#overview","title":"Overview","text":"<p>The Globally Accessible Data Environment \u2013 a centralized file service known as <code>GLADE</code> \u2013 uses high-performance Spectrum Scale and Lustre shared file system technology to give users a common view of their data across the HPC, analysis, and visualization resources that CISL manages.</p> File space <p>User quota</p> <p>(Size/Count)</p> Backup Purge           policy Descriptions/notices Home:         /glade/u/home/&lt;username&gt; 50 GB Yes Not purged User home directory         Access: POSIX Scratch: (Cheyenne)         /glade/scratch/&lt;username&gt; 10\u00a0TB No 120 days Temporary computational space         Access: POSIX Scratch: (Derecho) /glade/derecho/scratch/&lt;username&gt; 30TB / 10M No 180 days Derecho's scratch file system also includes a limit on a users'         total number of files\u00a0 Work:         /glade/work/&lt;username&gt; 2 TB No Not purged User work space         Access: POSIX Project:         /glade/p/entity/project_code N/A No Not purged Deprecated; will be migrated to Campaign Storage in           2023 Collections:         /glade/collections N/A No Not purged Curated collections (CMIP, RDA, others)         Access: POSIX Campaign Storage:         /glade/campaign N/A No Not purged Project space allocations (via allocation request) GLADE system status report <p>CISL backs up the GLADE home file space several times a week and also creates snapshots to enable users to recover deleted files quickly and easily. Data can remain in each of these spaces in accordance with the policies detailed below. The policies are subject to change; any changes necessary will be announced in advance.</p> <p>CISL does not provide backups of other spaces. You are responsible for the safe storage of any data that must be preserved.</p> <p>Best practice: Check your space usage regularly with <code>gladequota</code></p> <p>Check your space usage regularly with <code>gladequota</code> as described below, and remove data that you no longer need.</p> <p>You can conserve GLADE space by storing large files, such as tar files, rather than numerous small, individual files. This is because the system allocates a minimum amount of space for each file (currently configured to 16 KB), no matter how small the file is.  Thus 16KB is the smallest amount of space the system can allocate to a file, including empty files, directories and symlinks.</p>"},{"location":"storage-systems/glade/#status","title":"Status","text":""},{"location":"storage-systems/glade/#home-space","title":"Home space","text":"<p>Each user has a <code>/glade/u/home/&lt;username&gt;</code> space with a quota of 50 GB* for managing scripts, source code, and small data sets. It is backed up weekly, with backups retained for several months. CISL also creates snapshots of the space to enable users to recover deleted files quickly and easily.</p>"},{"location":"storage-systems/glade/#backup-policy","title":"Backup policy","text":"<ul> <li> <p>Your <code>/glade/u/home</code> directory is backed up several times a week while   your account is open. Each backup is kept for several weeks. The   frequency and scheduling of backups and the length of time they are   kept may change with no prior notice. If you are unable to find files   that you would like to restore in the snapshots of your home   directory, contact the NCAR Research Computing help   desk to request restoration of the files if   they are in a backup.</p> </li> <li> <p>Core dump files are not backed up. Core dump file names typically   follow this format: <code>core.xxxxx</code> (where the extension can include   from one to five digits).</p> </li> <li> <p>CISL does not purge or scrub your home directory, and deletes files   only as stated in the following data retention policy.</p> </li> </ul>"},{"location":"storage-systems/glade/#data-retention-policy","title":"Data retention policy","text":"<ul> <li> <p>When your account is closed, files will remain in your home directory   for 30 days, after which CISL backs up the final contents and removes   them from the file system. This backup is retained for six months   after account termination. However, note that your project and group   memberships are also terminated as described below, which can limit   your ability to access files based on shared group permissions.</p> </li> <li> <p>If one or more of your project allocations expires but your account is   not closed, files are retained in your home directory.</p> </li> <li> <p>Core dump files are not archived.</p> </li> </ul>"},{"location":"storage-systems/glade/#work-space","title":"Work space","text":"<p>Your <code>/glade/work/&lt;username&gt;</code> space is best suited for actively working with data sets over time periods greater than what is permitted in the scratch space.</p> <p>The default quota for these spaces is 1 TB.</p>"},{"location":"storage-systems/glade/#purge-policy","title":"Purge policy","text":"<ul> <li>This space is not purged or scrubbed. CISL deletes files only as   stated in the following data retention policy.</li> </ul>"},{"location":"storage-systems/glade/#data-retention-policy_1","title":"Data retention policy","text":"<ul> <li> <p>When your user account is closed, files are retained for 30 days   before being deleted.</p> </li> <li> <p>Files are not recoverable from backups, as there are none.</p> </li> <li> <p>If one or more of your project allocations expires but your account is   not closed, your work directory files are retained.</p> </li> </ul>"},{"location":"storage-systems/glade/#scratch-file-space","title":"Scratch file space","text":"<p>Each user has a <code>/glade/derecho/scratch/&lt;username&gt;</code> space by default, with an individual quota of 30 TB. The scratch file space is intended to support output from large-scale capability runs as well as computing and analysis workflows across CISL-managed resources. It is a temporary space for data to be analyzed and removed within a short amount of time.</p> <p>If you will need to occupy more than your quota of scratch space at some point, contact the NCAR Research Computing help desk to request a temporary increase. Include a paragraph justifying your need for additional space when making your request.</p>"},{"location":"storage-systems/glade/#purge-policy_1","title":"Purge policy","text":"<p>Individual files are removed from the scratch space automatically if they have not been accessed (for example: modified, read, or copied) in more than 120 days. A file's access time (<code>atime</code>) is updated at most once per day for purposes of I/O efficiency. To check a file's <code>atime</code>, run <code>ls -ul filename</code>.</p> <p>Users may not run <code>touch</code> commands or similar commands for the purpose of altering their files' timestamps to circumvent this purge policy. CISL staff will reduce the scratch quotas of users who violate this policy; running jobs may be killed as a result.</p> <p>In addition:</p> <ul> <li> <p>CISL routinely monitors scratch space usage to ensure that it remains   below the 90% mark and to determine if a reduction in the retention   period is necessary.</p> </li> <li> <p>We will announce in advance any changes to the retention period.</p> </li> </ul> <p>Best practice: To help us avoid the need to shorten the retention period, please use this space conscientiously.</p> <p>Delete files that you no longer need as soon as you're done with them rather than leave large amounts of data sitting untouched for the full 120 days. If you need to retain data on disk for more than 120 days, consider using your <code>/glade/work</code> space or Campaign Storage space.</p>"},{"location":"storage-systems/glade/#data-retention-policy_2","title":"Data retention policy","text":"<ul> <li> <p>When your account is closed, files are purged automatically as they   become 120 days old.</p> </li> <li> <p>If one or more of your project allocations expires but your account is   not closed, your scratch files are removed as stated in the purge   policy.</p> </li> <li> <p>Files are not recoverable from backups, as there are none.</p> </li> </ul>"},{"location":"storage-systems/glade/#campaign-storage-project-space","title":"Campaign Storage / project space","text":"<p>Dedicated project spaces on the <code>/glade/campaign</code> file system are available through our allocations process to support longer-term disk needs that are not easily accommodated by the scratch or work spaces. Allocations for project spaces are made to collaborative groups of users through the University/CHAP or NCAR allocations processes. The allocations are based on project needs and resource availability. Requests are reviewed according to the various allocation schedules.</p> <p>If you have a user account and project space but lack the directory permissions you need for that space, contact the NCAR Research Computing help desk to request changes. Identify the directories and the permissions you are requesting.</p>"},{"location":"storage-systems/glade/#access-reports","title":"Access reports","text":"<p>CISL generates weekly usage reports throughout /glade to help users manage their data. The reports provide a summary of when files were last accessed, how much space is used, and details for the top 25 users. The files are named <code>access_report.txt</code> and can be found at the top-level of shared file spaces, for example:</p> <ul> <li> <p><code>/glade/campaign/group_name/</code> (or similar, depending on the project)</p> </li> <li> <p><code>/glade/p/lab_name/group_name/</code> (or similar, depending on the project)</p> </li> <li> <p><code>/glade/p/univ/project_code/</code></p> </li> <li> <p><code>/glade/p/uwyo/project_code/</code></p> </li> </ul>"},{"location":"storage-systems/glade/#data-retention-policy_3","title":"Data retention policy","text":"<ul> <li> <p>Users' files are not deleted from project space after their accounts   become inactive.</p> </li> <li> <p>Files are not recoverable from backups, as there are none.</p> </li> <li> <p>CISL reserves the right to reclaim space from expired projects.</p> </li> </ul> <p>As disk space is a limited resource shared by the entire community, permanent storage of infrequently accessed data should be avoided. These spaces should be used as efficiently as possible so that other projects can take advantage of the resource. The Quasar tape storage system is more appropriate for long-term storage of infrequently accessed data.</p>"},{"location":"storage-systems/glade/#checking-space-usage","title":"Checking space usage","text":""},{"location":"storage-systems/glade/#knowing-your-quotas-and-usage-is-important","title":"Knowing your quotas and usage is important","text":"<p>All files that you own in your home, work, or scratch spaces are counted against your GLADE quota, regardless of the directory in which they are stored. If you write files to another user's home or scratch space, for example, they still count against your own individual user quota for that space.</p> <p>If you reach your disk quotas for the GLADE file spaces (see <code>gladequota</code> below), you may encounter problems until you remove files to make more space available. For example, you may not be able to log in, the system may appear hung, you may not be able to access some of your directories or files, your batch jobs may fail, and commands may not work as expected.</p> <p>If you cannot log in or execute commands, contact the NCAR Research Computing help desk. You can check your space usage as shown below.</p>"},{"location":"storage-systems/glade/#gladequota-command","title":"<code>gladequota</code> command","text":"<p>This command will generate a report showing your quota and usage information: <pre><code>gladequota\n\nCurrent GLADE space usage: csgteam\n\n  Space                                       Used        Quota      % Full    # Files\n----------------------------------------- ------------ ------------ --------- -----------\n/glade/derecho/scratch/csgteam              111.03 TiB   400.00 TiB   27.76 %    10226778\n/glade/cheyenne/scratch/csgteam             152.70 GiB   100.00 TiB    0.15 %      439076\n/glade/work/csgteam                           1.40 TiB     4.00 TiB   35.02 %     9153458\n/glade/u/home/csgteam                        68.27 GiB   100.00 GiB   68.27 %      237251\n----------------------------------------- ------------ ------------ --------- -----------\n/glade/collections/cmip                       1.26 PiB     2.93 PiB   42.95 %     3326866\n/glade/p/cisl/CSG                             1.69 TiB     5.00 TiB   33.79 %     8168998\n/glade/u/apps                                 2.02 TiB    10.00 TiB   20.19 %    20038473\n----------------------------------------- ------------ ------------ --------- -----------\nCampaign: csgteam (user total)                1.41 TiB          n/a       n/a     9891104\n/glade/campaign/cisl/csg                      7.94 TiB    23.00 TiB   34.54 %        2859\n/glade/campaign/cisl/sssg0001               217.15 TiB   250.00 TiB   86.86 %    50606617\n</code></pre></p> <p>Note</p> <p>Output from the <code>gladequota</code> command will show the home space quota as 100 GB instead of 50 GB. This is because the system stores dual copies of users' data for increased data integrity and safety. In some circumstances, queries of storage utilization from <code>du</code> and <code>ls</code> will also report a duplicated data footprint in your home directory for the same reason.</p>"},{"location":"storage-systems/glade/#general-data-retention-policies-and-procedures","title":"General data retention policies and procedures","text":""},{"location":"storage-systems/glade/#project-data","title":"Project data","text":"<p>When a sponsored project approaches expiry, there are several steps in the process that affect the accessibility of associated data:</p> <ul> <li> <p>30 days before expiration, the project PIs will receive an email   reminding them of the pending expiration. The project team should   assess remaining files and disposition appropriately in preparation   for group deactivation.</p> </li> <li> <p>90 days after project expiration, the UNIX group associated with the   project is removed. At this point users with accounts remaining on the   system will likely no longer have access permissions to the projects'   data, as the primary group no longer exists. It is   therefore imperative that any remaining project data be relocated   and ownership permissions assessed prior to this group deactivation.</p> </li> <li> <p>Finally, files are removed as scheduled on the timeline described   above for the relevant file system.</p> </li> </ul>"},{"location":"storage-systems/glade/#restoring-access-to-project-data","title":"Restoring access to project data","text":"<p>CISL has limited ability to modify access to project data after the 90-day post-expiry window. Such modifications require the approval of the original project owner. CISL has no ability to restore data after the purge or removal policies stated above have taken effect.</p>"},{"location":"storage-systems/glade/#user-accounts","title":"User accounts","text":"<p>User accounts are deactivated when they are no longer associated with an active project. When a user account is deactivated, several steps in the process affect the accessibility of the users' data:</p> <ul> <li> <p>30 days after a user account is deactivated, a final home directory   backup is performed and the home directory is removed.</p> </li> <li> <p>The user\u2019s work directory is removed. No backup is performed.</p> </li> <li> <p>Finally, additional scratch files are removed as scheduled on the   timeline described above for the relevant file system.</p> </li> </ul>"},{"location":"storage-systems/glade/#restoring-access-to-collaborators-data","title":"Restoring access to collaborators' data","text":"<p>A typical request for data access comes not from the departing user, but from remaining collaborators. Colleagues occasionally request access to a departed users' files, sometimes many months after the account is terminated, often when they realize the original owner set permissions that limit their access.</p> <p>While CISL has a limited ability to help in these situations, there are also legal limits to what we can do. For example, CISL cannot share files beyond the clear intent of the original owner as inferred from the UNIX file permissions. If a collaborator would like access to a file that was previously group- or world-readable, we may be able to help. If the original file was restricted to user-only read, however, we cannot override those intentions. The only exceptions to this policy are in compliance with broader UCAR IT records or investigation policies as described in UCAR's 1-7 Information Security Policy.</p>"},{"location":"storage-systems/glade/campaign/","title":"Campaign Storage file system","text":"<p>NCAR Campaign Storage is a resource for medium-term storage of project data, typically for three to five years, by NCAR labs and universities that have project allocations.</p> <p>Campaign Storage is accessible a number of ways that are described below:</p> <ul> <li> <p>through the Globus web and command-line interfaces</p> </li> <li> <p>from the data-access nodes, for Globus transfers and managing data   holdings</p> </li> <li> <p>from the Derecho and Casper clusters to facilitate data analysis and visualization   workflows</p> </li> </ul> <p>Files stored on this system are not backed up. Users are responsible for replicating any data they feel should be stored at an additional location as backup.</p> <p>Files that are deleted or overwritten cannot be recovered.</p>"},{"location":"storage-systems/glade/campaign/#globus-transfers","title":"Globus transfers","text":"<p>The Globus mapped collection established for the file system is NCAR Campaign Storage. How to make transfers to and from that collection is documented here: Globus file transfers.</p> <p>How to make transfers using the command line interface also is covered in detail in this tutorial: Using Globus v5 at NCAR (tutorial).</p> <p>CAVEAT: The Globus interface for transferring data does not handle symbolic links and does not create symbolic links on a destination endpoint.</p>"},{"location":"storage-systems/glade/campaign/#data-access-nodes","title":"Data-access nodes","text":"<p>The Campaign Storage file system is mounted on the data-access nodes as <code>/glade/campaign</code> to:</p> <ul> <li> <p>enable users to manage file and directory permissions using POSIX   commands.</p> </li> <li> <p>facilitate transfers of small files to and from GLADE spaces such as   <code>/glade/derecho/scratch</code> and <code>/glade/work</code>.</p> </li> </ul>"},{"location":"storage-systems/glade/campaign/#hpc-system-use","title":"HPC system use","text":"<p>The Campaign Storage file system can be accessed from the Casper and Derecho clusters as <code>/glade/campaign</code> so users are able to:</p> <ul> <li> <p>read and write data directly from their data analysis and   visualization workflows.</p> </li> <li> <p>submit batch scripts to migrate data to the Campaign Storage resource.</p> </li> </ul>"},{"location":"storage-systems/glade/campaign/#data-retention-policy","title":"Data retention policy","text":"<p>Campaign Storage is designed to provide medium-term storage for project data, typically for three to five years. While data will not be purged automatically after five years, retaining data longer will reduce the capacity for storing additional, new data. Users are expected to monitor their holdings, remove files that are no longer needed, and move necessary data to other storage options for longer-term preservation.</p> <p>NCAR researchers are expected to collaborate with CISL\u2019s Digital Asset Services Hub (log in to Sundog) to develop data migration plans for storage needs that exceed five years.</p> <p>University researchers are expected to transfer their project data to their home institutions or other alternative storage repositories within one year of their NSF grant expiring. CISL will not award storage space for researchers to carry data forward from one grant to another.</p>"},{"location":"storage-systems/glade/campaign/#allocations","title":"Allocations","text":""},{"location":"storage-systems/glade/campaign/#ncar-labs","title":"NCAR labs","text":"<p>Each NCAR lab has an allocation of Campaign Storage space and the labs manage how those allocations are used.</p> <p>Users who have questions related to lab allocations should contact the lab's own allocation representative.</p>"},{"location":"storage-systems/glade/campaign/#universities","title":"Universities","text":"<p>University users can request Campaign Storage space through the NCAR Resource Allocation System as supplements to their project allocations. Requests must include detailed justification for the amount of space requested.</p> <p>Because NCAR is not currently funded to provide long-term data storage services to the university community, university users' requests for these allocations are prioritized based on the following factors.</p>"},{"location":"storage-systems/glade/campaign/#higher-priority-is-given-to-requests-if","title":"Higher priority is given to requests if:","text":"<ul> <li> <p>You have an active project, supported by an active NSF award, for   using Cheyenne.</p> </li> <li> <p>Your request is for a period of no more than three (3) months and to   support migrating of your data to your home institution.</p> </li> </ul>"},{"location":"storage-systems/glade/campaign/#lower-priority-is-given-to-requests-if","title":"Lower priority is given to requests if:","text":"<ul> <li>Your need relates to satisfying external requirements or promises \u2013 to   a publisher or agency, for example \u2013 to retain data for extended   periods.</li> </ul> <p>Any data requiring longer storage should be migrated to your home institution or to another appropriate repository.</p>"},{"location":"storage-systems/glade/campaign/#reports","title":"Reports","text":"<p>The Systems Accounting Manager (SAM) provides overall summary information about the use of Campaign Storage allocations and other allocations.</p> <p>CISL is developing additional tools for use in allocation management.</p>"},{"location":"storage-systems/glade/campaign/#automated-data-compression","title":"Automated data compression","text":"<p>Campaign Storage has an automated data compression feature for long-duration data sets.  Our compression policy targets files that are 180 days old or older and 100MB in size or larger for \"z\" compression using IBM Spectrum Scale file system mechanisms (details below).</p> <p>The action is transparent to the user \u2013 that is, no changes to metadata timestamps or reported size occur, and subsequent reads of the data proceed as usual. During a read, the compressed data are sent to the file system client and then transparently uncompressed for application use.</p>"},{"location":"storage-systems/glade/campaign/#tool-and-accounting-behavior","title":"Tool and accounting behavior","text":"<p>The number of blocks reported consumed by the file will change. Note the following tool-specific behavior:</p> Tool Output ls -l shows original (uncompressed) file size stat shows compressed number of blocks, original file size du shows compressed file sizes. du \u2013apparent-size shows original (uncompressed) size gladequota shows project space usage after compression, as do SAM reports <p>Individual data sets can be excluded from the compression algorithm, if necessary. To discuss this option, please submit a request through the NCAR Research Computing help desk.</p>"},{"location":"storage-systems/glade/campaign/#compression-details","title":"Compression details","text":"<p>When a file is considered for compression, the algorithm tests compression of chunks of the file. If the realized compression efficiency of a given chunk is at least 10%, it is then stored compressed on disk.</p> <p>The compression status of a file can be queried via the <code>mmlsattr</code> command. Follow this example: <pre><code>/usr/lpp/mmfs/bin/mmlsattr -L filename\n</code></pre></p> <p>A file has been compressed if the <code>mmlsattr</code> output:</p> <ul> <li> <p>includes \"Misc attributes: COMPRESSION\" \u2013 which indicates that the   file was targeted for compression.</p> </li> <li> <p>does not include \"flags: illCompressed\" \u2013 which indicates the file was   targeted or deferred but not yet compressed.</p> </li> </ul> <p>Several output examples are provided below.</p> <p>User-driven manual compression is also possible before the automated policy is triggered if desired via the <code>mmchattr</code> command: <pre><code>/usr/lpp/mmfs/bin/mmchattr [-I defer] \u254ccompression z filename\n</code></pre></p> <ul> <li> <p>z: best compression (Campaign Storage default)</p> </li> <li> <p>If deferred, the file will be compressed during the next Campaign   Storage policy execution rather than instantly.</p> </li> </ul>"},{"location":"storage-systems/glade/campaign/#examples-commands-and-output","title":"Examples: commands and output","text":""},{"location":"storage-systems/glade/campaign/#run-du-ls-stat-for-an-original-uncompressed-file","title":"Run du, ls, stat for an original uncompressed file","text":"<pre><code>$ du -h 1GB.dat &amp;&amp; du -h --apparent-size 1GB.dat &amp;&amp; ls -lh 1GB.dat &amp;&amp; stat 1GB.dat\n1000M 1GB.dat\n1000M 1GB.dat\n-rw-r-----+ 1 benkirk csg 1000M Mar  9 10:08 1GB.dat\nFile: \u20181GB.dat\u2019\nSize: 1048576000 Blocks: 2048000    IO Block: 8388608 regular file\nDevice: 2dh/45d Inode: 1006073884  Links: 1\nAccess: (0640/-rw-r-----)  Uid: (38057/ benkirk)   Gid: ( 1564/     csg)\nAccess: 2022-03-09 10:08:00.479563000 -0700\nModify: 2022-03-09 10:08:01.486585235 -0700\nChange: 2022-03-09 10:08:01.486585235 -0700\nBirth: -\n</code></pre>"},{"location":"storage-systems/glade/campaign/#request-compression-manually","title":"Request compression manually","text":"<pre><code>$ /usr/lpp/mmfs/bin/mmchattr --compression z 1GB.dat\n</code></pre>"},{"location":"storage-systems/glade/campaign/#run-du-ls-stat-for-a-compressed-file-note-that-metadata-dates-are-not-changed","title":"Run du, ls, stat for a compressed file (note that metadata dates are not changed)","text":"<pre><code>$ du -h 1GB.dat &amp;&amp; du -h --apparent-size 1GB.dat &amp;&amp; ls -lh 1GB.dat &amp;&amp; stat 1GB.dat\n104M 1GB.dat\n1000M 1GB.dat\n-rw-r-----+ 1 benkirk csg 1000M Mar  9 10:08 1GB.dat\nFile: \u20181GB.dat\u2019\nSize: 1048576000 Blocks: 212992     IO Block: 8388608 regular file\nDevice: 2dh/45d Inode: 1006073884  Links: 1\nAccess: (0640/-rw-r-----)  Uid: (38057/ benkirk)   Gid: ( 1564/     csg)\nAccess: 2022-03-09 10:08:00.479563000 -0700\nModify: 2022-03-09 10:08:01.486585235 -0700\nChange: 2022-03-09 10:08:01.486585235 -0700\nBirth: -\n</code></pre>"},{"location":"storage-systems/glade/campaign/#list-file-attributes-to-verify-a-compressed-file","title":"List file attributes to verify a compressed file","text":"<pre><code>$ /usr/lpp/mmfs/bin/mmlsattr -L 1GB.dat\nfile name:            1GB.dat\nmetadata replication: 2 max 2\ndata replication:     1 max 2\nimmutable:            no\nappendOnly:           no\nflags:\nstorage pool name:    DATA\nfileset name:         csg\nsnapshot name:\ncreation time:        Wed Mar  9 10:08:00 2022\nMisc attributes:      ARCHIVE COMPRESSION (library z)\nEncrypted:            no\n</code></pre>"},{"location":"storage-systems/glade/campaign/#request-deferred-compression-of-a-file","title":"Request deferred compression of a file","text":"<pre><code>$ /usr/lpp/mmfs/bin/mmchattr -I defer --compression z 1GB_deferred.dat\n</code></pre> <p>Note that deferred compression is recommended when manually requesting compression for a large number of files. In this case, the <code>mmchattr</code> command will return immediately, and the file compression will occur at the next regularly scheduled system interval.</p>"},{"location":"storage-systems/glade/campaign/#list-file-attributes-note-that-illcompressed-indicates-the-compression-has-not-yet-been-applied","title":"List file attributes (note that \"illcompressed\" indicates the compression has not yet been applied)","text":"<pre><code>$ /usr/lpp/mmfs/bin/mmlsattr -L 1GB_deferred.dat\nfile name:            1GB_deferred.dat\nmetadata replication: 2 max 2\ndata replication:     1 max 2\nimmutable:            no\nappendOnly:           no\nflags:                illcompressed\nstorage pool name:    DATA\nfileset name:         csg\nsnapshot name:\ncreation time:        Wed Mar  9 10:07:17 2022\nMisc attributes:      ARCHIVE COMPRESSION (library z)\nEncrypted:            no\n</code></pre>"},{"location":"storage-systems/glade/lustre/","title":"Lustre scratch file system","text":"<p>The Derecho Storage subsystsem (Destor) scratch file system is a\u00a0Lustre-based Cray ClusterStor E1000 product configured as shown in the table below. An open-source, parallel system, Lustre will be familiar to users of similar POSIX-compliant file systems. This documentation provides a high-level overview of important Lustre concepts and terminology to help users achieve optimal performance.</p>"},{"location":"storage-systems/glade/lustre/#capacity-and-components","title":"Capacity and components","text":"<p>Total capacity of the system is 40 TB of metadata and 60 PB of data.</p> Component Quantity Details Metadata servers (MDS) 4 Each MDS has two 200 GbE CX6 Cassini network interfaces configured in an active/passive failover pair. Metadata targets (MDT) 4 Each metadata server has a single 12 TB MDT composed of 11 drives in a RAID-10 configuration formatted with <code>ldiskfs</code> (~40 TB usable metadata across the entire file system). Object storage servers (OSS) 24 Each OSS has a single 200 GbE CX6 Cassini network interface. Object storage targets (OST) 96 Each OSS has four 582 TB OSTs. Each OST is composed of 53 drives in a GridRAID configuration formatted with <code>ldiskfs</code> (~60PB usable data across the entire file system). (Additional hardware details are provided below)."},{"location":"storage-systems/glade/lustre/#terminology","title":"Terminology","text":""},{"location":"storage-systems/glade/lustre/#metadata-and-data","title":"Metadata and data","text":"<p>The notion of file metadata and data as related but separable entities is important to understanding Lustre because it is fundamental to the system's parallelization strategy. In a POSIX file system, the metadata describes information about a file (name, permissions, access controls, timestamps, and so on), and the data contains the contents of the file itself.</p> <p>Lustre employs one or more metadata servers (MDS) to store the metadata and data layout of each file, and several object storage servers (OSS) to hold the file contents.</p> <p>Each MDS has one or more metadata targets (MDT), which are storage devices attached to the MDS. Similarly, each OSS has one or more object storage target (OST) storage devices. Typically, the MDTs and OSTs are accessible from two different servers, providing fault tolerance and failover capabilities. A typical Lustre file system is shown in Figure 1 below.</p> <p> Figure 1: Sample Lustre file system: 4 metadata servers (MDS), 4 object storage servers (OSS). Credit: Introduction to Lustre Wiki.</p> <p>A file system may employ several metadata servers for scalability and load balancing, and several object storage servers for capacity and performance scalability. When a user creates a file on a Lustre file system, it communicates with an MDS that is responsible for managing the metadata of the file. The MDS also holds the file's striping layout, which is a template used to map file contents (conceptually, data blocks) onto one or more OSTs.</p> <p>This is important to understand because when users are interacting with a Lustre file, they are really interfacing with several storage servers. Different file operations require different server communication requirements. For example, querying a file's modification time is a metadata-only operation and thus requires communication only with the MDS, whereas querying a file's size involves each OSS over which the file is striped.</p>"},{"location":"storage-systems/glade/lustre/#file-striping","title":"File striping","text":"<p>File striping is a key feature of Lustre file systems. A file is said to be striped when its sequence of bytes is separated into small chunks, or stripes, so that read and write operations can involve multiple OSTs concurrently. This process is illustrated in Figures 2 and 3. In Figure 2, the sample file is split into five stripes: the first four are the same size while the fifth is smaller and contains the \"remainder\" of the file. This introduces an important striping concept: the stripe size.</p> <p> Figure 2: Logical view of a file, broken into five \"stripe\" segments. The first four are the same size while the fifth is smaller and contains the \"remainder\" of the file. Credit: Lustre User Guide.</p> <p>Figure 3 shows how the stripes can be mapped onto several OSTs as defined by the stripe count. In this example, the stripe count is four and the stripe segments are assigned in a round-robin fashion.</p> <p> Figure 3: Physical view of a file broken into five stripes across four OST devices. Credit: Lustre User Guide.</p> <p>Striping has important benefits as well as some drawbacks. Striping over more OSTs allows for more bandwidth. In general, as more OSTs are used, more servers are involved, so more network and disk subsystem bandwidth is available. Striping also allows for files larger than any single OST. The primary drawback of striping is overhead: as more OSTs are employed to store a file, more network overhead is required to orchestrate the components.</p> <p>The preceding discussion is focused on striping the blocks of a given file. When multiple MDTs are present in the file system, as is the case with Derecho, metadata striping is also typically employed and the contents of directories are spread across the available MDSs in the system.</p>"},{"location":"storage-systems/glade/lustre/#progressive-file-layouts","title":"Progressive file layouts","text":"<p>The configurable stripe size and stripe count parameters were at one time the only modifiable parameters available to govern striping behavior, which made it difficult to implement a one-size-fits-all default configuration on large systems with varied use cases. The introduction of progressive file layouts (PFLs) in modern Lustre versions, however, extended the striping concept to multiple, progressive segments of files, as shown in Figure 4.</p> <p> Figure 4: Sample progressive file layout with three components of different stripe patterns. Credit: PFL Prototype High Level Design.</p> <p>In Figure 4, a single file is mapped to three separate components, each with a different striping layout. The first component has a stripe size of 1 MB with a stripe count of 1, and is 2 MB in total extent. This means the first 2 MB of the file will be striped over only one OST, in two 1-MB chunks.</p> <p>The second component begins beyond this 2 MB threshold up to 256 MB size. It employs a stripe size of 1 MB but increases the stripe count to four.</p> <p>Finally, the third and final component begins when the file size exceeds 256 MB. The stripe size increases to 4 MB and the stripe count to 32 OSTs.</p> <p>PFLs are useful because they define a template that is much more general than a single stripe size/count pair. They allow small files to be striped over a small number of OSTs and only incur the overhead of additional OST stripes when the file is sufficiently large to benefit from increased bandwidth.</p>"},{"location":"storage-systems/glade/lustre/#inodes-and-data-blocks","title":"<code>inodes</code> and data blocks","text":"<p>When a file system is constructed, the underlying storage device blocks are segregated into two components: data blocks and inodes.</p> <p>Data blocks are the most familiar; storing a 1 GB file simply requires a sufficient number of data blocks to hold its contents.</p> <p>Inodes, by contrast, are index nodes that hold the metadata associated with a file or directory: ownership, time stamps, striping information, and so on. The number of inodes available in a file system is generally fixed and provides a strict limit on the number of files and directories the file system can hold.</p> <p>This is especially important in a Lustre file system. Its capacity is limited by the size and quantity of OSTs, and its file count capacity is also limited by the number of inodes available on the MDTs. In an extreme example, it is possible to exhaust the available inodes in a file system before its storage capacity by creating many tiny files, so it is important to manage both the overall file system data volume and the file count.</p>"},{"location":"storage-systems/glade/lustre/#derechos-destor-lustre-scratch-file-system","title":"Derecho's Destor Lustre scratch file system","text":"<p>With the background provided above, we can now discuss Derecho's Destor Lustre file system specific storage configuration.</p>"},{"location":"storage-systems/glade/lustre/#configuration","title":"Configuration","text":"<p>Derecho's Destor Lustre file system hardware configuration is shown schematically below.  The system has 4 MDS servers (each with a single MDT), and 24 OSS servers (each with 4 OSTs).</p> <p></p>"},{"location":"storage-systems/glade/lustre/#default-striping","title":"Default Striping","text":"<p>Destor default striping configuration</p> <p>Destor uses progressive file layouts (PFLs) to accommodate a wide variety of file sizes and access patterns.</p> File Segment Stripe Count Stripe Size 0-16MB 1 1MB 16MB-16GB 4 16MB 16GB-64GB 12 16MB 64GB+ 24 16MB <p>This default PFL was created at the base of the file system and is inherited by default for all sub-directories. Users may apply alternate striping patterns for any directory or file they own.  (File striping must be set prior to creation and cannot be changed on existing files.)</p> <p>Note that per-directory file striping is inherited by new files and sub-directories created within.</p> <p>The default Destor stripe configuration can be applied if needed with the <code>lfs setstripe</code> command: <pre><code>lfs setstripe \\\n          -E 16M -c 1  -S  1M \\\n          -E 16G -c 4  -S 16M \\\n          -E 64G -c 12 -S 16M \\\n          -E  -1 -c 24 -S 16M\n</code></pre></p> <p>See <code>man lfs-setstripe</code> for additional details.</p>"},{"location":"storage-systems/glade/lustre/#performance-expectations","title":"Performance expectations","text":"<p>Lustre in general, and on Derecho specifically, is designed for high-speed, parallel access to large files.</p> <p>Key points:</p> <ul> <li>Derecho has &gt;2400 compute nodes (each a Lustre client),</li> <li>All scratch metadata traffic, for all users, is served by only 4 MDS servers<ul> <li>Tiny files do not effectively use the primary performance potential.</li> </ul> </li> <li>Large files can be effectively spread across the storage cluster.<ul> <li>5,088 hard drives spread across 96 OSTs served up by 24 OSSes.</li> <li>Destor can deliver over 300 GB/sec of large file access bandwdth.</li> </ul> </li> </ul>"},{"location":"storage-systems/glade/lustre/#jobstats","title":"Jobstats","text":"<p>Lustre provides a mechanism to collate I/O statistics for reporting purposes using the job stats mechanism. On Destor we collect job statistics according to <code>PBS_JOBID</code>.  These statistics are then presented in a graphana dashboard, where users can optionally query the statistics for a particular job.</p>"},{"location":"storage-systems/glade/lustre/#best-practices","title":"Best practices","text":""},{"location":"storage-systems/glade/lustre/#manage-your-file-count-and-file-volume","title":"Manage your file count and file volume","text":"<p>Users have quotas for both data volume and file count on the Derecho Lustre file system. The <code>gladequota</code> utility is preferred for reporting comprehensive storage usage across all GLADE file spaces, including Lustre scratch spaces. Additionally, the <code>lfs quota</code> command can be used to query Lustre-specific quota information.</p> <p>Many simulation codes produce large quantities of small, diagnostic output files that are useful for diagnosing problems but not often referenced for successful production runs. Consider removing or tarring such files incrementally in your workflow to manage overall file count.</p>"},{"location":"storage-systems/glade/lustre/#avoid-unnecessary-metadata-requests","title":"Avoid unnecessary metadata requests","text":"<p>From the background provided above, it is clear not all metadata access requests are equal. Querying a file's timestamps is a cheap operation requiring communication with the appropriate MDT, whereas querying a file's size requires communication with each and every OST holding data stripes. Therefore, it is a best practice to be aware of these performance implications and request only the metadata needed for a given operation.</p> <p>For example, especially when in a large directory with hundreds of files, avoid typing <code>ls -l</code> if a simple <code>ls</code> will do. The former will communicate with every MDS and OSS in the file system in order to determine the current file size, while the latter is simply an MDS communication. Unnecessary communication can make the file system feel slower to you and other users. When file size is required, limit the request to the file(s) of interest when practical.</p> <p>Finally, Lustre provides the notion of a \"lazy\" file size that can be useful in circumstances where approximation is appropriate, for example finding the largest or smallest files in a directory tree. See examples below.</p> <p>Similarly, avoid excessive file status calls when possible. When repeatedly checking a file's status in a script \u2013 inside a loop for example \u2013 consider adding a <code>sleep</code> command as a preventive measure. This will prevent flooding the MDS with status requests when your loop executes very quickly.</p>"},{"location":"storage-systems/glade/lustre/#prefer-lustre-specific-lfs-find-command","title":"Prefer Lustre-specific <code>lfs find</code> command","text":"<p>Lustre's <code>lfs find</code> is an optimized implementation of the familiar <code>find</code> command. It will request only the data required to perform the specified action, and so should be preferred whenever possible. See the examples and use cases below.</p>"},{"location":"storage-systems/glade/lustre/#examples-tools-tips-tricks","title":"Examples, tools, tips, tricks","text":""},{"location":"storage-systems/glade/lustre/#using-df-and-lfs-df-to-query-file-system-status","title":"Using <code>df</code> and <code>lfs df</code> to query file system status","text":"<p>Use the familiar <code>df</code> utility to query overall file system capacity. For example, <code>df -h</code> shows the data size of a file system in a human-readable format: <pre><code>df -h /glade/derecho/scratch\nFilesystem                            Size  Used Avail Use% Mounted on\n10.14.64.3@tcp:10.14.64.4@tcp:/desc1   55P  5.9P   49P  11% /glade/derecho/scratch\n</code></pre></p> <p>Use <code>df -ih</code> to get the corresponding metadata information: <pre><code>df -ih /glade/derecho/scratch\nFilesystem                           Inodes IUsed IFree IUse% Mounted on\n10.14.64.3@tcp:10.14.64.4@tcp:/desc1    16G  105M   16G    1% /glade/derecho/scratch\n</code></pre></p> <p>In the example, the file system overall capacity is 1.2 PB, of which 35 TB is used. The file system has 1.2 billion inodes, 19 million of which are used, providing an additional limit on the total number of files and directories that can be stored.</p> <p>Running <code>lfs df</code> provides similar information but at the Lustre-aware component level. For example, <code>lfs df -h</code> shows the data size broken down by MDS and OST components:</p> <pre><code>lfs df -h /glade/derecho/scratch\nUUID                       bytes        Used   Available Use% Mounted on\ndesc1-MDT0000_UUID         11.8T       26.9G       11.6T   1% /glade/derecho/scratch[MDT:0]\ndesc1-MDT0001_UUID         11.8T       29.7G       11.6T   1% /glade/derecho/scratch[MDT:1]\ndesc1-MDT0002_UUID         11.8T       32.7G       11.6T   1% /glade/derecho/scratch[MDT:2]\ndesc1-MDT0003_UUID         11.8T       22.9G       11.6T   1% /glade/derecho/scratch[MDT:3]\ndesc1-OST0000_UUID        581.4T       62.3T      513.2T  11% /glade/derecho/scratch[OST:0]\ndesc1-OST0001_UUID        581.4T       61.7T      513.9T  11% /glade/derecho/scratch[OST:1]\ndesc1-OST0002_UUID        581.4T       62.2T      513.3T  11% /glade/derecho/scratch[OST:2]\ndesc1-OST0003_UUID        581.4T       62.0T      513.5T  11% /glade/derecho/scratch[OST:3]\n...\ndesc1-OST005c_UUID        581.4T       61.7T      513.8T  11% /glade/derecho/scratch[OST:92]\ndesc1-OST005d_UUID        581.4T       62.2T      513.3T  11% /glade/derecho/scratch[OST:93]\ndesc1-OST005e_UUID        581.4T       61.8T      513.7T  11% /glade/derecho/scratch[OST:94]\ndesc1-OST005f_UUID        581.4T       62.4T      513.1T  11% /glade/derecho/scratch[OST:95]\n\nfilesystem_summary:        54.5P        5.8P       48.1P  11% /glade/derecho/scratch\n</code></pre> <p>This sample file system is composed of four MDTs and 96 OSTs, and <code>lfs df</code> shows the data size of each component. Administrators typically monitor this information to ensure overall file system health, but it can provide useful user diagnostics as well. If one or more of the OSTs is temporarily unavailable due to a storage server issue, for example, <code>lfs df</code> will hang at the affected component, indicating the file system is not healthy. Using <code>lfs dh -ih</code> works similarly, showing the per-component inode usage. Because Lustre file systems typically have a smaller number of MDTs than OSTs, the per-MDT inode usage is an important bound on the overall file system file/directory count capacity.</p>"},{"location":"storage-systems/glade/lustre/#using-lfs-find-to-change-directory-tree-ownership-or-permissions","title":"Using <code>lfs find</code> to change directory tree ownership or permissions","text":"<p>Tools such as <code>chown</code>, <code>chgrp</code>, and <code>chmod</code> provide a recursive option to allow easy application to all the contents of a given directory. Best practice is to avoid such features and invoke the desired action through <code>lfs find</code> instead.</p> <p>For example, if you want to change the group ownership of an entire directory tree, you might run a command similar to <code>chgrp -R &lt;newgroup&gt; &lt;dirname&gt;</code>. However, you can do it more efficiently \u2013 albeit more verbosely \u2013 with <code>lfs find</code> as follows: <pre><code>lfs find &lt;dirname&gt; -print0 | xargs -0 chgrp &lt;newgroup&gt;\n</code></pre></p> <p>To be UNIX-specific about the preceding command, it first asks <code>lfs find</code> to list all the contents of a directory and print them separated with a <code>NULL</code> character (<code>\\0</code>). This list is then sent to the command <code>xargs</code>, which is told to expect a <code>NULL</code>-separated list with the <code>-0</code> flag. Then <code>xargs</code> will run the command <code>chgrp &lt;newgroup&gt;</code> on batches of files and split what could be a long list of files into small enough chunks to comply with UNIX's maximum command line argument restrictions. See <code>man lfs-find</code> and <code>man xargs</code> for additional details and examples.</p>"},{"location":"storage-systems/glade/lustre/#using-lfs-find-to-tar-a-directory-tree","title":"Using <code>lfs find</code> to <code>tar</code> a directory tree","text":"<p>This example shows how to create a <code>tar</code> archive file from a specified directory tree efficiently: <pre><code>lfs find &lt;dirname&gt; -print0 | \\\n   tar --create --acls --no-recursion \\\n      --verbose --index-file=my_archive.idx --tape-length=1G \\\n      --file=my_archive-{0000..9999}.tar --null -T -\n</code></pre></p> <p>First, <code>lfs find</code> will list all contents of the directory, <code>NULL</code> separated. Then <code>tar</code> will operate on the list of files and subdirectories. Its behavior is modified by the following flags:</p> <ul> <li> <p><code>--create</code> \u2013 Create a tar archive.</p> </li> <li> <p><code>--acls</code> \u2013 Include any file/subdirectory access control lists (ACLs)   encountered in the output tar files. This option is necessary to   preserve ACL information when unpacking the archives later.</p> </li> <li> <p><code>--no-recursion</code> \u2013 List everything in the directory: files, links,   subdirectories, etc. By default <code>tar</code> will recurse into any   directory name it encounters, so <code>--no-recursion</code> tells it not to do   so, since the contents will be listed anyway. Combined   with <code>--acls</code> this allows you to properly set ACLs on directories.</p> </li> <li> <p><code>--verbose</code> \u2013 Print each file/subdirectory as it is processed.</p> </li> <li> <p><code>--index-file=my_archive.idx</code> \u2013 Redirect the list created   by <code>--verbose</code> into a file named <code>my_archive.idx</code>.</p> </li> <li> <p><code>--tape-length=1G --file=my_archive-{0000..9999}.tar</code> \u2013 This   instructs <code>tar</code> to create a series of files   \u2013 <code>my_archive-0000.tar</code>, <code>my_archive-0001.tar</code>, and so on \u2013 in which   each file is no larger than 1 GB.</p> </li> <li> <p><code>--null -T -</code> \u2013 This tells <code>tar</code>that the input file list is   <code>NULL</code>-separated and coming in on standard input.</p> </li> </ul> <p>The process creates several tar files but does not modify the original source tree directory. One consequence is that storage volume increases during this process until the user removes the directory. An alternative to consider carefully is to also use the <code>--remove-files</code> option. It will remove each source file after it is successfully added to the tar archive, so the overall storage requirements should remain flat. This is just one example of many possibilities with this approach. See <code>man lfs-find</code> and <code>man tar</code> for more ideas.</p>"},{"location":"storage-systems/glade/lustre/#using-lfs-find-lazy-to-efficiently-locate-old-large-files","title":"Using <code>lfs find --lazy</code> to efficiently locate old, large files","text":"<p>Determining the precise size of a file on a Lustre system is generally an expensive operation in that it requires communication with every object storage server that stores segments of the file. In some cases, knowing the approximate file size may be sufficient, and it can be obtained solely from the metadata server(s). For example, to locate all files in a directory modified seven or more days ago that are approximately 10 MB or larger, run: <pre><code>lfs find &lt;dirname&gt; --lazy --size +10M --mtime +7 -type f -print\n</code></pre></p> <p>The <code>--lazy</code> flag requests the approximate file size instead of requiring the precise size and the associated communication overhead. That approach can be useful for quickly locating files to clean up and recover quota space. It could also be combined with <code>xargs</code> and <code>rm</code> to remove the files, similar to the <code>chgrp</code> example.</p>"},{"location":"storage-systems/glade/lustre/#more-resources","title":"More resources","text":"<ul> <li> <p>Introduction to   Lustre</p> </li> <li> <p>lustre.org</p> </li> <li> <p>Oak Ridge Leadership Computing Facility Lustre 101   resources</p> </li> </ul>"},{"location":"storage-systems/glade/recovering-files-from-snapshots/","title":"Recovering files from snapshots","text":"<p>CISL creates snapshots of the GLADE home file space several times each day in addition to multiple backups each week. These snapshots are records of the state of the file system at various times.</p> <p>Snapshots enable you to retrieve copies of deleted files yourself, quickly and easily, or recover earlier versions of files that have been revised. (To recover files or directories from backups rather than snapshots, contact the NCAR Research Computing help desk.)</p> <p>The number of snapshots that are available at any one time varies, and the intervals between snapshots may change at any time without prior notice.</p>"},{"location":"storage-systems/glade/recovering-files-from-snapshots/#retrieving-directories-and-files","title":"Retrieving directories and files","text":"<p>If you need to retrieve directories or files, first determine if they are available in one or more snapshots by running <code>snapls</code> as shown below, then copy the files to your home space. The files will retain the permissions that existed when the snapshot was created.</p>"},{"location":"storage-systems/glade/recovering-files-from-snapshots/#find-a-directory","title":"Find a directory","text":"<p>To see if your current directory is present in any snapshots, just run <code>snapls</code> on your command line. You can also specify a directory by executing <code>snapls -ldhtr directory_name</code>.</p> <p>In this example, your current directory is <code>/glade/u/home/username</code>. The output from snapls identifies recent snapshots with date and time stamps in this format: <code>YYYYMMDD-hhmmss</code>.</p> <pre><code>snapls\n\ndrwxr-xr-x 41 username ncar 16384 Jul 8 11:51\n/glade/u/home/.snapshots/20200208-120001/username\n\ndrwxr-xr-x 41 username ncar 16384 Jul 7 10:49\n/glade/u/home/.snapshots/20200207-110001/username\n\ndrwxr-xr-x 40 username ncar 16384 Jul 7 09:59\n/glade/u/home/.snapshots/20200207-100001/username\n\ndrwxr-xr-x 40 username ncar 16384 Jul 7 13:25\n/glade/u/home/.snapshots/20200207-180001/username\n</code></pre> <p>Change to /username in the most recent snapshot directory that is identified. <pre><code>cd /glade/u/home/.snapshots/20200208-120001/username\n</code></pre></p> <p>Copy the files that you need back to your home directory or a subdirectory. <pre><code>cp file1 file2 file3 /glade/u/home/username\n</code></pre></p>"},{"location":"storage-systems/glade/recovering-files-from-snapshots/#find-and-copy-a-file","title":"Find and copy a file","text":"<p>You can find an individual file by identifying it as in this example. The output shows that <code>filename</code> is available in two snapshots. <pre><code>snapls -ldhtr filename\n\ndrwxr-xr-x 40 username ncar 16384 Jul 7 09:59\n/glade/u/home/.snapshots/20200207-100001/username\n\ndrwxr-xr-x 40 username ncar 16384 Jul 7 13:25\n/glade/u/home/.snapshots/20200207-180001/username\n</code></pre> When you identify the file you want, you can copy it back to your current directory as shown here. <pre><code>cp /glade/u/home/.snapshots/20200207-100001/username/filename .\n</code></pre></p>"},{"location":"storage-systems/glade/recovering-files-from-snapshots/#comparing-snapshots","title":"Comparing snapshots","text":"<p>You can use the <code>diff</code> command to identify changes that were made between snapshots, as in this example. <pre><code>diff /glade/u/home/.snapshots/20200208-100001/username/filename \\\n     /glade/u/home/.snapshots/20200207-180001/username/filename\n</code></pre></p> <p>This can be useful if you need to roll back to an earlier version of a file, but it is not a substitute for following version control best practices.</p> <p>The <code>diff</code> command is best used for comparing single files or small directory trees within snapshots.</p>"},{"location":"storage-systems/glade/removing-large-number-of-files/","title":"Removing large numbers of files","text":"<p>The recommended way to remove thousands or hundreds of thousands of files from a <code>GLADE</code> directory is by running a batch job on Casper.</p> <p>Removing large numbers of files can take several hours, so you will need to provide enough wall-clock time in the job to accommodate this. You can use the sample script below with your own project code, job name, and other customizations.</p> <p>Test your file removal process first!!</p> <p>Before removing large numbers of files, create a \"play\" directory inside your <code>/glade/derecho/scratch</code> user space and try the batch job with some fictional files and subdirectories to make sure that it does what you want. Carefully specify the files that you want removed before submitting a job like this.</p>"},{"location":"storage-systems/glade/removing-large-number-of-files/#create-job-script-and-submit","title":"Create job script and submit","text":"<p>Create a job script following the example just below. To submit the job when your script is ready, run the PBS Pro <code>qsub</code> command followed by the name of your script file, as in this example: <pre><code>qsub remove_files.pbs\n</code></pre></p>"},{"location":"storage-systems/glade/removing-large-number-of-files/#example-scripts","title":"Example scripts","text":"<p>Removing files or complete subdirectories</p> Removing FilesRemoving a Directory Tree remove_files.pbs<pre><code>#!/bin/bash\n#PBS -N remove_files_job\n#PBS -A &lt;project_code&gt;\n#PBS -l select=1:ncpus=1\n#PBS -l walltime=24:00:00\n#PBS -j oe\n#PBS -q casper\n\nexport TMPDIR=${SCRATCH}/temp &amp;&amp; mkdir -p ${TMPDIR}\n\n# the subdirectory and file pattern for removal\nsubdir=\"/glade/derecho/scratch/${USER}/directory_name\"\nfile_pattern=\"files_to_remove*\"\n\n# testing mode (set to False to actually remove files)\ntest_only=True\n\n[[ True == ${test_only} ]] &amp;&amp; do_echo=\"echo\" || do_echo=\"\"\n\ncd ${subdir} || { echo \"cannot cd ${subdir}, aborting!!\"; exit 1; }\n\n# use the \"lfs find\" command to locate files in the requested\n# subdirectory matching the specified file pattern.\n# We then send this list, NULL separated, to the xargs command\n# to do the removal.\nlfs find ${subdir} -type f -name \"${file_pattern}\" -print0 | \\\n    xargs -0 ${do_echo} rm -f\n</code></pre> remove_subdir.pbs<pre><code>#!/bin/bash\n#PBS -N remove_files_job\n#PBS -A &lt;project_code&gt;\n#PBS -l select=1:ncpus=1\n#PBS -l walltime=24:00:00\n#PBS -j oe\n#PBS -q casper\n\nexport TMPDIR=${SCRATCH}/temp &amp;&amp; mkdir -p ${TMPDIR}\n\n# the subdirectory and file pattern for removal\nsubdir=\"/glade/derecho/scratch/${USER}/directory_name\"\n\n# testing mode (set to False to actually remove files)\ntest_only=True\n\n[[ True == ${test_only} ]] &amp;&amp; do_echo=\"echo\" || do_echo=\"\"\n\ncd ${subdir} || { echo \"cannot cd ${subdir}, aborting!!\"; exit 1; }\n\n# (1) use the \"lfs find\" command to locate files in the requested\n# subdirectory. We then send this list, NULL separated,\n# to the xargs command to do the removal.\nlfs find . ! -type d -print0 | \\\n    xargs -0 ${do_echo} rm -f\n\n# (2) remove the entire subdirectory tree\ncd -\n${do_echo} rm -rf ${subdir}\n</code></pre>"},{"location":"storage-systems/glade/setting-file-directory-permissions/","title":"Setting file and directory permissions","text":"<p>This information is intended to help GLADE file system users understand common POSIX-standard commands. Note that:</p> <ul> <li> <p>Experienced users may prefer to manage permissions using octal numbers   rather than the methods described below.</p> </li> <li> <p>Some also find access control lists (ACLs) useful for facilitating   short-term file sharing among selected users. See Using access control lists.</p> </li> </ul>"},{"location":"storage-systems/glade/setting-file-directory-permissions/#existing-files-and-directories","title":"Existing files and directories","text":"<p>Should you need to change permissions for existing files or directories \u2013 to allow other users to modify or execute them, for example \u2013 follow the chmod examples below.</p>"},{"location":"storage-systems/glade/setting-file-directory-permissions/#new-files-and-directories","title":"New files and directories","text":"<p>Files and directories that you create in your GLADE file spaces have certain permissions by default. To change the default settings, use the umask command described below.</p> <p>Don\u2019t run sudo on NCAR systems</p> <p>If you need help with tasks that you think require <code>sudo</code> privileges, or if you aren\u2019t sure, please contact HPC User Support before trying to run sudo yourself. The command fails when unauthorized users run it and sends a security alert to system administrators.</p>"},{"location":"storage-systems/glade/setting-file-directory-permissions/#about-permissions","title":"About permissions","text":"<p>Files in a UNIX system have associated permissions that determine who can read (<code>r</code>), write (<code>w</code>), and execute (<code>x</code>) them.</p> <p>Directory permissions use those same flags to indicate who can list files in a directory (<code>r</code>), create and remove files in the directory (<code>w</code>), or cd into or traverse (<code>x</code>) the directory. Carefully consider both the file permissions and the directory permissions to get the desired end result. For example, you can give a user read permission for a file, but the user won't have access to it without also having permission to traverse the directory tree that contains the file.</p> <p>Three additional things to note regarding directory permissions:</p> <ol> <li> <p>Users who have write permission for a directory can delete files in     the directory without having write permission for those files.</p> </li> <li> <p>Subdirectories can have less restrictive permissions than their     parent directories. However, if you change directory permissions     recursively     (see chmod below), you     are changing them for all of the files and subdirectories in that     directory tree.</p> </li> <li> <p>An alternative to changing permissions recursively is to set them     selectively as shown in this example below.</p> </li> </ol> <p>About execute flags: <code>X</code> vs. <code>x</code></p> <p>When setting permissions, the execute flag can be set to upper-case <code>X</code>, which differs from the lower-case <code>x</code> setting. The <code>X</code> permission allows execution only if the target is a directory or if the execute permission has already been set for the user or group. It is useful in the case of handling directory trees recursively.</p> <p>To see who can work with your files and directories, log in and look at the output of an <code>ls \u2011l</code> command.</p> <p>Here is an example.</p> <p></p> <p>The first column is a string of 10 permission flags.</p> <p>The first flag indicates, for most directory contents, that what is listed is a file (-) or a directory (d).</p> <p>The other nine flags, in groups of three, indicate:</p> Permission Field Representation the user\u2019s (owner\u2019s) permissions -rwxr-xr-x group members\u2019 permissions -rwxr-xr-x others\u2019 permissions -rwxr-xr-x <p>\"Others\" means everyone else who can log in on the machine.</p>"},{"location":"storage-systems/glade/setting-file-directory-permissions/#changing-permissions-with-chmod","title":"Changing permissions with <code>chmod</code>","text":"<p>To modify the permission flags on existing files and directories, use the <code>chmod</code> command (\"change mode\"). It can be used for individual files or it can be run recursively with the <code>-R</code> option to change permissions for all of the subdirectories and files within a directory.</p> <p>The <code>chmod</code> command specifies which class or classes (<code>u</code>: user, <code>g</code>: group, <code>o</code>: other) have access to the file or directory in various modes (<code>r</code>: read, <code>w</code>: write, <code>x</code>: execute).</p> <ul> <li> <p>Use the operators + and - to add or remove selected   permissions for a class without changing its other permissions.</p> </li> <li> <p>Use = to specify all of the permissions for a class at once. If a   class is not mentioned explicitly, the permissions are unchanged even   if the = operator is used for a different class.</p> </li> </ul> <p>Follow this format: <pre><code>chmod [classes][operator][modes] filename\n</code></pre></p>"},{"location":"storage-systems/glade/setting-file-directory-permissions/#examples","title":"Examples","text":""},{"location":"storage-systems/glade/setting-file-directory-permissions/#add-selected-permissions-for-a-group","title":"Add selected permissions for a group","text":"<p>Only the owner can read, write, and execute this file: <pre><code>-rwx------ 1 username group 57 Apr 11 12:29 filename\n</code></pre> Add group (<code>g</code>) permissions to read (<code>r</code>) and execute (<code>x</code>) like this: <pre><code>chmod g+rx filename\n</code></pre> The new file permissions are shown here: <pre><code>-rwxr-x--- 1 username group 57 May 14 09:54 filename\n</code></pre> Note that the permissions that were not specified were not changed: The user class permissions and other class permissions did not change, and the writing permission for the group class remains unchanged.</p>"},{"location":"storage-systems/glade/setting-file-directory-permissions/#specify-all-permissions-for-a-group","title":"Specify all permissions for a group","text":"<p>To set permissions for a single class, such as group (<code>g</code>), use the = operator. <pre><code>chmod g=rx filename\n</code></pre> In this case, the only permissions affected were those for the specified class: group. The group can only read or execute the file, but not write. Permissions for the user class and other class were not changed because they were not specified.</p>"},{"location":"storage-systems/glade/setting-file-directory-permissions/#specify-permissions-for-sets-of-classes","title":"Specify permissions for sets of classes","text":"<p>To set permissions for multiple classes with a single command, separate the class settings with a comma. <pre><code>chmod u=rwx,g=rwx,o+rx filename\n</code></pre> The new file permissions are shown here: <pre><code>-rwxrwxr-x 1 username group 57 May 14 09:54 filename\n</code></pre></p>"},{"location":"storage-systems/glade/setting-file-directory-permissions/#set-permissions-selectively","title":"Set permissions selectively","text":"<p>This example shows how to give your group access to all of the files and subdirectories in a directory but limit other users' access to specified files. <pre><code>chmod -R u=rwx,g=rwx,o+x /glade/u/home/username/directory/\n\nchmod u=rwx,g=rwx,o+rx\n/glade/u/home/username/directory/subdirectory/file1\n\nchmod u=rwx,g=rwx,o+rx\n/glade/u/home/username/directory/subdirectory/file2\n</code></pre> The result is that group members have all rights to files in the specified directories and subdirectories. Others have permission to traverse the directories as needed to read and execute two specified files.</p>"},{"location":"storage-systems/glade/setting-file-directory-permissions/#changing-default-permissions-with-umask","title":"Changing default permissions with umask","text":"<p>To change the default permissions that are set when you create a file or directory within a session or with a script, use the umask command.</p> <p>The syntax is similar to that of chmod (above), but use the = operator to set the default permissions.</p>"},{"location":"storage-systems/glade/setting-file-directory-permissions/#examples_1","title":"Examples","text":"<p>The <code>umask</code> examples shown here will give you and group members read, write, and execute permission. Others will have only read and execute permission.</p>"},{"location":"storage-systems/glade/setting-file-directory-permissions/#bash-users","title":"bash users","text":"<pre><code>umask u=rwx,g=rwx,o=rx\n</code></pre>"},{"location":"storage-systems/glade/setting-file-directory-permissions/#tcsh-users","title":"tcsh users","text":"<pre><code>umask 002\n</code></pre>"},{"location":"storage-systems/glade/setting-file-directory-permissions/#managing-groups","title":"Managing groups","text":"<p>Several additional commands are useful for managing groups to control who can access files and directories. For example, you can limit access to users who share your core-hour or storage space allocation.</p> <p>Say you don't want all members of the ncar group to have group permissions to read, write, and execute certain files. You should have a UNIX group that corresponds to your project code \u2013 such as group uabc0001 for project code UABC0001. You can use the commands described below to set or change group ownership of certain files and directories so only members of that UNIX group have permission to access them.</p> <p>If there is no group that allows you to share as needed with other users who have NCAR user accounts:</p> <ul> <li> <p>Consider using the setfacl command to set up an   access control list.</p> </li> <li> <p>Request creation of a custom group.</p> </li> </ul> <p>To share with colleagues who do not have NCAR user accounts, see Sharing data and making unattended transfers.</p>"},{"location":"storage-systems/glade/setting-file-directory-permissions/#identifying-current-group-and-others-id","title":"Identifying current group and others - <code>id</code>","text":"<p>Files or directories that you create or edit become associated with your current UNIX group. Usually, that is your default primary group unless you change groups after you log in. (See \"Changing current group\" below.)</p> <p>If you aren't sure what your current group is, or which other groups you belong to, you can find out by running the <code>id</code> command after you log in. It will return your user ID (<code>uid</code>) and your current group (<code>gid</code>), and it will list any other groups with which you are associated.</p> <p>Example: <pre><code>id\n\nuid=12345(jsmith) gid=1000(ncar) groups=1000(ncar),54321(cisl)\n</code></pre></p>"},{"location":"storage-systems/glade/setting-file-directory-permissions/#changing-current-group-sg","title":"Changing current group - sg","text":"<p>To change from one group to another during a login session, follow this example using the <code>sg</code> command and the name of the new group. <pre><code>sg new_groupname\n</code></pre></p> <p>The command will start a new shell with your new current group ID in effect. When you exit that shell, you change back to your previously used group ID.</p> <p>Some users prefer <code>newgrp</code> over <code>sg</code> for this, but <code>sg</code> has the advantage of retaining your existing user environment while changing your current group.</p>"},{"location":"storage-systems/glade/setting-file-directory-permissions/#changing-default-group","title":"Changing default group","text":"<p>To change your default primary group \u2013 the group that will be in effect each time you subsequently log in \u2013 use the Systems Accounting Manager (SAM). Changes made in SAM typically take effect the next business day.</p>"},{"location":"storage-systems/glade/setting-file-directory-permissions/#changing-group-ownership-of-a-file-or-directory-chgrp","title":"Changing group ownership of a file or directory - chgrp","text":"<p>Use <code>chgrp</code> as shown here to change ownership of a file or directory to a different group. <pre><code>chgrp new_groupname filename\n\nchgrp new_groupname directory\n</code></pre></p> <p>To change group ownership of a directory and all of the files and subdirectories in that directory, use chgrp recursively. <pre><code>chgrp -R new_group directory\n</code></pre></p>"},{"location":"storage-systems/glade/using-access-control-lists/","title":"Using access control lists","text":"<p>Access control lists (ACLs) are tools for managing permissions within a file system by giving users and groups read, write, and/or execute permissions on files or directories outside of the traditional UNIX permissions. The UNIX permissions for managing files on GLADE remain in effect, but ACLs can be used to facilitate short-term file sharing among users who cannot be put in the same UNIX group.</p> <p>In the Cheyenne/GLADE environment, the most common use cases are:</p> <p>Sharing files among users in different NCAR labs or universities. Sharing files with short-term visitors, interns, students, or others during a short project period. Following are examples of how to create an ACL that allows other individuals and groups to work with your files, and how to propagate permissions to new files and directories. To create and manage ACLs on the Campaign Storage file system, log in to Casper or the data-access nodes rather than Cheyenne.</p>"},{"location":"storage-systems/glade/using-access-control-lists/#create-and-view-an-access-control-list","title":"Create and view an access control list","text":"<p>Use the <code>setfacl</code> command with the <code>--modify</code> option to give an individual user access to a file or directory that you own.</p> <p>In this example, user shiquan is sharing a file with user bjsmith. Listing the file before and after creating the ACL shows how the file permissions changed. <pre><code>ls -l testfile.txt\n-rw------- 1 shiquan ncar 18 Mar 4 09:00 testfile.txt\n\nsetfacl --modify user:bjsmith:rwx testfile.txt\nls -l testfile.txt\n-rw-rwx---+ 1 shiquan ncar 18 Mar 4 09:00 testfile.txt\n</code></pre> The <code>+</code> as the last character in the permissions string indicates that an ACL exists for the directory.</p>"},{"location":"storage-systems/glade/using-access-control-lists/#about-execute-flags-x-vs-x","title":"About execute flags: <code>X</code> vs. <code>x</code>","text":"<p>When setting permissions, the execute flag can be set to upper-case <code>X</code>, which differs from the lower-case <code>x</code> setting. The <code>X</code> permission allows execution only if the target is a directory or if the execute permission has already been set for the user or group. It is useful in the case of handling directory trees recursively.</p> <p>Run <code>getfacl</code> as shown here to view any ACLs applied to a file or directory.</p> <pre><code>getfacl testfile.txt\n</code></pre> <p>The output will look something like this, illustrating in this case that one user's permissions (<code>rwx</code>) are different from other users' permissions (<code>rw-</code>). <pre><code># file: testfile.txt\n# owner: shiquan\n# group: ncar\nuser::rw-\nuser:bjsmith:rwx\ngroup::---\nmask::rwx\nother::---\n</code></pre></p>"},{"location":"storage-systems/glade/using-access-control-lists/#give-access-to-a-group","title":"Give access to a group","text":"<p>To give an existing group access to a file or directory, use <code>setfacl</code> as shown here. In this example, the name of the group being given read/write/execute permissions is <code>csg</code>.</p> <pre><code>setfacl --modify group:csg:rwx testfile.txt\n</code></pre> <p>Run <code>getfacl</code> to see the resulting ACL. <pre><code>getfacl testfile.txt\n\n# file: testfile.txt\n# owner: shiquan\n# group: ncar\nuser::rw-\nuser:bjsmith:rwx\ngroup::---\ngroup:csg:rwx\nmask::rwx\nother::---\n</code></pre></p>"},{"location":"storage-systems/glade/using-access-control-lists/#use-default-acls-to-propagate-permissions","title":"Use default ACLs to propagate permissions","text":"<p>ACLs can be set to propagate permissions to files and subdirectories as they are created. This is done using default ACLs.</p> <p>In this example, a new directory is accessible only to the user who created it. <pre><code>drwx------  2 shiquan ncar 4096 Mar  6 10:12 my_shared_directory/\n</code></pre> To enable another user to read and navigate into the directory, follow this example. <pre><code>setfacl --modify user:bjsmith:r-x my_shared_directory\n</code></pre> To set the directory's default ACL so that any new files or subdirectories automatically have those same permissions, use the <code>setfacl</code> command as shown here.</p> <p><pre><code>setfacl --modify default:user:bjsmith:r-x my_shared_directory\n\nls -ld my_shared_directory\ndrwxr-x---+ 2 shiquan ncar 4096 Mar  6 10:12 my_shared_directory\n</code></pre> The <code>+</code> as the last character in the permissions string indicates that an ACL exists for the directory.</p> <p>Run <code>getfacl</code> to see the resulting ACL and the default ACLs. <pre><code>getfacl my_shared_directory\n\n# file: my_shared_directory\n# owner: shiquan\n# group: ncar\nuser::rwx\nuser:bjsmith:r-x\ngroup::---\nmask::r-x\nother::---\ndefault:user::rwx\ndefault:user:bjsmith:r-x\ndefault:group::---\ndefault:mask::r-x\ndefault:other::---\n</code></pre></p>"},{"location":"storage-systems/glade/using-access-control-lists/#default-permissions-for-a-specified-user","title":"Default permissions for a specified user","text":"<p>ACLs for a specified user or group are independent of default ACLs.</p> <p>The next example illustrates how to modify a default ACL to set default permissions for a specified user.</p> <pre><code>setfacl --default --modify user:bjsmith:rwx my_shared_directory\n</code></pre> <p>The directory now has a default ACL that will make any new file in the directory accessible to the designated user with the specified permissions (rwx). Here is an example of a new file created in that directory.</p> <pre><code>-rw-rw----+ 1 shiquan ncar 23 Mar  6 10:19 my_shared_directory/newfile\n</code></pre> <p>Most users in the group get <code>rw-</code> permission. However, as a result of the default ACL behavior established above, user <code>bjsmith</code>'s permissions are different (<code>rwx</code>). The following <code>getfacl</code> output with the comment <code>#effective:rw-</code> shows the difference.</p> <pre><code>getfacl my_shared_directory/newfile\n\n# file: my_shared_directory/newfile\n# owner: shiquan\n# group: ncar\nuser::rw-\nuser:bjsmith:rwx #effective:rw-\ngroup::---\nmask::rw-\nother::---\n</code></pre>"},{"location":"storage-systems/glade/using-access-control-lists/#remove-an-access-control-list","title":"Remove an access control list","text":"<p>To remove all ACLs from a file, run <code>setfacl --remove-all</code> followed by the filename. <pre><code>setfacl --remove-all testfile.txt\n</code></pre> To remove selected permissions previously set for a user, run <code>setfacl -x</code> as shown here. <pre><code>setfacl -x user:bjsmith testfile.txt\n</code></pre></p>"},{"location":"storage-systems/glade/using-access-control-lists/#advanced-use-of-acls","title":"Advanced use of ACLs","text":"<p>Users often have different default umask settings that can conflict with a file or directory ACL. The following example sets an ACL for a directory and all of its files and subdirectories, and it also sets the default ACL for any future files and subdirectories, ensuring the directory is protected from unknown umask settings. <pre><code>setfacl  -R -m o:r-x  -dm o:r-x  /glade/scratch/bjsmith\n</code></pre> Note that there are two clauses in this example:</p> <ol> <li>The first (<code>-R -m o:r-x</code>) recursively sets permissions for \"others\" to read and execute (<code>r-x</code>) on all existing files and subdirectories under <code>/glade/scratch/bjsmith</code>.</li> <li>The second (<code>-dm o:r-x</code>) sets the default ACL for \"others\" to read and execute (<code>r-x</code>) on all future files and subdirectories.</li> </ol>"},{"location":"storage-systems/glade/using-access-control-lists/#more-information","title":"More information","text":"<p>See the manual pages for the commands for more information.</p> <ul> <li><code>man setfacl</code></li> <li><code>man getfacl</code></li> </ul>"},{"location":"storage-systems/quasar/","title":"Quasar archive for data collections","text":"<p>The Quasar archive is a cold, tape-based archive for storing curated data collections that have an indefinite lifetime. It is not designed to serve data or to store data that will be frequently accessed, overwritten, or deleted. (Active data should be on GLADE or Campaign Storage rather than on Quasar.)</p> <p>Before requesting access, please review the following information regarding how to archive files, the minimum and maximum file sizes, and related use policies.</p>"},{"location":"storage-systems/quasar/#storing-data","title":"Storing data","text":"<p>Users store data on Quasar by transferring files via the Globus mapped collection named NCAR Quasar. For documentation about how to use Globus, see Globus file transfers.</p> <p> A note about verifying Globus transfers</p> <p>Using the Globus checksum sync option when transferring files can result in \"operation timed out\" error messages when it causes file recalls from tape, which can be slow.</p> <p>To avoid such errors when doing an incremental backup, use a different sync level \u2013 exists, mtime or size, for example \u2013 when making the transfer.</p> <p>To verify that a just-completed transfer did not encounter any corruption, do the checksum immediately to complete it before files are transferred to tape and purged from the disk cache.</p>"},{"location":"storage-systems/quasar/#file-size-requirements","title":"File size requirements","text":"<p>The following requirements apply to files stored in a project's high-level allocation (RDA or EOL, for example).</p> <ul> <li> <p>The maximum file size is 5 TB.</p> </li> <li> <p>The target file size for Quasar is 1 GB or larger, so use a tool   like tar to combine multiple smaller files into a larger file or files   before storing them.</p> </li> <li>At least 90% of a project's files must be at least 100 MB.</li> <li>Up to 10% of a project's files may be smaller than 100 MB.</li> <li>The length of a file name and its full path name cannot exceed 1022   characters.</li> </ul>"},{"location":"storage-systems/quasar/#file-reads-and-datametadata-change-frequency","title":"File reads and data/metadata change frequency","text":"<p>The system is designed to support large file writes effectively. As a tape-based archive, however, it is not designed to support frequent read activity. File reads should be infrequent, and data and metadata changes should also be rare.</p> <p>Under normal operational use, no more than 10% of your files should be read, rewritten, renamed, or deleted during any 12-month period. If a special case arises \u2013 a recovery operation, for example \u2013 and you anticipate more activity, please contact the NCAR Research Computing help desk.</p>"},{"location":"storage-systems/quasar/#disaster-recovery","title":"Disaster recovery","text":"<p>Disaster recovery storage is available to approved projects. When a disaster recovery account is approved, a secondary directory tree is made available for users\u2019 data</p> <p>The data are written to a separate pool of tapes from the primary data copies. The disaster recovery tapes are moved from the TS4500 library as they fill up and are stored in a fireproof vault in Cheyenne, Wyoming. See Quasar system specifications for details.</p>"},{"location":"storage-systems/quasar/#policies","title":"Policies","text":"<ul> <li>The system is not backed up.</li> <li>Vendor support for the system is 9 a.m. to 5 p.m. next business day,   so problems that occur outside of those hours may need to wait to be   resolved.</li> <li>CISL does not enforce file size at writing time, but when files   smaller than the minimum size are found on the system, you may be   asked to relocate the holdings to more appropriate storage such as the   NCAR Campaign Storage file system or Stratus object storage system.</li> <li>If excessive read, rewrite, or metadata change activity is detected,   you may be asked to relocate the holdings to more appropriate storage   such as the NCAR Campaign Storage file system or Stratus object   storage system.</li> </ul>"},{"location":"storage-systems/quasar/quasar%2Bsystem%2Bspecifications/","title":"Quasar system specifications","text":"<p>The following information applies to the Quasar system hardware and disaster recovery storage maintained at the NCAR-Wyoming Supercomputing Center in Cheyenne, Wyoming.</p>"},{"location":"storage-systems/quasar/quasar%2Bsystem%2Bspecifications/#quasar-system","title":"Quasar system","text":"<p>IBM TS4500 robotic library with 2,198 slots and dual accessors;           media verification enabled.</p> <p>22 IBM TS1160 tape drives</p> <p>2 PB IBM ESS disk cache</p> <p>5 Dell PowerEdge R540 data movers</p> <p>1620 JE cartridges, each with an uncompressed capacity of 20 TB</p> <p>IBM Spectrum Archive and IBM Spectrum Scale software</p> <p>User interface: Globus</p> <p>High-availability capabilities include:</p> <ul> <li><p>Dual accessors in library</p></li> <li><p>Spectrum Archive software can failover to any of the               movers</p></li> <li><p>Disk cache uses declustered RAID to be able to handle multiple               disk failures</p></li> <li><p>Disk cache comprises two servers, which are redundant</p></li> </ul>"},{"location":"storage-systems/quasar/quasar%2Bsystem%2Bspecifications/#configuration","title":"Configuration","text":""},{"location":"storage-systems/quasar/quasar%2Bsystem%2Bspecifications/#fireproof-safe-for-disaster-recovery-tapes","title":"Fireproof safe for disaster recovery tapes","text":"<p>\u201cB\u201d rated heavy duty construction.</p> <p>2-Hr. 350\u00b0F fire protection. Tested at temperatures up to 1700\u00b0F.</p> <p>U.L. Listed Residential Security Container burglary rating</p> <p>Overall thickness of 4-5/8,\" constructed with 2\" defense barrier           of outer and inner steel plates.</p> <p>Heat-expandable intumescent door seal guards contents against severe           fires.</p> <p>Heavy-duty steel hinges provide easy, smooth door operation.</p> <p>Two 1-1/2\"-diameter solid steel chrome-plated locking bolts.</p> <p>Two 1-1/2\" diameter solid steel deadbolts lock deep into the body,           preventing door removal during a forced entry attempt.</p> <p>Equipped with the AMSEC ESL10XL U.L. Listed Type 1 electronic           lock.</p> <p>Lock protected by a tempered glass relock device.</p> <p>Weight: 318 lbs</p> <p>Interior dimensions: 19\"H x 12.5\"W x 12.2\"D</p> <p>Outside dimensions: 24.5\"H x 18\"W x 19.875\"D</p> <p>Cubic inches: 2,898</p> <p>Shelves: 2</p> <p>Steel door thickness: 2\" inches</p> <p>Body thickness: 2.875\"</p> <p>Inner wall steel thickness: 2.875\"</p> <p>Burglary protection (Safe Rating): RSC I</p> <p>Fill type: Concrete mix</p> <p>Fire protection: 120-minute</p> <p>Customizable: Yes</p> <p>Safe type: Commercial Security Safe | Composite Safe | Retail Safe |           Back Office Safe</p> <p>USA-made or foreign-made: Foreign-made</p>"},{"location":"storage-systems/quasar/quasar%2Bsystem%2Bspecifications/#door","title":"Door","text":""},{"location":"storage-systems/quasar/quasar%2Bsystem%2Bspecifications/#body","title":"Body","text":""},{"location":"storage-systems/quasar/quasar%2Bsystem%2Bspecifications/#locking-mechanism","title":"Locking mechanism","text":""},{"location":"storage-systems/quasar/quasar%2Bsystem%2Bspecifications/#additional-information","title":"Additional information","text":""},{"location":"storage-systems/stratus/","title":"Stratus object storage system","text":"<p>Stratus, the CISL object storage disk system described here, is for long-term data storage.</p> <p>Some documents attached below include the name of the vendor \u2013 Active Scale, a division of Western Digital \u2013 and some refer to the system with the name \"Data Commons S3.\"</p>"},{"location":"storage-systems/stratus/#system-overview","title":"System overview","text":"<p>Stratus does NOT have POSIX file system access. In fact, it differs from other file systems in many ways:</p> <ul> <li>There is no directory structure, only a flat hierarchy with a single   level (bucket and content of the bucket).</li> <li>The data and metadata are accessed programmatically (rather than at   the command line) with get/put commands, via an HTTP REST API.</li> <li>Data and metadata can be accessed either via a library (such as   Python's <code>boto3</code>) or a web browser (either directly for the HTTP calls   or via web interface).</li> <li>The system uses an API that is similar but not identical to the Amazon   Web Services S3.</li> <li>Accounts are identified by a key pair: access key and secret key, as   in these examples:<ul> <li>Access key: <code>AK0IYXKCCIA63BMNCOUN</code></li> <li>Secret key: <code>Joeke2uHHebQdKJBgTVUzp+j7uRDthPdIBl5YaLE</code></li> </ul> </li> <li>Accounts are associated with email, and each email address can have   only a single account with a single role. A person who needs two roles   must use two separate emails.</li> <li>Two roles exist:<ul> <li>Admin \u2013 An admin can create buckets and users, set up read/write   access control for users, and do everything a user can do; owns data   created by users.</li> <li>Users \u2013 Users may access buckets and read or write data inside   buckets if the admin granted access. Users cannot create   buckets.</li> </ul> </li> </ul>"},{"location":"storage-systems/stratus/#policies","title":"Policies","text":"<ul> <li>The system is not backed up.</li> <li>Support will be provided during business hours on business days.</li> <li>CISL will create only one admin account per lab. The admin will be   able to create accounts for other users. Because the secret key-based   logins do not expire, the admin will also delete accounts as   appropriate \u2013 for example, when a user leaves NCAR.</li> </ul>"},{"location":"storage-systems/stratus/#requesting-account","title":"Requesting account","text":"<p>Contact CISL to request an account. You will be asked to: - Specify how much disk space you need. - Give a brief description (one sentence) of your intended use case. - Acknowledge that you will be the admin and will manage buckets and   users.</p>"},{"location":"storage-systems/stratus/#documentation-and-additional-information","title":"Documentation and additional information","text":"<p>This related page will help you get started as an object storage admin: Getting started with object storage admin account.</p> <ul> <li> <p>Additional documentation is attached below.</p> </li> <li> <p>The system is accessible only via the NCAR VPN. This is important   mostly for the browser-based access, since CISL anticipates that the   server-based access will be from an internal server anyway.</p> </li> <li> <p>The access and secret credentials will be sent via email. They are all   it takes a user to login (there is no UCAS, CIT, or Duo login). The   NCAR username is irrelevant for this system.</p> </li> <li> <p>The way that these credentials are (unlike username/password) seems to   nudge users towards nonoptimal patterns, such as hardcoding them into   the source code. Users are strongly advised to NOT do that.   Instead, use a separate file (outside of version control) similar to   the following and source that file before running your code. This   applies to both admin and user accounts. <pre><code>export AWS_ACCESS_KEY_ID='xxx'\nexport AWS_SECRET_ACCESS_KEY='yyy'\n</code></pre></p> </li> <li> <p>Admins might want to create a separate user account for themselves   with just reading (and perhaps writing) capabilities and not admin   capabilities. This would require use of a different email address,   since the system does not allow reuse of existing emails. Admins   might use a personal email, or a (group) alias setup in PeopleDB.</p> </li> </ul>"},{"location":"storage-systems/stratus/#click-to-download","title":"Click to download","text":"<ul> <li> <p>Object_Storage_S3_API_Reference.pdf</p> </li> <li> <p>Object_Storage_View_User_Guide.pdf</p> </li> </ul>"},{"location":"storage-systems/stratus/getting-started-with-an-object-storage-admin-account/","title":"Getting started with an object storage admin account","text":"<p>This page describes and shows\u00a0how to get started as an admin for the Stratus object storage system.</p>"},{"location":"storage-systems/stratus/getting-started-with-an-object-storage-admin-account/#using-the-web-gui-to-log-into-your-s3-admin-account","title":"Using the web GUI to log into your S3 admin account","text":"<p>After connecting to the UCAR internal network or the VPN:</p> <ul> <li> <p>Set your browser to point to this   URL: https://stratus-admin.ucar.edu:10443/asview</p> </li> <li> <p>Enter your access ID and secret key</p> </li> </ul> <p> Figure 1.</p>"},{"location":"storage-systems/stratus/getting-started-with-an-object-storage-admin-account/#creating-buckets","title":"Creating buckets","text":"<p>Figure 2 shows the screen where you'll create buckets.  To create a bucket, press the \"Create Bucket\" button and you'll be prompted for a bucket name.  Note that the bucket name must be globally unique for the entire system. If a different account holder on the system already has a bucket with that name, you'll get an error. This behavior conforms with the AWS S3 API. (See also https://stackoverflow.com/a/59656742/25891)</p> <p> Figure 2.</p> <p>Figure 3 shows your new bucket and will show all your buckets as you create them.</p> <p> Figure 3.</p> <p>Now that a bucket has been created, you can write objects to the bucket and read from it using the access and secret keys that you used to log onto the web interface.</p> <p>There are many clients that can be used to take S3 actions, such as transferring objects to/from the system, listing buckets, etc.</p> <ul> <li> <p>For example, the cyberduck desktop client can be utilized.</p> </li> <li> <p>Or python scripts (using the boto3 library) can be used to perform S3   operations. CISL has tried alternative access libraries bucketstore   and apache-libcloud, but they lack the ability to select the URL where   to connect, and therefore cannot be used with CISL's hardware (only   with AWS).</p> </li> </ul> <p>These other clients can also be used to create buckets and manage your account (not everything has to be done through this web interface).</p> <p>When connecting to the system with a client to perform S3 operations (e.g., transfer data, etc.), use the host name <code>stratus.ucar.edu</code> rather than <code>stratus-admin.ucar.edu</code>. The host name <code>stratus-admin.ucar.edu</code> should only be used to connect to the administrative web GUI.</p>"},{"location":"storage-systems/stratus/getting-started-with-an-object-storage-admin-account/#granting-permissions-to-other-users","title":"Granting permissions to other users","text":"<p>By default, only the account owner has the privileges to write to and read from buckets in the account. It's possible to grant other users on the system and even anonymous users (those without any keys) write and read privileges. The account owner is responsible for informing additional users of CISL communications regarding this system, such as announcements of planned downtime, or ensuring that they subscribe to the \"Stratus Object Storage\" Notifier list.</p> <p>To grant other users access, you may need to first add the user. Press the \u201cUsers\u201d link on the left side of the screen and add the user.  Once the user has been added click on a specific bucket name shown in the list of buckets in Figure 3. Figure 4 shows the options you will see for granting access. Selecting \u201cUser Permission\u201d will allow you to grant access to a specific user.</p> <p> Figure 4.</p> <p> Figure 5.</p> <p>In the section titled \"Add Other Account or User\" as shown in Figure 5, enter the email address of the person you would like to grant access to. Next, select the permissions you want to grant:</p> Permission Meaning Read objects Allows the user to read objects from the bucket Write objects Allows the user to write objects to the bucket Read bucket permissions Allows the user to read the permissions on the bucket and, for example, see what other users have permissions on the bucket Write bucket permissions Allows the user to change the permissions on a bucket. For example, a user could grant another user permissions on the bucket <p>Once granted, the specific user will have the permissions that you've configured for that bucket. Since each bucket has its own permissions, you'll need to set permissions for each bucket if you want other users to be able to access it.</p> <p>It's also possible to grant permissions to what are termed public users.  A public user is defined as either an anonymous user or an authenticated user. An anonymous user means a user who does not need any keys to access the bucket; anonymous users can only read objects from buckets. An authenticated user means any user on the system. To grant permissions to public users, click on \"Public Permissions\" in the screen that was visited earlier and shown in Figure 6.</p> <p> Figure 6.</p> <p> Figure 7.</p> <p>Select the permissions you want to grant, as shown in Figure 7, to enable access for authenticated users or anonymous users. Note that the Data Commons System is currently only reachable from devices on the internal UCAR network. Devices outside of the UCAR network are not able to access the storage system, even for anonymous access.</p>"},{"location":"storage-systems/stratus/getting-started-with-an-object-storage-admin-account/#vendor-documentation","title":"Vendor documentation","text":"<p>Here is a link to vendor documentation that may help you: S3 API Reference</p>"},{"location":"storage-systems/stratus/getting-started-with-an-object-storage-admin-account/#glossary","title":"Glossary","text":"<p>AWS SDK - Amazon Web Services Software Development Kit for Python (Boto3) (http://aws.amazon.com/sdk-for-python/)</p> <p>Bucket - Container to store objects, similar to a directory</p> <p>Cyberduck - Cloud storage browser (http://cyberduck.io)</p> <p>Object - data structure that stores both file metadata and data</p>"},{"location":"user-support/","title":"Index","text":""},{"location":"user-support/#consulting","title":"Consulting","text":""},{"location":"user-support/#consultant-on-duty","title":"Consultant on Duty","text":""},{"location":"user-support/#virtual-consulting","title":"Virtual Consulting","text":""},{"location":"user-support/#documentation","title":"Documentation","text":"<p>Search for relevant documentation with the \"Search\" field at upper-right or use one of the service desk links below to submit a help request (login required). Where a login is required, use your NCAR/UCAR username and CIT password. Users who do not have CIT passwords (non-NCAR/UCAR staff members, for example) can reset their passwords here or call 303-497-2400 for assistance.</p>"},{"location":"user-support/#ncar-help-desk-resources","title":"NCAR Help Desk Resources","text":""},{"location":"user-support/#research-computing-help-desk","title":"Research Computing help desk","text":"<p>Contact the help desk for assistance with Cheyenne, Casper, and related data-storage systems.</p>"},{"location":"user-support/#enterprise-service-desk-staff-support","title":"Enterprise Service Desk staff support","text":"<p>The Enterprise Service Desk portal is for NCAR/UCAR staff who need support for internal IT services.</p> <p>CISL staff ask that you provide the information listed below when you report the failure of a Cheyenne or Casper job.\u00a0This information will help us find a solution as quickly as possible.</p>"},{"location":"user-support/#best-practices-for-support-tickets","title":"Best Practices for Support Tickets","text":"<p>When submitting a support ticket please include as much detail as possible to enable quicker resolution. It is also important to preserve any input files, binaries and executables in the directories as they were when the problem occurred.</p> <ul> <li> <p>Resource name (Derecho, Casper, JupyterHub,...),</p> </li> <li> <p>Exact error messages and/or paths to error output,</p> </li> <li> <p>Batch script location,</p> </li> <li> <p>PBS JobID(s) of failed effort,</p> </li> <li> <p>Run &amp; source directory paths (ideally UNIX-readable by \u2018others\u2019),</p> </li> <li> <p>Any other pertinent information:</p> <ul> <li> <p>Last time this exact workflow was successful, if any (or changes since last success),</p> </li> <li> <p>Troubleshooting steps already attempted, etc.</p> </li> </ul> </li> </ul> <p>And please remember to let us know when your issue is resolved!</p> <p>https://rchelp.ucar.edu</p>"},{"location":"user-support/#getting-connected","title":"Getting Connected","text":""},{"location":"user-support/#ncar-hpc-users-group","title":"NCAR HPC Users' Group","text":"<p>The NCAR HPC Users\u2019 Group, NHUG, is a dedicated community that aims to promote the productive use of high-performance computing (HPC) facilities at NCAR and increase collaboration among all of the NCAR HPC community.</p> <p>NHUG is open to all HPC users and holds monthly meetings featuring different HPC-related topics. See the NHUG page for additional details.</p>"},{"location":"user-support/#daily-bulletin","title":"Daily Bulletin","text":"<p>If you are not already receiving the Daily Bulletin, use this link to subscribe:</p> <p>https://ncar.pub/CISL-Daily-Bulletin-subscribe</p> <p>New users of our HPC systems are subscribed automatically.</p>"},{"location":"user-support/#sundog","title":"Sundog","text":"<p>The CISL on Sundog intranet space is open to all NCAR and UCAR staff, who log in using a CIT password.</p> <p>What to do:</p> <ol> <li> <p>Use this link to go to the CISL Sundog space.</p> </li> <li> <p>Log in if you haven't already done so.</p> </li> <li> <p>Click the Submit button under \"Joining Instructions.\" (Required     only once.)</p> </li> </ol>"},{"location":"user-support/how-to-join-cisl-on-sundog/","title":"How to join cisl on sundog","text":"<p>The CISL on Sundog intranet space is open to all NCAR and UCAR staff, who log in using a CIT password.</p> <p>What to do:</p> <ol> <li> <p>Use this link to go to the CISLspace.</p> </li> <li> <p>Log in if you haven't already done so.</p> </li> <li> <p>Click the Submit button under \"Joining Instructions.\" (Required     only once.)</p> </li> </ol>"},{"location":"user-support/how-to-subscribe-to-cisl-daily-bulletin/","title":"How to subscribe to cisl daily bulletin","text":"<p>If you are not already receiving the Daily Bulletin, use this link to subscribe:</p> <p>https://ncar.pub/CISL-Daily-Bulletin-subscribe</p> <p>New users of our HPC systems are subscribed automatically.</p>"}]}